{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea4a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import main_2_pre_processing\n",
    "import pandas as pd\n",
    "from src.pre_proc import ais_query\n",
    "import config\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1c5d9",
   "metadata": {},
   "source": [
    "##### Load data from filtered parquet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b428c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1\n",
      "Raw DataFrame shape: (7276150, 28)\n",
      "Raw DataFrame columns:\n",
      "['Timestamp', 'Type of mobile', 'Latitude', 'Longitude', 'Navigational status', 'ROT', 'SOG', 'COG', 'Heading', 'IMO', 'Callsign', 'Name', 'Ship type', 'Cargo type', 'Width', 'Length', 'Type of position fixing device', 'Draught', 'Destination', 'ETA', 'Data source type', 'A', 'B', 'C', 'D', 'Date', 'MMSI', 'Segment']\n"
     ]
    }
   ],
   "source": [
    "# Filtered AIS data is located at \"ais-data/parquet/\" and contains your filtered downloaded data (filtered based on bbox, ship types, etc.)\n",
    "# Still contains NaN values and unprocessed columns\n",
    "raw_df = ais_query.query_ais_duckdb(\"ais-data/parquet\", verbose=True)\n",
    "print(f\"Raw DataFrame shape: {raw_df.shape}\")\n",
    "print(f\"Raw DataFrame columns:\\n{raw_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ec4ab",
   "metadata": {},
   "source": [
    "##### Look at how many records and unique days are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4a4f0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded: 7276150 records.\n",
      "Continuous periods (split when a day is missing):\n",
      "2025-08-01 -> 2025-09-27 (58 days)\n",
      "Total periods: 1\n"
     ]
    }
   ],
   "source": [
    "unique_days = pd.Series(raw_df['Timestamp'].dt.normalize().unique())\n",
    "unique_days = pd.to_datetime(unique_days).sort_values()\n",
    "print(f\"Data loaded: {len(raw_df)} records.\")\n",
    "#print(\"Unique days:\", [d.strftime('%Y-%m-%d') for d in unique_days])\n",
    "\n",
    "# Print continuous periods (split when a day is missing)\n",
    "if unique_days.empty:\n",
    "    print(\"No days found in data.\")\n",
    "else:\n",
    "    days = pd.to_datetime(unique_days).sort_values().reset_index(drop=True)\n",
    "    diffs = days.diff().dt.days\n",
    "    groups = (diffs.fillna(1) != 1).cumsum()\n",
    "    periods = [(grp.iloc[0], grp.iloc[-1]) for _, grp in days.groupby(groups)]\n",
    "\n",
    "    print(\"Continuous periods (split when a day is missing):\")\n",
    "    for start, end in periods:\n",
    "        if start == end:\n",
    "            print(start.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            length = (end - start).days + 1\n",
    "            print(f\"{start.strftime('%Y-%m-%d')} -> {end.strftime('%Y-%m-%d')} ({length} days)\")\n",
    "    print(f\"Total periods: {len(periods)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ba6fa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up memory\n",
    "del raw_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f5bb8",
   "metadata": {},
   "source": [
    "##### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e7a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENT_MAX_LENGTH: 300\n",
      "NUMERIC_COLS: ['Latitude', 'Longitude', 'SOG', 'COG']\n",
      "TRAIN_START_DATE: 2025-08-01\n",
      "TRAIN_END_DATE: 2025-08-31\n",
      "TEST_START_DATE: 2025-09-01\n",
      "TEST_END_DATE: 2025-09-01\n"
     ]
    }
   ],
   "source": [
    "# Setup this parameters at your convenience into config.py\n",
    "\n",
    "SEGMENT_MAX_LENGTH = config.SEGMENT_MAX_LENGTH # Maximum length of segments (minimum lenght is set during filtering, and should be equal to this, if we want fixed-length segments)\n",
    "NUMERIC_COLS = config.NUMERIC_COLS             # Numeric columns to be normalized\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE     # Start date for training data you want to PRE-PROCESS\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE         # End date for training data you want to PRE-PROCESS\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE       # Start date for test data you want to PRE-PROCESS\n",
    "TEST_END_DATE = config.TEST_END_DATE           # End date for test data you want to PRE-PROCESS\n",
    "\n",
    "print(f\"SEGMENT_MAX_LENGTH: {SEGMENT_MAX_LENGTH}\")\n",
    "print(f\"NUMERIC_COLS: {NUMERIC_COLS}\")\n",
    "print(f\"TRAIN_START_DATE: {TRAIN_START_DATE}\")\n",
    "print(f\"TRAIN_END_DATE: {TRAIN_END_DATE}\")\n",
    "print(f\"TEST_START_DATE: {TEST_START_DATE}\")\n",
    "print(f\"TEST_END_DATE: {TEST_END_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e3bb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for training period: 2025-08-01 to 2025-08-31\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1 AND Date IN ('2025-08-01', '2025-08-02', '2025-08-03', '2025-08-04', '2025-08-05', '2025-08-06', '2025-08-07', '2025-08-08', '2025-08-09', '2025-08-10', '2025-08-11', '2025-08-12', '2025-08-13', '2025-08-14', '2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20', '2025-08-21', '2025-08-22', '2025-08-23', '2025-08-24', '2025-08-25', '2025-08-26', '2025-08-27', '2025-08-28', '2025-08-29', '2025-08-30', '2025-08-31')\n",
      "[pre_processing] Initial data size: 3996082 records.\n",
      "[pre_processing] Dropping unnecessary columns and rows with missing values...\n",
      "[pre_processing] Data size after dropping: 3925808 records.\n",
      "[pre_processing] Splitting segments to max length 300...\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/pre_processed_df_train.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG', 'MMSI', 'DeltaT', 'Segment_nr', 'NavStatus_0', 'NavStatus_1', 'NavStatus_2', 'NavStatus_3', 'NavStatus_4', 'NavStatus_5', 'NavStatus_6', 'NavStatus_7', 'NavStatus_8', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/pre_processing_metadata_train.json...\n"
     ]
    }
   ],
   "source": [
    "main_2_pre_processing.main_pre_processing(dataframe_type=\"train\")\n",
    "# The pre-processed DataFrame is now saved to the path specified in config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "\n",
    "# YOU CAN ALSO PRE_PROCESS JUST THE TRAIN DATAFRAME AND THEN SPLIT IN TRAIN/TEST LATER IF YOU PREFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbabc020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for testing period: 2025-09-01 to 2025-09-01\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1 AND Date IN ('2025-09-01')\n",
      "[pre_processing] Initial data size: 111347 records.\n",
      "[pre_processing] Dropping unnecessary columns and rows with missing values...\n",
      "[pre_processing] Data size after dropping: 111338 records.\n",
      "[pre_processing] Splitting segments to max length 300...\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/pre_processed_df_test.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG', 'MMSI', 'DeltaT', 'Segment_nr', 'NavStatus_0', 'NavStatus_1', 'NavStatus_2', 'NavStatus_3', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/pre_processing_metadata_test.json...\n"
     ]
    }
   ],
   "source": [
    "main_2_pre_processing.main_pre_processing(dataframe_type=\"test\")\n",
    "# The pre-processed DataFrame is now saved to the path specified in config.PRE_PROCESSING_DF_TEST_PATH"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
