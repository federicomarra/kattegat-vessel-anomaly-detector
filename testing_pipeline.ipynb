{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a37322ad",
   "metadata": {},
   "source": [
    "# How to run scripts with CPU use (for testing)\n",
    "- 1. Log in\n",
    "ssh sXXXXXX@login.hpc.dtu.dk\n",
    "\n",
    "- 2. Go to project\n",
    "cd /dtu/blackhole/0e/213550/dark-vessel-hunter\n",
    "\n",
    "- 3. Start interactive node\n",
    "linuxsh\n",
    "\n",
    "- 4. Back to project (now on compute node)\n",
    "cd /dtu/blackhole/0e/213550/dark-vessel-hunter\n",
    "\n",
    "- 5. Load Python\n",
    "module load python3/3.10.12\n",
    "\n",
    "- 6. Activate venv\n",
    "source .venv/bin/activate\n",
    "\n",
    "- 7. Run your script (short tests only)\n",
    "python3 main_3_training.py  # example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea4a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import config\n",
    "import main_2_pre_processing\n",
    "from src.pre_proc import ais_query\n",
    "from src.utils import training_utils\n",
    "from src.utils import evaluation_utils\n",
    "from src.utils import inspection_utils\n",
    "\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE     \n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE         \n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE       \n",
    "TEST_END_DATE = config.TEST_END_DATE           \n",
    "\n",
    "PRE_PROCESSING_DF_TRAIN_PATH = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "PRE_PROCESSING_DF_TEST_PATH = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "\n",
    "SEGMENT_MAX_LENGTH = config.SEGMENT_MAX_LENGTH \n",
    "NUMERIC_COLS = config.NUMERIC_COLS             \n",
    "FEATURE_COLS = config.FEATURE_COLS\n",
    "\n",
    "WEIGHTS_PATH = config.WEIGHTS_PATH\n",
    "PLOT_PATH = config.PLOT_PATH\n",
    "PREDICTION_DF_PATH = config.PREDICTION_DF_PATH\n",
    "PREDICTION_DENORM_DF_PATH = config.PREDICTION_DENORM_DF_PATH\n",
    "\n",
    "MAPS_PATH = config.MAPS_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201443ae",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1c5d9",
   "metadata": {},
   "source": [
    "##### OPTIONAL: Load data from filtered parquet database and look at how many records and unique days are in the data\n",
    "--> if it's unable to allocate memory, it means that your database is too big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b428c91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtered AIS data is located at \"ais-data/parquet/\" and contains your DOWNLOADED and then FILTERED data (filtered based on bbox, ship types, etc.)\n",
    "# Still contains NaN values and unprocessed columns\n",
    "raw_df = ais_query.query_ais_duckdb(\"ais-data/parquet\", verbose=True)\n",
    "\n",
    "\n",
    "print(f\"Raw DataFrame shape: {raw_df.shape}\")\n",
    "print(f\"Raw DataFrame columns:\\n{raw_df.columns.tolist()}\")\n",
    "\n",
    "unique_days = pd.Series(raw_df['Timestamp'].dt.normalize().unique())\n",
    "unique_days = pd.to_datetime(unique_days).sort_values()\n",
    "print(f\"Data loaded: {len(raw_df)} records.\")\n",
    "#print(\"Unique days:\", [d.strftime('%Y-%m-%d') for d in unique_days])\n",
    "\n",
    "# Print continuous periods (split when a day is missing)\n",
    "if unique_days.empty:\n",
    "    print(\"No days found in data.\")\n",
    "else:\n",
    "    days = pd.to_datetime(unique_days).sort_values().reset_index(drop=True)\n",
    "    diffs = days.diff().dt.days\n",
    "    groups = (diffs.fillna(1) != 1).cumsum()\n",
    "    periods = [(grp.iloc[0], grp.iloc[-1]) for _, grp in days.groupby(groups)]\n",
    "\n",
    "    print(\"Continuous periods (split when a day is missing):\")\n",
    "    for start, end in periods:\n",
    "        if start == end:\n",
    "            print(start.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            length = (end - start).days + 1\n",
    "            print(f\"{start.strftime('%Y-%m-%d')} -> {end.strftime('%Y-%m-%d')} ({length} days)\")\n",
    "    print(f\"Total periods: {len(periods)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del raw_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f5bb8",
   "metadata": {},
   "source": [
    "##### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "42e7a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENT_MAX_LENGTH: 300\n",
      "NUMERIC_COLS: ['Latitude', 'Longitude', 'SOG', 'COG']\n",
      "TRAIN_START_DATE: 2025-08-01\n",
      "TRAIN_END_DATE: 2025-08-31\n",
      "TEST_START_DATE: 2025-09-01\n",
      "TEST_END_DATE: 2025-09-03\n"
     ]
    }
   ],
   "source": [
    "# Setup this parameters at your convenience into config.py\n",
    "print(f\"SEGMENT_MAX_LENGTH: {SEGMENT_MAX_LENGTH}\")   # Maximum length of segments (minimum length is set during filtering, but during processing we discard segments shorter than SEGMENT_MAX_LENGTH anyway)\n",
    "print(f\"NUMERIC_COLS: {NUMERIC_COLS}\")               # Numeric columns to be normalized\n",
    "print(f\"TRAIN_START_DATE: {TRAIN_START_DATE}\")       # Start date for training data you want to PRE-PROCESS\n",
    "print(f\"TRAIN_END_DATE: {TRAIN_END_DATE}\")           # End date for training data you want to PRE-PROCESS\n",
    "print(f\"TEST_START_DATE: {TEST_START_DATE}\")         # Start date for test data you want to PRE-PROCESS\n",
    "print(f\"TEST_END_DATE: {TEST_END_DATE}\")             # End date for test data you want to PRE-PROCESS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7010772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRE_PROCESSING_DF_TRAIN_PATH: ais-data/df_preprocessed/pre_processed_df_train.parquet\n",
      "PRE_PROCESSING_DF_TEST_PATH: ais-data/df_preprocessed/pre_processed_df_test.parquet\n"
     ]
    }
   ],
   "source": [
    "print(f\"PRE_PROCESSING_DF_TRAIN_PATH: {PRE_PROCESSING_DF_TRAIN_PATH}\") # Pre-processed TRAINING data will be saved here after pre-processing, change this path in config.py if needed\n",
    "print(f\"PRE_PROCESSING_DF_TEST_PATH: {PRE_PROCESSING_DF_TEST_PATH}\")   # Pre-processed TEST data will be saved here after pre-processing, change this path in config.py if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e3bb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for training period: 2025-08-01 to 2025-08-31\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1 AND Date IN ('2025-08-01', '2025-08-02', '2025-08-03', '2025-08-04', '2025-08-05', '2025-08-06', '2025-08-07', '2025-08-08', '2025-08-09', '2025-08-10', '2025-08-11', '2025-08-12', '2025-08-13', '2025-08-14', '2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20', '2025-08-21', '2025-08-22', '2025-08-23', '2025-08-24', '2025-08-25', '2025-08-26', '2025-08-27', '2025-08-28', '2025-08-29', '2025-08-30', '2025-08-31')\n",
      "[pre_processing] Initial data size: 3996082 records.\n",
      "[pre_processing] Dropping unnecessary columns and rows with missing values...\n",
      "[pre_processing] Data size after dropping: 3925808 records.\n",
      "[pre_processing] Splitting segments to max length 300...\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_train.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG', 'MMSI', 'Segment_nr', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/df_preprocessed/pre_processing_metadata_train.json...\n"
     ]
    }
   ],
   "source": [
    "# Queries the dates that are between TRAIN_START_DATE and TRAIN_END_DATE\n",
    "main_2_pre_processing.main_pre_processing(dataframe_type=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbabc020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for testing period: 2025-09-01 to 2025-09-03\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1 AND Date IN ('2025-09-01', '2025-09-02', '2025-09-03')\n",
      "[pre_processing] Initial data size: 367732 records.\n",
      "[pre_processing] Dropping unnecessary columns and rows with missing values...\n",
      "[pre_processing] Data size after dropping: 367230 records.\n",
      "[pre_processing] Splitting segments to max length 300...\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_test.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG', 'MMSI', 'Segment_nr', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/df_preprocessed/pre_processing_metadata_test.json...\n"
     ]
    }
   ],
   "source": [
    "# Queries the dates that are between TEST_START_DATE and TEST_END_DATE\n",
    "main_2_pre_processing.main_pre_processing(dataframe_type=\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6649daf",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3d5ca025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUMERIC_COLS: ['Latitude', 'Longitude', 'SOG', 'COG']\n",
      "FEATURE_COLS: ['Latitude', 'Longitude', 'SOG', 'COG']\n",
      "PRE_PROCESSING_DF_TRAIN_PATH: ais-data/df_preprocessed/pre_processed_df_train.parquet\n",
      "PRE_PROCESSING_DF_TEST_PATH: ais-data/df_preprocessed/pre_processed_df_test.parquet\n"
     ]
    }
   ],
   "source": [
    "print(f\"NUMERIC_COLS: {NUMERIC_COLS}\")   # Columns that will be normalized\n",
    "print(f\"FEATURE_COLS: {FEATURE_COLS}\")   # Columns that will be used as features\n",
    "print(f\"PRE_PROCESSING_DF_TRAIN_PATH: {PRE_PROCESSING_DF_TRAIN_PATH}\")   # Path to pre-processed training DataFrame\n",
    "print(f\"PRE_PROCESSING_DF_TEST_PATH: {PRE_PROCESSING_DF_TEST_PATH}\")   # Path to pre-processed testing DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a49acb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed training DataFrame from file to  ---> df_seq\n",
    "\n",
    "# df_seq is list of dictionaries with the following structure:  (If you want to modify it, look at training_utils.py load_df_seq function)\n",
    "# {       \n",
    "#         \"Segment_nr\": seg_id,                # metadata\n",
    "#         \"MMSI\": mmsi,                        # metadata  \n",
    "#         \"FirstTimestamp\": first_timestamp,   # metadata\n",
    "#         \"ShipTypeID\": ship_type_id,          # used to condition the model during decoding\n",
    "#         \"Sequence\": X,                       # actual features\n",
    "# })      \n",
    " \n",
    "\n",
    "# Loading \n",
    "df_seq_train = training_utils.load_df_seq(PRE_PROCESSING_DF_TRAIN_PATH)\n",
    "\n",
    "# Splitting into train and val sets (80%-20% split) (validation set useful for early stopping and other metrics during training)\n",
    "df_seq_train, df_seq_val = train_test_split(\n",
    "        df_seq_train,\n",
    "        test_size=0.2,\n",
    "        random_state=5,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24915434",
   "metadata": {},
   "source": [
    "##### Dataset and Dataloader classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5294e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import \n",
    "# This is an ad-hoc Dataset class to wrap up the df_seq into a PyTorch Dataset\n",
    "# Try to keep as it is unless you need to modify something specific about features/labels\n",
    "class AISTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapping df_seq:\n",
    "      - Sequence: list-of-lists (T, Features) or np.ndarray\n",
    "      - ShipTypeID: integer class id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_seq: pd.DataFrame):\n",
    "        self.df = df_seq.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = np.array(row[\"Sequence\"], dtype=np.float32)  # (T, Features)\n",
    "        x = torch.from_numpy(seq)                         # (T, Features)\n",
    "\n",
    "        ship_type_id = int(row[\"ShipTypeID\"])\n",
    "        ship_type_id = torch.tensor(ship_type_id, dtype=torch.long)\n",
    "\n",
    "        return x, ship_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b9f9c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Give df_seq to Dataset class to create train and val DATASETS\n",
    "train_dataset = AISTrajectoryDataset(df_seq_train)\n",
    "val_dataset = AISTrajectoryDataset(df_seq_val) # useful for early stopping during training\n",
    "\n",
    "# Give the datasets to DataLoader to create train and val DATALOADERS\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64, # can be modified\n",
    "    shuffle=True,  # usually shuffling for training to improve generalization\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64, # can be modified\n",
    "    shuffle=False, # usually no shuffling for validation, because we want consistent evaluation\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "572b7e8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence shape: T=300, F=4, num_shiptypes=17\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Infer sequence shape and num_shiptypes (needed for model definition)\n",
    "sample_x, _ = train_dataset[0]\n",
    "T, F = sample_x.shape\n",
    "num_shiptypes = df_seq_train[\"ShipTypeID\"].nunique()\n",
    "print(f\"Sequence shape: T={T}, F={F}, num_shiptypes={num_shiptypes}\")\n",
    "\n",
    "# Device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360a195",
   "metadata": {},
   "source": [
    "##### Model class (import your model here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e960ac0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from src.train.models.AE_simple import LSTMAutoencoderWithShipType\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMAutoencoderWithShipType(\n",
    "    input_dim=F,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    num_shiptypes=num_shiptypes,\n",
    "    shiptype_emb_dim=8,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253704d",
   "metadata": {},
   "source": [
    "##### Loss function (can be modified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8b68a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sequence_loss_fn(\n",
    "    recon_batch: torch.Tensor,  # (B, T, F)\n",
    "    x_batch: torch.Tensor,      # (B, T, F)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns loss per sequence: shape (B,).\n",
    "    Here you can easily change the definition (MSE, MAE, weights, etc.).\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(recon_batch, x_batch, reduction=\"none\")  # (B, T, F)\n",
    "    # mean on time and feature, keeping batch separate\n",
    "    seq_error = mse.mean(dim=(1, 2))  # (B,)\n",
    "    return seq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd082ed",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6280824f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS_PATH: models/AE_simple.pth\n"
     ]
    }
   ],
   "source": [
    "print(f\"WEIGHTS_PATH: {WEIGHTS_PATH}\") # Change this path in config.py if needed, the model weights will be saved here after training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d590b5d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = sequence_loss_fn\n",
    "max_grad_norm = 1.0 # Maximum gradient norm for gradient clipping (prevent exploding gradients, can be adjusted)\n",
    "\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "\n",
    "    for x, ship_type_id in train_loader:\n",
    "        x = x.to(device)\n",
    "        ship_type_id = ship_type_id.to(device)\n",
    "\n",
    "        recon, _ = model(x, ship_type_id)\n",
    "        seq_errors = criterion(recon, x) \n",
    "        loss = seq_errors.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "    # Total train loss noemralized by number of samples (a batch could have different size, so we sum and normalize here)\n",
    "    train_loss = train_loss_sum / len(train_loader.dataset)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "    # ---- Validation ---- # Useful for early stopping/model selection\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, ship_type_id in val_loader:\n",
    "            x = x.to(device)\n",
    "            ship_type_id = ship_type_id.to(device)\n",
    "\n",
    "            recon, _ = model(x, ship_type_id)\n",
    "            seq_errors = criterion(recon, x)\n",
    "            loss = seq_errors.mean()\n",
    "            \n",
    "            val_loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / len(val_loader.dataset)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{epochs} - \"\n",
    "        f\"train MSE: {train_loss:.6f} - val MSE: {val_loss:.6f}\"\n",
    "    )\n",
    "\n",
    "\n",
    "torch.save(model.state_dict(), WEIGHTS_PATH)\n",
    "print(f\"Model weights saved to {WEIGHTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18db1e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e671cbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS_PATH: models/AE_simple.pth\n",
      "PLOT_PATH: eval/plots\n",
      "PREDICTION_DF_PATH: eval/predictions_df.parquet\n",
      "PREPROCESSING_DF_TEST_PATH: ais-data/df_preprocessed/pre_processed_df_test.parquet\n"
     ]
    }
   ],
   "source": [
    "print(\"WEIGHTS_PATH:\", WEIGHTS_PATH)\n",
    "print(\"PLOT_PATH:\", PLOT_PATH)\n",
    "print(\"PREDICTION_DF_PATH:\", PREDICTION_DF_PATH)\n",
    "print(f\"PREPROCESSING_DF_TEST_PATH: {PRE_PROCESSING_DF_TEST_PATH}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21dbf2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-processed training DataFrame from file to  ---> df_seq\n",
    "df_seq_test = training_utils.load_df_seq(PRE_PROCESSING_DF_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8dcf3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequence shape: T=300, F=4, num_shiptypes=17\n",
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset and dataloader\n",
    "class AISTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapping df_seq:\n",
    "      - Sequence: list-of-lists (T, F) or np.ndarray\n",
    "      - ShipTypeID: integer class id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_seq: pd.DataFrame):\n",
    "        self.df = df_seq.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = np.array(row[\"Sequence\"], dtype=np.float32)  # (T, F)\n",
    "        x = torch.from_numpy(seq)                         # (T, F)\n",
    "\n",
    "        ship_type_id = int(row[\"ShipTypeID\"])\n",
    "        ship_type_id = torch.tensor(ship_type_id, dtype=torch.long)\n",
    "\n",
    "        return x, ship_type_id\n",
    "\n",
    "test_dataset = AISTrajectoryDataset(df_seq_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Infer sequence shape and num_shiptypes\n",
    "sample_x, _ = test_dataset[0]\n",
    "T, F = sample_x.shape\n",
    "df_seq_train = training_utils.load_df_seq(PRE_PROCESSING_DF_TRAIN_PATH) # TO BE DELETED LATER\n",
    "num_shiptypes = df_seq_train[\"ShipTypeID\"].nunique()                    # For now it uses training df to get num_shiptypes, BUT IT HAS TO BE CONSISTENT, SO WE NEED TO DEFINE A FIXED NUMBER (e.g., limit the number of ship types during pre-processing)\n",
    "print(f\"Sequence shape: T={T}, F={F}, num_shiptypes={num_shiptypes}\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e02d39",
   "metadata": {},
   "source": [
    "##### Put here your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6113b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from src.train.models.AE_simple import LSTMAutoencoderWithShipType\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMAutoencoderWithShipType(\n",
    "    input_dim=F,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    num_shiptypes=num_shiptypes,\n",
    "    shiptype_emb_dim=8,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "83b674c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMAutoencoderWithShipType(\n",
       "  (encoder): LSTM(4, 128, batch_first=True)\n",
       "  (fc_latent): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (shiptype_emb): Embedding(17, 8)\n",
       "  (fc_z_st_to_h): Linear(in_features=72, out_features=128, bias=True)\n",
       "  (decoder): LSTM(4, 128, batch_first=True)\n",
       "  (fc_out): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4555da74",
   "metadata": {},
   "source": [
    "##### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b370c367",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sequence_loss_fn(\n",
    "    recon_batch: torch.Tensor,  # (B, T, F)\n",
    "    x_batch: torch.Tensor,      # (B, T, F)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns loss per sequence: shape (B,).\n",
    "    Here you can easily change the definition (MSE, MAE, weights, etc.).\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(recon_batch, x_batch, reduction=\"none\")  # (B, T, F)\n",
    "    # mean on time and feature, keeping batch separate\n",
    "    seq_error = mse.mean(dim=(1, 2))  # (B,)\n",
    "    return seq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13106daf",
   "metadata": {},
   "source": [
    "##### Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0139bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reconstruction errors on test ===\n",
      "Test:\n",
      "  mean: 0.000228\n",
      "  std:  0.000564\n",
      "  q95:  0.000799\n",
      "  q99:  0.001409\n",
      "  max:  0.012264\n",
      "\n",
      "Suggested anomaly threshold (99th percentile of test): 0.001409\n",
      "Plots saved in directory: \"eval/plots\"\n",
      "Evaluation results: {'test_scores': array([1.1595870e-04, 3.2631713e-05, 8.8864195e-05, ..., 2.9249548e-05,\n",
      "       2.4567564e-05, 1.4920163e-04], shape=(1021,), dtype=float32), 'threshold_99': np.float32(0.0014091902)}\n"
     ]
    }
   ],
   "source": [
    "# Save some plots and get evaluation results about reconstruction errors\n",
    "results = evaluation_utils.make_plots(model=model, test_loader=test_loader, device=device, seq_loss_fn=sequence_loss_fn)\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d086f84",
   "metadata": {},
   "source": [
    "##### Prediction df - for further analysis (e.g plotting on maps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "38a36a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Predictions DataFrame saved to eval/predictions_df.parquet\n",
      "Predictions DataFrame shape: (1021, 6)\n",
      "Predictions DataFrame columns: ['Segment_nr', 'MMSI', 'ShipTypeID', 'Sequence_real', 'Sequence_pred', 'recon_error']\n",
      "\n",
      "Structure of a Sequence_real cell:\n",
      "type: <class 'list'>\n",
      "numpy shape: (300, 4)\n",
      "T (timesteps): 300 F (features per timestep): 4\n"
     ]
    }
   ],
   "source": [
    "# Build predictions DataFrame for further analysis\n",
    "df_predictions = evaluation_utils.build_predictions_df(model=model, dataset=test_loader.dataset, device=device, seq_loss_fn=sequence_loss_fn)\n",
    "\n",
    "# Save predictions DataFrame to file\n",
    "df_predictions.to_parquet(PREDICTION_DF_PATH, index=False)\n",
    "print(\"\")\n",
    "print(f\"Predictions DataFrame saved to {PREDICTION_DF_PATH}\")\n",
    "\n",
    "\n",
    "# Understanding the structure of the predictions DataFrame\n",
    "print(\"Predictions DataFrame shape:\", df_predictions.shape)\n",
    "print(\"Predictions DataFrame columns:\", df_predictions.columns.tolist())\n",
    "print(\"\")\n",
    "\n",
    "print(f\"Structure of a Sequence_real cell:\")\n",
    "seq = df_predictions[\"Sequence_real\"].iloc[0]\n",
    "arr = np.array(seq)\n",
    "print(\"type:\", type(seq))\n",
    "print(\"numpy shape:\", arr.shape)\n",
    "# fallback / readable shape\n",
    "print(\"T (timesteps):\", len(seq), \"F (features per timestep):\", len(seq[0]) if len(seq) > 0 else 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eb052",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4cbdd",
   "metadata": {},
   "source": [
    "##### FIRST OF ALL --> Denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "46a489e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREDICTION_DF_PATH: eval/predictions_df.parquet\n",
      "PREDICTION_DENORM_DF_PATH: eval/predictions_denorm_df.parquet\n"
     ]
    }
   ],
   "source": [
    "print(f\"PREDICTION_DF_PATH: {PREDICTION_DF_PATH}\")                 # Path to predictions DataFrame, saved after evaluation\n",
    "print(f\"PREDICTION_DENORM_DF_PATH: {PREDICTION_DENORM_DF_PATH}\")   # Path to denormalized predictions DataFrame, to be saved after denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0621c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denormalized Predictions DataFrame shape: (1021, 13)\n",
      "Denormalized Predictions DataFrame columns: ['Segment_nr', 'MMSI', 'ShipTypeID', 'recon_error', 'real_Latitude', 'real_Longitude', 'real_SOG', 'real_COG', 'pred_Latitude', 'pred_Longitude', 'pred_SOG', 'pred_COG', 'ShipType']\n",
      "\n",
      "Structure of a real_Longitude cell:\n",
      "type: <class 'list'>\n",
      "numpy shape: (300,)\n"
     ]
    }
   ],
   "source": [
    "# Load predictions DataFrame from file\n",
    "df_predictions = pd.read_parquet(PREDICTION_DF_PATH)\n",
    "\n",
    "# Denormalize predictions\n",
    "df_denormalized = inspection_utils.denormalize_predictions(\n",
    "    df=df_predictions,\n",
    "    metadata_path=config.PRE_PROCESSING_METADATA_TEST_PATH,\n",
    ")\n",
    "\n",
    "# Understanding the structure of the denomarlized pedictions DataFrame\n",
    "print(\"Denormalized Predictions DataFrame shape:\", df_denormalized.shape)\n",
    "print(\"Denormalized Predictions DataFrame columns:\", df_denormalized.columns.tolist())\n",
    "print(\"\")\n",
    "print(f\"Structure of a real_Longitude cell:\")\n",
    "seq = df_denormalized[\"real_Longitude\"].iloc[0]\n",
    "arr = np.array(seq)\n",
    "print(\"type:\", type(seq))\n",
    "print(\"numpy shape:\", arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e610bcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Denormalized Predictions DataFrame saved to eval/predictions_denorm_df.parquet\n"
     ]
    }
   ],
   "source": [
    "#Save denormalized predictions DataFrame to file\n",
    "df_denormalized.to_parquet(PREDICTION_DENORM_DF_PATH, index=False)\n",
    "print(f\"Denormalized Predictions DataFrame saved to {PREDICTION_DENORM_DF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccf7a0",
   "metadata": {},
   "source": [
    "##### Plot on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de6c8e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eval/maps/ais_worst_first_10.html'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This function TAKES AS INPUT THE DENORMALIZED PREDICTIONS DATAFRAME and saves interactive HTML maps visualizing the AIS tracks and reconstruction errors.\n",
    "df_denormalized = pd.read_parquet(PREDICTION_DENORM_DF_PATH)\n",
    "\n",
    "# 1 single segment\n",
    "#inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_maps.html\", segment=7235, zoom_start=7)\n",
    "\n",
    "# random 8 tracks\n",
    "inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_maps.html\", n_random=8, zoom_start=7)\n",
    "\n",
    "# a single MMSI\n",
    "#inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_mmsi_219005866.html\", mmsi=219005866, zoom_start=8)\n",
    "\n",
    "# a list of MMSI\n",
    "#inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_some_mmsi.html\", mmsi=[219005866, 241455000], zoom_start=8)\n",
    "\n",
    "# head 10, best reconstructions\n",
    "df_denormalized = df_denormalized.sort_values(by=[\"recon_error\"]) \n",
    "inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_best_first_10.html\", head_n=10)\n",
    "# head 10, worst reconstructions\n",
    "df_denormalized = df_denormalized.sort_values(by=[\"recon_error\"], ascending=False)\n",
    "inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_worst_first_10.html\", head_n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
