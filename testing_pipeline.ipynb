{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5ea4a5ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import main_2_pre_processing\n",
    "from src.pre_proc import ais_query\n",
    "import config\n",
    "import pandas as pd\n",
    "import gc\n",
    "import main_2_pre_processing\n",
    "from src.pre_proc import ais_query\n",
    "import config\n",
    "import training_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "SEGMENT_MAX_LENGTH = config.SEGMENT_MAX_LENGTH # Maximum length of segments (minimum lenght is set during filtering, and should be equal to this, if we want fixed-length segments)\n",
    "NUMERIC_COLS = config.NUMERIC_COLS             # Numeric columns to be normalized\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE     # Start date for training data you want to PRE-PROCESS\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE         # End date for training data you want to PRE-PROCESS\n",
    "\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE       # Start date for test data you want to PRE-PROCESS\n",
    "TEST_END_DATE = config.TEST_END_DATE    \n",
    "\n",
    "NUMERIC_COLS = config.NUMERIC_COLS\n",
    "FEATURE_COLS = config.FEATURE_COLS\n",
    "\n",
    "PRE_PROCESSING_DF_TRAIN_PATH = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "PRE_PROCESSING_DF_TEST_PATH = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "\n",
    "WEIGHTS_PATH = config.WEIGHTS_PATH\n",
    "PLOT_PATH = config.PLOT_PATH\n",
    "PREDICTION_DF_PATH = config.PREDICTION_DF_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201443ae",
   "metadata": {},
   "source": [
    "## Saving pre-processed DF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6b1c5d9",
   "metadata": {},
   "source": [
    "##### Load data from filtered parquet database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b428c91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 555. MiB for an array with shape (10, 7276150) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Filtered AIS data is located at \"ais-data/parquet/\" and contains your filtered downloaded data (filtered based on bbox, ship types, etc.)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# Still contains NaN values and unprocessed columns\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m raw_df \u001b[38;5;241m=\u001b[39m \u001b[43mais_query\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery_ais_duckdb\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mais-data/parquet\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw DataFrame shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mraw_df\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRaw DataFrame columns:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mraw_df\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/src/pre_proc/ais_query.py:96\u001b[0m, in \u001b[0;36mquery_ais_duckdb\u001b[0;34m(root_path, dates, mmsi, segments, columns, verbose)\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[query_ais_duckdb] SQL:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, sql)\n\u001b[1;32m     95\u001b[0m con \u001b[38;5;241m=\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mconnect(database\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m:memory:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m con\u001b[38;5;241m.\u001b[39mclose()\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/frame.py:1926\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for orient parameter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1922\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1923\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1928\u001b[0m     realdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    777\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    778\u001b[0m     )\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/internals/construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/internals/managers.py:2163\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[1;32m   2161\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[0;32m-> 2163\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/internals/managers.py:1807\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[0;32m-> 1807\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/internals/managers.py:2288\u001b[0m, in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   2286\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[0;32m-> 2288\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213582/dark-vessel-hunter/dlproject/lib/python3.10/site-packages/pandas/core/internals/managers.py:2320\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   2317\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m bvals2[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39m_concat_same_type(bvals2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   2319\u001b[0m argsort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(new_mgr_locs)\n\u001b[0;32m-> 2320\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnew_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m   2321\u001b[0m new_mgr_locs \u001b[38;5;241m=\u001b[39m new_mgr_locs[argsort]\n\u001b[1;32m   2323\u001b[0m bp \u001b[38;5;241m=\u001b[39m BlockPlacement(new_mgr_locs)\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 555. MiB for an array with shape (10, 7276150) and data type object"
     ]
    }
   ],
   "source": [
    "# Filtered AIS data is located at \"ais-data/parquet/\" and contains your filtered downloaded data (filtered based on bbox, ship types, etc.)\n",
    "# Still contains NaN values and unprocessed columns\n",
    "raw_df = ais_query.query_ais_duckdb(\"ais-data/parquet\", verbose=True)\n",
    "print(f\"Raw DataFrame shape: {raw_df.shape}\")\n",
    "print(f\"Raw DataFrame columns:\\n{raw_df.columns.tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83ec4ab",
   "metadata": {},
   "source": [
    "##### Look at how many records and unique days are in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4a4f0b8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'raw_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m unique_days \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(\u001b[43mraw_df\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mdt\u001b[38;5;241m.\u001b[39mnormalize()\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m      2\u001b[0m unique_days \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mto_datetime(unique_days)\u001b[38;5;241m.\u001b[39msort_values()\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mData loaded: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(raw_df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m records.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'raw_df' is not defined"
     ]
    }
   ],
   "source": [
    "unique_days = pd.Series(raw_df['Timestamp'].dt.normalize().unique())\n",
    "unique_days = pd.to_datetime(unique_days).sort_values()\n",
    "print(f\"Data loaded: {len(raw_df)} records.\")\n",
    "#print(\"Unique days:\", [d.strftime('%Y-%m-%d') for d in unique_days])\n",
    "\n",
    "# Print continuous periods (split when a day is missing)\n",
    "if unique_days.empty:\n",
    "    print(\"No days found in data.\")\n",
    "else:\n",
    "    days = pd.to_datetime(unique_days).sort_values().reset_index(drop=True)\n",
    "    diffs = days.diff().dt.days\n",
    "    groups = (diffs.fillna(1) != 1).cumsum()\n",
    "    periods = [(grp.iloc[0], grp.iloc[-1]) for _, grp in days.groupby(groups)]\n",
    "\n",
    "    print(\"Continuous periods (split when a day is missing):\")\n",
    "    for start, end in periods:\n",
    "        if start == end:\n",
    "            print(start.strftime(\"%Y-%m-%d\"))\n",
    "        else:\n",
    "            length = (end - start).days + 1\n",
    "            print(f\"{start.strftime('%Y-%m-%d')} -> {end.strftime('%Y-%m-%d')} ({length} days)\")\n",
    "    print(f\"Total periods: {len(periods)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba6fa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Free up memory\n",
    "del raw_df\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7f5bb8",
   "metadata": {},
   "source": [
    "##### Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42e7a1bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SEGMENT_MAX_LENGTH: 300\n",
      "NUMERIC_COLS: ['Latitude', 'Longitude', 'SOG', 'COG']\n",
      "TRAIN_START_DATE: 2025-08-01\n",
      "TRAIN_END_DATE: 2025-08-31\n",
      "TEST_START_DATE: 2025-09-01\n",
      "TEST_END_DATE: 2025-09-03\n"
     ]
    }
   ],
   "source": [
    "# Setup this parameters at your convenience into config.py\n",
    "\n",
    "SEGMENT_MAX_LENGTH = config.SEGMENT_MAX_LENGTH # Maximum length of segments (minimum lenght is set during filtering, and should be equal to this, if we want fixed-length segments)\n",
    "NUMERIC_COLS = config.NUMERIC_COLS             # Numeric columns to be normalized\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE     # Start date for training data you want to PRE-PROCESS\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE         # End date for training data you want to PRE-PROCESS\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE       # Start date for test data you want to PRE-PROCESS\n",
    "TEST_END_DATE = config.TEST_END_DATE           # End date for test data you want to PRE-PROCESS\n",
    "\n",
    "print(f\"SEGMENT_MAX_LENGTH: {SEGMENT_MAX_LENGTH}\")\n",
    "print(f\"NUMERIC_COLS: {NUMERIC_COLS}\")\n",
    "print(f\"TRAIN_START_DATE: {TRAIN_START_DATE}\")\n",
    "print(f\"TRAIN_END_DATE: {TRAIN_END_DATE}\")\n",
    "print(f\"TEST_START_DATE: {TEST_START_DATE}\")\n",
    "print(f\"TEST_END_DATE: {TEST_END_DATE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e3bb6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for training period: 2025-08-01 to 2025-08-31\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1 AND Date IN ('2025-08-01', '2025-08-02', '2025-08-03', '2025-08-04', '2025-08-05', '2025-08-06', '2025-08-07', '2025-08-08', '2025-08-09', '2025-08-10', '2025-08-11', '2025-08-12', '2025-08-13', '2025-08-14', '2025-08-15', '2025-08-16', '2025-08-17', '2025-08-18', '2025-08-19', '2025-08-20', '2025-08-21', '2025-08-22', '2025-08-23', '2025-08-24', '2025-08-25', '2025-08-26', '2025-08-27', '2025-08-28', '2025-08-29', '2025-08-30', '2025-08-31')\n",
      "[pre_processing] Initial data size: 3996082 records.\n",
      "[pre_processing] Dropping unnecessary columns and rows with missing values...\n",
      "[pre_processing] Data size after dropping: 3925808 records.\n",
      "[pre_processing] Splitting segments to max length 300...\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_train.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG', 'MMSI', 'Segment_nr', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/df_preprocessed/pre_processing_metadata_train.json...\n"
     ]
    }
   ],
   "source": [
    "main_2_pre_processing.main_pre_processing(dataframe_type=\"train\")\n",
    "# The pre-processed DataFrame is now saved to the path specified in config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "\n",
    "# YOU CAN ALSO PRE_PROCESS JUST THE TRAIN DATAFRAME AND THEN SPLIT IT IN TRAIN/TEST IF YOU PREFER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cbabc020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for testing period: 2025-09-01 to 2025-09-03\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1 AND Date IN ('2025-09-01', '2025-09-02', '2025-09-03')\n",
      "[pre_processing] Initial data size: 367732 records.\n",
      "[pre_processing] Dropping unnecessary columns and rows with missing values...\n",
      "[pre_processing] Data size after dropping: 367230 records.\n",
      "[pre_processing] Splitting segments to max length 300...\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_test.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG', 'MMSI', 'Segment_nr', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/df_preprocessed/pre_processing_metadata_test.json...\n"
     ]
    }
   ],
   "source": [
    "main_2_pre_processing.main_pre_processing(dataframe_type=\"test\")\n",
    "# The pre-processed DataFrame is now saved to the path specified in config.PRE_PROCESSING_DF_TEST_PATH"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6649daf",
   "metadata": {},
   "source": [
    "## Model Loading and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1cca9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gc\n",
    "import main_2_pre_processing\n",
    "from src.pre_proc import ais_query\n",
    "import config\n",
    "import training_utils\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d5ca025",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMERIC_COLS = config.NUMERIC_COLS\n",
    "FEATURE_COLS = config.FEATURE_COLS\n",
    "\n",
    "PRE_PROCESSING_DF_TRAIN_PATH = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "PRE_PROCESSING_DF_TEST_PATH = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "\n",
    "\n",
    "# Load pre-processed training DataFrame from file to  ---> sequences\n",
    "df_seq_train = training_utils.load_df_seq(PRE_PROCESSING_DF_TRAIN_PATH)\n",
    "\n",
    "df_seq_train, df_seq_val = train_test_split(\n",
    "        df_seq_train,\n",
    "        test_size=0.2,\n",
    "        random_state=5,\n",
    "    )\n",
    "\n",
    "\n",
    "# df_seq is a dict with the following structure:   \n",
    "# {       \n",
    "#         \"Segment_nr\": seg_id,\n",
    "#         \"MMSI\": mmsi,\n",
    "#         \"ShipTypeID\": ship_type_id,\n",
    "#         \"FirstTimestamp\": first_timestamp,\n",
    "#         \"Sequence\": X,\n",
    "# })                                                    if you want to modify it, look at training_utils.py load_df_seq function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "123d9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24915434",
   "metadata": {},
   "source": [
    "##### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b5294e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AISTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapping df_seq:\n",
    "      - Sequence: list-of-lists (T, F) or np.ndarray\n",
    "      - ShipTypeID: integer class id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_seq: pd.DataFrame):\n",
    "        self.df = df_seq.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = np.array(row[\"Sequence\"], dtype=np.float32)  # (T, F)\n",
    "        x = torch.from_numpy(seq)                         # (T, F)\n",
    "\n",
    "        ship_type_id = int(row[\"ShipTypeID\"])\n",
    "        ship_type_id = torch.tensor(ship_type_id, dtype=torch.long)\n",
    "\n",
    "        return x, ship_type_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0b9f9c4b",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_seq_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m AISTrajectoryDataset(\u001b[43mdf_seq_train\u001b[49m)\n\u001b[1;32m      2\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m AISTrajectoryDataset(df_seq_val) \u001b[38;5;66;03m# useful for early stopping during training\u001b[39;00m\n\u001b[1;32m      4\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\n\u001b[1;32m      5\u001b[0m     train_dataset,\n\u001b[1;32m      6\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      7\u001b[0m     shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m      8\u001b[0m     drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m      9\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_seq_train' is not defined"
     ]
    }
   ],
   "source": [
    "train_dataset = AISTrajectoryDataset(df_seq_train)\n",
    "val_dataset = AISTrajectoryDataset(df_seq_val) # useful for early stopping during training\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "\n",
    "# Infer sequence shape and num_shiptypes\n",
    "sample_x, _ = train_dataset[0]\n",
    "T, F = sample_x.shape\n",
    "num_shiptypes = df_seq_train[\"ShipTypeID\"].nunique()\n",
    "print(f\"Sequence shape: T={T}, F={F}, num_shiptypes={num_shiptypes}\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9360a195",
   "metadata": {},
   "source": [
    "##### Model class (import your model here)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e960ac0a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'F' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 6\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodelAE\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LSTMAutoencoderWithShipType\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Instantiate the model\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMAutoencoderWithShipType(\n\u001b[0;32m----> 6\u001b[0m     input_dim\u001b[38;5;241m=\u001b[39m\u001b[43mF\u001b[49m,\n\u001b[1;32m      7\u001b[0m     hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[1;32m      8\u001b[0m     latent_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m,\n\u001b[1;32m      9\u001b[0m     num_shiptypes\u001b[38;5;241m=\u001b[39mnum_shiptypes,\n\u001b[1;32m     10\u001b[0m     shiptype_emb_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m8\u001b[39m,\n\u001b[1;32m     11\u001b[0m     num_layers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     12\u001b[0m     dropout\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m,\n\u001b[1;32m     13\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'F' is not defined"
     ]
    }
   ],
   "source": [
    "# Import\n",
    "from modelAE import LSTMAutoencoderWithShipType\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMAutoencoderWithShipType(\n",
    "    input_dim=F,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    num_shiptypes=num_shiptypes,\n",
    "    shiptype_emb_dim=8,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1253704d",
   "metadata": {},
   "source": [
    "##### Loss function \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8b68a162",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def sequence_loss_fn(\n",
    "    recon_batch: torch.Tensor,  # (B, T, F)\n",
    "    x_batch: torch.Tensor,      # (B, T, F)\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Returns loss per sequence: shape (B,).\n",
    "    Here you can easily change the definition (MSE, MAE, weights, etc.).\n",
    "    \"\"\"\n",
    "    mse = F.mse_loss(recon_batch, x_batch, reduction=\"none\")  # (B, T, F)\n",
    "    # mean on time and feature, keeping batch separate\n",
    "    seq_error = mse.mean(dim=(1, 2))  # (B,)\n",
    "    return seq_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd082ed",
   "metadata": {},
   "source": [
    "##### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d590b5d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - train MSE: 0.159706 - val MSE: 0.015657\n",
      "Epoch 2/10 - train MSE: 0.011318 - val MSE: 0.006591\n",
      "Epoch 3/10 - train MSE: 0.003598 - val MSE: 0.001126\n",
      "Epoch 4/10 - train MSE: 0.000710 - val MSE: 0.000477\n",
      "Epoch 5/10 - train MSE: 0.000387 - val MSE: 0.000328\n",
      "Epoch 6/10 - train MSE: 0.000319 - val MSE: 0.000346\n",
      "Epoch 7/10 - train MSE: 0.000285 - val MSE: 0.000254\n",
      "Epoch 8/10 - train MSE: 0.000248 - val MSE: 0.000301\n",
      "Epoch 9/10 - train MSE: 0.000246 - val MSE: 0.000205\n",
      "Epoch 10/10 - train MSE: 0.000187 - val MSE: 0.000166\n"
     ]
    }
   ],
   "source": [
    "from torch.nn.utils import clip_grad_norm_\n",
    "max_grad_norm = 1.0 # Maximum gradient norm for gradient clipping (prevent exploding gradients, can be adjusted)\n",
    "\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "history = {\"train_loss\": [], \"val_loss\": []}\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # ---- Train ----\n",
    "    model.train()\n",
    "    train_loss_sum = 0.0\n",
    "\n",
    "    for x, ship_type_id in train_loader:\n",
    "        x = x.to(device)\n",
    "        ship_type_id = ship_type_id.to(device)\n",
    "\n",
    "        recon, _ = model(x, ship_type_id)\n",
    "        seq_errors = sequence_loss_fn(recon, x) \n",
    "        loss = seq_errors.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "    # Total train loss noemralized by number of samples (a batch could have different size, so we sum and normalize here)\n",
    "    train_loss = train_loss_sum / len(train_loader.dataset)\n",
    "    history[\"train_loss\"].append(train_loss)\n",
    "\n",
    "    # ---- Validation ---- # Useful for early stopping/model selection\n",
    "    model.eval()\n",
    "    val_loss_sum = 0.0\n",
    "    with torch.no_grad():\n",
    "        for x, ship_type_id in val_loader:\n",
    "            x = x.to(device)\n",
    "            ship_type_id = ship_type_id.to(device)\n",
    "\n",
    "            recon, _ = model(x, ship_type_id)\n",
    "            seq_errors = sequence_loss_fn(recon, x)\n",
    "            loss = seq_errors.mean()\n",
    "            \n",
    "            val_loss_sum += loss.item() * x.size(0)\n",
    "\n",
    "    val_loss = val_loss_sum / len(val_loader.dataset)\n",
    "    history[\"val_loss\"].append(val_loss)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch}/{epochs} - \"\n",
    "        f\"train MSE: {train_loss:.6f} - val MSE: {val_loss:.6f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "729ce3c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model weights saved to models/dark_vessel_model.pth\n"
     ]
    }
   ],
   "source": [
    "WEIGHTS_PATH = config.WEIGHTS_PATH\n",
    "torch.save(model.state_dict(), WEIGHTS_PATH)\n",
    "print(f\"Model weights saved to {WEIGHTS_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b18db1e",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e671cbf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WEIGHTS_PATH: models/dark_vessel_model.pth\n",
      "PLOT_PATH: eval/plots\n",
      "PREDICTION_DF_PATH: eval/val_predictions.parquet\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "\n",
    "WEIGHTS_PATH = config.WEIGHTS_PATH\n",
    "PLOT_PATH = config.PLOT_PATH\n",
    "PREDICTION_DF_PATH = config.PREDICTION_DF_PATH\n",
    "\n",
    "print(\"WEIGHTS_PATH:\", WEIGHTS_PATH)\n",
    "print(\"PLOT_PATH:\", PLOT_PATH)\n",
    "print(\"PREDICTION_DF_PATH:\", PREDICTION_DF_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21dbf2bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import training_utils\n",
    "\n",
    "# Load pre-processed TEST DataFrame from file to  ---> sequences\n",
    "# CAN ALSO BE THE TRAIN/VAL SPLIT IF YOU PREFER, JUST TO SEE HOW IT RECONSTRUCTS KNOWN DATA\n",
    "df_seq_test = training_utils.load_df_seq(PRE_PROCESSING_DF_TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8dcf3da",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_seq_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m sample_x, _ \u001b[38;5;241m=\u001b[39m test_dataset[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     36\u001b[0m T, F \u001b[38;5;241m=\u001b[39m sample_x\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m---> 37\u001b[0m num_shiptypes \u001b[38;5;241m=\u001b[39m \u001b[43mdf_seq_train\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShipTypeID\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mnunique()\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSequence shape: T=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mT\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, F=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mF\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, num_shiptypes=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_shiptypes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Device\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_seq_train' is not defined"
     ]
    }
   ],
   "source": [
    "# Load test dataset and dataloader\n",
    "class AISTrajectoryDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset wrapping df_seq:\n",
    "      - Sequence: list-of-lists (T, F) or np.ndarray\n",
    "      - ShipTypeID: integer class id\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df_seq: pd.DataFrame):\n",
    "        self.df = df_seq.reset_index(drop=True)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        row = self.df.iloc[idx]\n",
    "        seq = np.array(row[\"Sequence\"], dtype=np.float32)  # (T, F)\n",
    "        x = torch.from_numpy(seq)                         # (T, F)\n",
    "\n",
    "        ship_type_id = int(row[\"ShipTypeID\"])\n",
    "        ship_type_id = torch.tensor(ship_type_id, dtype=torch.long)\n",
    "\n",
    "        return x, ship_type_id\n",
    "\n",
    "test_dataset = AISTrajectoryDataset(df_seq_test)\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "\n",
    "# Infer sequence shape and num_shiptypes\n",
    "sample_x, _ = test_dataset[0]\n",
    "T, F = sample_x.shape\n",
    "num_shiptypes = df_seq_train[\"ShipTypeID\"].nunique()\n",
    "print(f\"Sequence shape: T={T}, F={F}, num_shiptypes={num_shiptypes}\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e02d39",
   "metadata": {},
   "source": [
    "##### Put here your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6113b766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "from modelAE import LSTMAutoencoderWithShipType\n",
    "\n",
    "# Instantiate the model\n",
    "model = LSTMAutoencoderWithShipType(\n",
    "    input_dim=F,\n",
    "    hidden_dim=128,\n",
    "    latent_dim=64,\n",
    "    num_shiptypes=num_shiptypes,\n",
    "    shiptype_emb_dim=8,\n",
    "    num_layers=1,\n",
    "    dropout=0.3,\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "83b674c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMAutoencoderWithShipType(\n",
       "  (encoder): LSTM(4, 128, batch_first=True)\n",
       "  (fc_latent): Linear(in_features=128, out_features=64, bias=True)\n",
       "  (shiptype_emb): Embedding(17, 8)\n",
       "  (fc_z_st_to_h): Linear(in_features=72, out_features=128, bias=True)\n",
       "  (decoder): LSTM(4, 128, batch_first=True)\n",
       "  (fc_out): Linear(in_features=128, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved model weights\n",
    "model.load_state_dict(torch.load(WEIGHTS_PATH, map_location=device))\n",
    "model.to(device)\n",
    "model.eval() # put model in evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13106daf",
   "metadata": {},
   "source": [
    "##### Plots\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0139bad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Reconstruction errors on test ===\n",
      "Test:\n",
      "  mean: 0.000228\n",
      "  std:  0.000564\n",
      "  q95:  0.000799\n",
      "  q99:  0.001409\n",
      "  max:  0.012264\n",
      "\n",
      "Suggested anomaly threshold (99th percentile of test): 0.001409\n",
      "Plots saved in directory: \"eval/plots\"\n",
      "Evaluation results: {'test_scores': array([1.1595870e-04, 3.2631713e-05, 8.8864195e-05, ..., 2.9249548e-05,\n",
      "       2.4567564e-05, 1.4920163e-04], shape=(1021,), dtype=float32), 'threshold_99': np.float32(0.0014091902)}\n"
     ]
    }
   ],
   "source": [
    "import evaluation_utils\n",
    "# Save some plots and get evaluation results about reconstruction errors\n",
    "results = evaluation_utils.make_plots(model=model, test_loader=test_loader, device=device, seq_loss_fn=sequence_loss_fn)\n",
    "print(\"Evaluation results:\", results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d086f84",
   "metadata": {},
   "source": [
    "##### Prediction df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "38a36a66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions DataFrame shape: (1021, 6)\n",
      "Predictions DataFrame columns: ['Segment_nr', 'MMSI', 'ShipTypeID', 'Sequence_real', 'Sequence_pred', 'recon_error']\n",
      "Predictions DataFrame saved to eval/val_predictions.parquet\n"
     ]
    }
   ],
   "source": [
    "# Build predictions DataFrame for further analysis\n",
    "df_predictions = evaluation_utils.build_predictions_df(model=model, dataset=test_loader.dataset, device=device, seq_loss_fn=sequence_loss_fn)\n",
    "print(\"Predictions DataFrame shape:\", df_predictions.shape)\n",
    "print(\"Predictions DataFrame columns:\", df_predictions.columns.tolist())\n",
    "\n",
    "# Save predictions DataFrame to file\n",
    "df_predictions.to_parquet(PREDICTION_DF_PATH, index=False)\n",
    "print(f\"Predictions DataFrame saved to {PREDICTION_DF_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0eb052",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ffbc710e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Free up memory\n",
    "del df_seq_train\n",
    "del df_seq_val\n",
    "del df_seq_test\n",
    "del df_predictions\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f4cbdd",
   "metadata": {},
   "source": [
    "##### Denormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0621c586",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> raw normalized real_lon (prima della denorm)\n",
      "[0.006969360169023275, 0.008903004229068756, 0.010337387211620808, 0.012496885843575, 0.014291846193373203, 0.016035296022892, 0.017628174275159836, 0.019609365612268448, 0.021055635064840317, 0.023064564913511276, 0.025025945156812668, 0.026832792907953262, 0.028453407809138298, 0.030398936942219734, 0.03180558234453201, 0.03374319151043892, 0.03573627024888992, 0.03573627024888992, 0.037578776478767395, 0.039254866540431976, 0.04125586897134781, 0.043034981936216354, 0.04463182017207146, 0.04659320041537285, 0.048404011875391006, 0.05004047602415085, 0.05200185626745224, 0.05379285290837288, 0.05543724447488785, 0.05743032321333885, 0.059225283563137054, 0.06088552251458168, 0.0628984123468399, 0.06470922380685806, 0.06637738645076752, 0.07024863362312317, 0.07188510149717331, 0.07575634866952896, 0.07740073651075363, 0.07917588949203491, 0.08116500079631805, 0.0828014686703682, 0.08462416380643845, 0.08665290474891663, 0.08830522000789642, 0.0901477262377739, 0.09222005307674408, 0.09387236833572388, 0.09567921608686447, 0.09768814593553543, 0.0995425432920456, 0.10125429183244705, 0.10125429183244705, 0.10333454608917236, 0.1050383672118187, 0.1069205030798912, 0.10894131660461426, 0.11079175025224686, 0.11247576028108597, 0.11438959091901779, 0.11647777259349823, 0.11817367374897003, 0.12022222578525543, 0.1220766231417656, 0.12379233539104462, 0.12571409344673157, 0.12776264548301697, 0.1294347643852234, 0.13147935271263123, 0.13338921964168549, 0.13512079417705536, 0.13700687885284424, 0.1390475183725357, 0.14072756469249725, 0.14276818931102753, 0.1446225792169571, 0.1463184803724289, 0.14821647107601166, 0.15026898682117462, 0.1521233767271042, 0.15401344001293182, 0.15592330694198608, 0.15763109922409058, 0.1595132201910019, 0.16137555241584778, 0.16347165405750275, 0.16539736092090607, 0.16730722784996033, 0.16880501806735992, 0.16880501806735992, 0.17275550961494446, 0.17482784390449524, 0.17669808864593506, 0.1785881370306015, 0.18023648858070374, 0.18205523490905762, 0.18385018408298492, 0.18587100505828857, 0.18752728402614594, 0.18954414129257202, 0.191220223903656, 0.19330839812755585, 0.19499637186527252, 0.19685474038124084, 0.19874875247478485, 0.20082899928092957, 0.20248131453990936, 0.204502135515213, 0.20636841654777527, 0.20809204876422882, 0.21000193059444427, 0.2120702862739563, 0.21371467411518097, 0.2157156765460968, 0.21753837168216705, 0.2192065417766571, 0.22109660506248474, 0.22316496074199677, 0.22486086189746857, 0.22688961029052734, 0.22873607277870178, 0.2303963154554367, 0.2322903275489807, 0.2343626618385315, 0.2361021488904953, 0.2361021488904953, 0.23822598159313202, 0.24011604487895966, 0.2418079823255539, 0.24368219077587128, 0.2457822561264038, 0.24749399721622467, 0.24957425892353058, 0.2514563798904419, 0.2531839907169342, 0.2551017701625824, 0.2571741044521332, 0.25885018706321716, 0.26089081168174744, 0.26259464025497437, 0.26453620195388794, 0.26643815636634827, 0.2685461640357971, 0.27023017406463623, 0.2722628712654114, 0.2739429175853729, 0.276003360748291, 0.277679443359375, 0.27970024943351746, 0.2813129425048828, 0.2833179235458374, 0.2849821150302887, 0.28699105978012085, 0.2886076867580414, 0.290584921836853, 0.2922095060348511, 0.29421448707580566, 0.2958390414714813, 0.29780834913253784, 0.299440860748291, 0.301493376493454, 0.303343802690506, 0.303343802690506, 0.3049802780151367, 0.3067910671234131, 0.3088237941265106, 0.3104800581932068, 0.3125167191028595, 0.314398854970932, 0.31606701016426086, 0.3178778290748596, 0.3198511004447937, 0.32147568464279175, 0.3234845995903015, 0.32529541850090027, 0.3269200026988983, 0.3287268280982971, 0.33070802688598633, 0.3323880732059479, 0.3344128727912903, 0.33625537157058716, 0.3379473090171814, 0.33980169892311096, 0.341802716255188, 0.34342727065086365, 0.34540846943855286, 0.3470330536365509, 0.34885576367378235, 0.3527190685272217, 0.35437139868736267, 0.35638031363487244, 0.3580048978328705, 0.35977211594581604, 0.36175331473350525, 0.3635562062263489, 0.3651411533355713, 0.3669242262840271, 0.36872315406799316, 0.36872315406799316, 0.3707360327243805, 0.37257853150367737, 0.3743933141231537, 0.3760773241519928, 0.3779594600200653, 0.37981781363487244, 0.381822794675827, 0.38366132974624634, 0.3871482312679291, 0.3889828026294708, 0.39081740379333496, 0.3928183913230896, 0.39461731910705566, 0.3962537944316864, 0.3981042206287384, 0.39995068311691284, 0.4017733931541443, 0.4037545919418335, 0.40556538105010986, 0.4072216749191284, 0.40909191966056824, 0.41095423698425293, 0.4127769470214844, 0.41478586196899414, 0.41658875346183777, 0.4182251989841461, 0.42003998160362244, 0.4218309819698334, 0.42383989691734314, 0.42566657066345215, 0.4274456799030304, 0.4290306270122528, 0.4308136999607086, 0.4326403737068176, 0.4346809983253479, 0.4346809983253479, 0.43833431601524353, 0.4399470090866089, 0.44192028045654297, 0.44353294372558594, 0.4454864263534546, 0.4472813606262207, 0.44908425211906433, 0.4508831799030304, 0.45263850688934326, 0.4544215798377991, 0.4562323987483978, 0.45786887407302856, 0.459889680147171, 0.4617004990577698, 0.46354299783706665, 0.4653854966163635, 0.46720027923583984, 0.46885257959365845, 0.4704890549182892, 0.47262874245643616, 0.4744197428226471, 0.47625431418418884, 0.477886825799942, 0.4796817898750305, 0.48131826519966125, 0.48345398902893066, 0.4870399236679077, 0.48864468932151794, 0.49043965339660645, 0.49209198355674744, 0.4942752420902252, 0.49570170044898987, 0.4978691339492798, 0.49949371814727783, 0.5012688636779785, 0.5028696656227112, 0.5050410628318787, 0.5085992813110352, 0.5102000832557678, 0.5119950175285339, 0.5135998129844666, 0.515771210193634, 0.5193254351615906, 0.5209183096885681, 0.5226934552192688, 0.5242506861686707, 0.526350736618042, 0.5277771949768066, 0.529877245426178, 0.5316365957260132, 0.533245325088501, 0.5348698496818542, 0.5369897484779358, 0.5383765697479248, 0.5405162572860718, 0.5423191785812378, 0.5439714789390564, 0.5455960631370544, 0.5494910478591919, 0.5513098239898682, 0.5531246066093445, 0.5547293424606323, 0.5567105412483215, 0.558533251285553, 0.5603361129760742]\n",
      "len: 300\n",
      "unique (norm): [0.006969360169023275, 0.008903004229068756, 0.010337387211620808, 0.012496885843575, 0.014291846193373203, 0.016035296022892, 0.017628174275159836, 0.019609365612268448, 0.021055635064840317, 0.023064564913511276]\n",
      "mean_lon: 11.285771119461316 std_lon: 0.25237330967966404\n",
      "denorm_example: [11.287529999953522, 11.288018000104698, 11.288380000085352, 11.288924999902347, 11.28937799998657, 11.289818000190307, 11.290220000146748, 11.290719999961603, 11.291084999770037, 11.29159200004486]\n",
      "unique (denorm): [11.287529999953522, 11.288018000104698, 11.288380000085352, 11.288924999902347, 11.28937799998657, 11.289818000190307, 11.290220000146748, 11.290719999961603, 11.291084999770037, 11.29159200004486]\n"
     ]
    }
   ],
   "source": [
    "import inspection_utils\n",
    "import pandas as pd\n",
    "from inspection_utils import denormalize_predictions\n",
    "\n",
    "import pandas as pd\n",
    "import gc\n",
    "import main_2_pre_processing\n",
    "from src.pre_proc import ais_query\n",
    "import config\n",
    "import pandas as pd\n",
    "import gc\n",
    "import main_2_pre_processing\n",
    "from src.pre_proc import ais_query\n",
    "import config\n",
    "import training_utils\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import os\n",
    "\n",
    "\n",
    "SEGMENT_MAX_LENGTH = config.SEGMENT_MAX_LENGTH # Maximum length of segments (minimum lenght is set during filtering, and should be equal to this, if we want fixed-length segments)\n",
    "NUMERIC_COLS = config.NUMERIC_COLS             # Numeric columns to be normalized\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE     # Start date for training data you want to PRE-PROCESS\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE         # End date for training data you want to PRE-PROCESS\n",
    "\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE       # Start date for test data you want to PRE-PROCESS\n",
    "TEST_END_DATE = config.TEST_END_DATE    \n",
    "\n",
    "NUMERIC_COLS = config.NUMERIC_COLS\n",
    "FEATURE_COLS = config.FEATURE_COLS\n",
    "\n",
    "PRE_PROCESSING_DF_TRAIN_PATH = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "PRE_PROCESSING_DF_TEST_PATH = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "\n",
    "WEIGHTS_PATH = config.WEIGHTS_PATH\n",
    "PLOT_PATH = config.PLOT_PATH\n",
    "PREDICTION_DF_PATH = config.PREDICTION_DF_PATH\n",
    "\n",
    "df_predictions = pd.read_parquet(PREDICTION_DF_PATH)\n",
    "# print(\"Loaded Predictions DataFrame shape:\", df_predictions.shape)\n",
    "# print(df_predictions.head().to_string)\n",
    "\n",
    "\n",
    "df_denormalized = denormalize_predictions(\n",
    "    df=df_predictions,\n",
    "    metadata_path=config.PRE_PROCESSING_METADATA_TEST_PATH,\n",
    ")\n",
    "\n",
    "# print(\"Denormalized Predictions DataFrame head: \")\n",
    "# print(df_denormalized.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ccf7a0",
   "metadata": {},
   "source": [
    "##### Plot on map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "de6c8e33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'eval/maps/ais_worst_first_10.html'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import inspection_utils\n",
    "import config\n",
    "MAPS_PATH = config.MAPS_PATH\n",
    "\n",
    "# un singolo segmento\n",
    "#inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_maps.html\", segment=7235, zoom_start=7)\n",
    "\n",
    "# random 8 tracce\n",
    "inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_maps.html\", n_random=8, zoom_start=7)\n",
    "\n",
    "# un singolo MMSI\n",
    "#inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_mmsi_219005866.html\", mmsi=219005866, zoom_start=8)\n",
    "\n",
    "# lista di MMSI\n",
    "#inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_some_mmsi.html\", mmsi=[219005866, 241455000], zoom_start=8)\n",
    "\n",
    "# head 10\n",
    "df_denormalized = df_denormalized.sort_values(by=[\"recon_error\"]) \n",
    "inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_best_first_10.html\", head_n=10)\n",
    "# head 10\n",
    "df_denormalized = df_denormalized.sort_values(by=[\"recon_error\"], ascending=False)\n",
    "inspection_utils.save_interactive_html(df_denormalized, out_html=f\"{MAPS_PATH}/ais_worst_first_10.html\", head_n=10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlproject",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
