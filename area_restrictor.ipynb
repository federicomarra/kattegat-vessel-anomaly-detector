{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "33cfbad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from shapely.geometry import Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf564568",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ais_filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e8e7740",
   "metadata": {},
   "source": [
    "## FUNCTIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "499aabc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mini_cleaning_pipeline(df: pd.DataFrame) -> pd.DataFrame:\n",
    "\n",
    "    df = df[df[\"Type of mobile\"].isin([\"Class A\", \"Class B\"])].drop(columns=[\"Type of mobile\"])\n",
    "\n",
    "    df = df.rename(columns={\"# Timestamp\": \"Timestamp\"})\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"%d/%m/%Y %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "    df = df.drop_duplicates([\"Timestamp\", \"MMSI\", ], keep=\"first\")\n",
    "    df = df.drop(columns=df.columns[-5:])\n",
    "\n",
    "    knots_to_ms = 0.514444\n",
    "    df[\"SOG\"] = knots_to_ms * df[\"SOG\"]\n",
    "    \n",
    "    return df\n",
    "    \n",
    "def quick_summary(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Generate a quick summary of the AIS data in the DataFrame.\n",
    "    Args:\n",
    "        df: DataFrame with AIS data, must contain 'MMSI', 'Timestamp', 'Latitude', 'Longitude' columns.\n",
    "        \n",
    "    Returns:\n",
    "        None: Prints summary statistics.\n",
    "        \"\"\"\n",
    "    # Number of unique vessels\n",
    "    num_vessels = int(df['MMSI'].nunique())\n",
    "    df = df.rename(columns={\"# Timestamp\": \"Timestamp\"})\n",
    "    df[\"Timestamp\"] = pd.to_datetime(df[\"Timestamp\"], format=\"%d/%m/%Y %H:%M:%S\", errors=\"coerce\")\n",
    "\n",
    "    # Spatial extent\n",
    "    lat_min, lat_max = df['Latitude'].min(), df['Latitude'].max()\n",
    "    lon_min, lon_max = df['Longitude'].min(), df['Longitude'].max()\n",
    "    centroid_lat, centroid_lon = df['Latitude'].mean(), df['Longitude'].mean()\n",
    "\n",
    "    # Messages per vessel\n",
    "    msgs_per_vessel = df.groupby('MMSI').size()\n",
    "    msgs_stats = msgs_per_vessel.describe().to_dict()\n",
    "    top_10_vessels = msgs_per_vessel.sort_values(ascending=False).head(10)\n",
    "\n",
    "    # Print concise overview\n",
    "    print(f\"Number of unique vessels: {num_vessels}\")\n",
    "    print(f\"Latitude range: {lat_min:.6f} -- {lat_max:.6f}\")\n",
    "    print(f\"Longitude range: {lon_min:.6f} -- {lon_max:.6f}\")\n",
    "    print(f\"Centroid (lat, lon): ({centroid_lat:.6f}, {centroid_lon:.6f})\")\n",
    "    print(\"\\nMessages per vessel (summary):\")\n",
    "    for k, v in msgs_stats.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "    print(\"\\nTop 10 vessels by number of messages (MMSI: count):\")\n",
    "    print(top_10_vessels.to_string())\n",
    "\n",
    "\n",
    "def singularize_vessels(df, mmsi_col=\"MMSI\", start_col_index=11, join_conflicts=True, sep=\" | \"):\n",
    "    cols = list(df.columns[start_col_index:])\n",
    "    if mmsi_col not in df.columns:\n",
    "        raise KeyError(f\"MMSI column '{mmsi_col}' not found in dataframe\")\n",
    "\n",
    "    def _agg(series):\n",
    "        vals = series.dropna().unique().tolist()\n",
    "        if len(vals) == 0:\n",
    "            return np.nan\n",
    "        if len(vals) == 1:\n",
    "            return vals[0]\n",
    "        if join_conflicts:\n",
    "            return sep.join(map(str, vals))\n",
    "        return vals\n",
    "\n",
    "    grouped = df.groupby(mmsi_col)[cols].agg(_agg).reset_index()\n",
    "    ordered_cols = [mmsi_col] + cols\n",
    "    return grouped[ordered_cols]\n",
    "\n",
    "def filter_inside_square(df, bbox) -> pd.DataFrame:\n",
    "    north, west, south, east = bbox\n",
    "    df = df[(df[\"Latitude\"] <= north) & (df[\"Latitude\"] >= south) & (df[\"Longitude\"] >= west) & (df[\"Longitude\"] <= east)] \n",
    "    return df\n",
    "\n",
    "def is_inside_polygon(lat, lon, polygon_coords):\n",
    "    \"\"\"\n",
    "    Check if a point (lat, lon) is inside a polygon.\n",
    "    \n",
    "    Args:\n",
    "        lat: Latitude\n",
    "        lon: Longitude\n",
    "        polygon_coords: List of (lon, lat) tuples defining polygon vertices\n",
    "        \n",
    "    Returns:\n",
    "        Boolean: True if point is inside polygon\n",
    "    \"\"\"\n",
    "    point = Point(lat, lon)\n",
    "    polygon = Polygon(polygon_coords)\n",
    "    return polygon.contains(point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e26a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a9753f2d",
   "metadata": {},
   "source": [
    "# ðŸ›°ï¸ AIS Data Filtering and Preprocessing â€” Kattegat Submarine Cable Area\n",
    "\n",
    "This notebook documents the initial steps of data preparation for the AIS anomaly detection project focusing on the submarine communication cables in the Kattegat area (Denmarkâ€“Sweden).\n",
    "\n",
    "The goal is to isolate AIS data within a defined polygon surrounding three specific cable routes (**GC2**, **Kattegat 2A**, and **Kattegat 2B**) and perform preliminary cleaning and filtering.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2b51d",
   "metadata": {},
   "source": [
    "## ðŸ§­ Area of Interest Definition\n",
    "\n",
    "We are interested in identifying potential anomalous or risky vessel behaviors near submarine cables.\n",
    "\n",
    "By Danish law, a 200 m protection zone exists on each side of these cables, but in this study we extend the inspection zone to **5 km** to capture behavioral precursors such as early anchoring, trawling, or route deviations before ships enter the restricted corridor.\n",
    "\n",
    "The polygon defining our area of interest was manually approximated based on the **DKCPC map**.\n",
    "\n",
    "It includes a section of the Danish coast near Saeby and extends to the Swedish side around Lerkil, covering the main cable routes.  \n",
    "The coordinates are stored as `(latitude, longitude)` pairs and define an octagonal region enclosing the study zone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3fbd739",
   "metadata": {},
   "outputs": [],
   "source": [
    "#initial filter on bounding box (take northest and southest, westest and eastest points):\n",
    "bbox = [57.58, 10.5, 57.12, 11.92]  # north, west, south, east\n",
    "# Define polygon coordinates as (lat, lon)\n",
    "polygon_coords = [\n",
    "    (57.3500, 10.5162),  # coast top left\n",
    "    (57.5120, 10.9314),  # sea top left\n",
    "    (57.5785, 11.5128),  # sea top right\n",
    "    (57.5230, 11.9132),  # top right (Swedish coast)\n",
    "    (57.4078, 11.9189),  # bottom right (Swedish coast)\n",
    "    (57.1389, 11.2133),  # sea bottom right\n",
    "    (57.1352, 11.0067),  # sea bottom left\n",
    "    (57.1880, 10.5400),  # coast bottom left\n",
    "    (57.3500, 10.5162),  # close polygon (duplicate of first)\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c018791d",
   "metadata": {},
   "source": [
    "## âš™ï¸ Data Loading\n",
    "\n",
    "In this step, AIS data (in CSV format) is loaded and receives a first hand cleaning 10% of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4f42f82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_28 = pd.read_csv(\"ais-data/aisdk-2025-10-28.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e7c7078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No filtering: 15,646,647 rows, 2,961 unique vessels, 26 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\"No filtering: {len(df_28):,} rows, {df_28['MMSI'].nunique():,} unique vessels, {df_28.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f5096e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cleaned = mini_cleaning_pipeline(df_28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0dffe131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initial filtering complete: 8,978,067 rows, 2,777 unique vessels, 20 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\" Initial filtering complete: {len(df_cleaned):,} rows, {df_cleaned['MMSI'].nunique():,} unique vessels, {df_cleaned.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a31bc42a",
   "metadata": {},
   "source": [
    "## ðŸ“ SQUARE Filtering\n",
    "\n",
    "The box coordinates defined earlier are used to filter the AIS dataset.\n",
    "Each point is checked to determine whether it falls inside the box using simple operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4bee294",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inside_square = filter_inside_square(df_cleaned, bbox)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e96823bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Bounding box filtering complete: 562,116 rows, 253 unique vessels, 20 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\" Bounding box filtering complete: {len(df_inside_square):,} rows, {df_inside_square['MMSI'].nunique():,} unique vessels, {df_inside_square.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63bcd40e",
   "metadata": {},
   "source": [
    "## ðŸ“ Polygon Filtering\n",
    "\n",
    "The polygon coordinates defined earlier are used to filter the AIS dataset.\n",
    "\n",
    "Each point is checked to determine whether it falls inside the polygon using geometric operations (e.g. with the `shapely` library).\n",
    "\n",
    "The resulting dataset contains only positions **within the defined Kattegat zone**, roughly **3â€“4 %** of the original AIS dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4fb9703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inside_poly = df_inside_square[\n",
    "    df_inside_square.apply(lambda row: is_inside_polygon(row['Latitude'], row['Longitude'], polygon_coords), axis=1)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ee428d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Polygon filtering complete: 314,285 rows, 172 unique vessels, 20 columns\n"
     ]
    }
   ],
   "source": [
    "print(f\" Polygon filtering complete: {len(df_inside_poly):,} rows, {df_inside_poly['MMSI'].nunique():,} unique vessels, {df_inside_poly.shape[1]} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df1ab7",
   "metadata": {},
   "source": [
    "## ðŸ“Š Data Overview After Filtering\n",
    "\n",
    "We summarize the filtered dataset:\n",
    "\n",
    "- Number of unique vessels  \n",
    "- Time coverage of the subset  \n",
    "- Spatial extent and approximate number of messages per vessel  \n",
    "\n",
    "This quick overview helps confirm that the filtering worked as expected and that the dataset is representative of the study area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c77ae63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique vessels: 172\n",
      "Latitude range: 57.163097 -- 57.578258\n",
      "Longitude range: 10.517063 -- 11.867375\n",
      "Centroid (lat, lon): (57.361891, 11.230398)\n",
      "\n",
      "Messages per vessel (summary):\n",
      "  count: 172.0\n",
      "  mean: 1827.2383720930231\n",
      "  std: 2845.5646981958407\n",
      "  min: 2.0\n",
      "  25%: 544.75\n",
      "  50%: 633.0\n",
      "  75%: 1068.25\n",
      "  max: 17475.0\n",
      "\n",
      "Top 10 vessels by number of messages (MMSI: count):\n",
      "MMSI\n",
      "219009229    17475\n",
      "220279000    11712\n",
      "220323000    10512\n",
      "220046000     9308\n",
      "219010207     9083\n",
      "219006219     8854\n",
      "220334000     8836\n",
      "219001553     8814\n",
      "219000543     8698\n",
      "220349000     8662\n"
     ]
    }
   ],
   "source": [
    "quick_summary(df_inside_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43968802",
   "metadata": {},
   "source": [
    "# USING SUMMARY FUNCTIONS TO FILTER AND DIVIDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cad7c83c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import ais_filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aa54a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before filtering: 15,646,647 rows, 2,961 unique vessels\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 119. MiB for an array with shape (1, 15646647) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m df_filtered = \u001b[43mais_filtering\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdf_filter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_28\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\Documents\\local_dir\\local_python_code\\DEEPLEARNING_CODE\\proj\\darkvessel-hunter\\dark-vessel-hunter\\ais_filtering.py:47\u001b[39m, in \u001b[36mdf_filter\u001b[39m\u001b[34m(df, verbose_mode, polygon_filter)\u001b[39m\n\u001b[32m     45\u001b[39m \u001b[38;5;66;03m# ---- INITIAL FILTERING ----\u001b[39;00m\n\u001b[32m     46\u001b[39m df = df.rename(columns={\u001b[33m\"\u001b[39m\u001b[33m# Timestamp\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m\"\u001b[39m}) \u001b[38;5;66;03m# Rename column for consistency\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m47\u001b[39m df[\u001b[33m\"\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m\"\u001b[39m] = pd.to_datetime(df[\u001b[33m\"\u001b[39m\u001b[33mTimestamp\u001b[39m\u001b[33m\"\u001b[39m], \u001b[38;5;28mformat\u001b[39m=\u001b[33m\"\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m/\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm/\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m, errors=\u001b[33m\"\u001b[39m\u001b[33mcoerce\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m# Convert to datetime\u001b[39;00m\n\u001b[32m     49\u001b[39m df = df[df[\u001b[33m\"\u001b[39m\u001b[33mMMSI\u001b[39m\u001b[33m\"\u001b[39m].str.len() == \u001b[32m9\u001b[39m]  \u001b[38;5;66;03m# Adhere to MMSI format\u001b[39;00m\n\u001b[32m     50\u001b[39m df = df[df[\u001b[33m\"\u001b[39m\u001b[33mMMSI\u001b[39m\u001b[33m\"\u001b[39m].str[:\u001b[32m3\u001b[39m].astype(\u001b[38;5;28mint\u001b[39m).between(\u001b[32m200\u001b[39m, \u001b[32m775\u001b[39m)]  \u001b[38;5;66;03m# Adhere to MID standard\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\.conda\\envs\\course02807\\Lib\\site-packages\\pandas\\core\\frame.py:5774\u001b[39m, in \u001b[36mDataFrame.rename\u001b[39m\u001b[34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[39m\n\u001b[32m   5643\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrename\u001b[39m(\n\u001b[32m   5644\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5645\u001b[39m     mapper: Renamer | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5653\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5654\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5655\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5656\u001b[39m \u001b[33;03m    Rename columns or index labels.\u001b[39;00m\n\u001b[32m   5657\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5772\u001b[39m \u001b[33;03m    4  3  6\u001b[39;00m\n\u001b[32m   5773\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5774\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_rename\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5775\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5776\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5777\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5778\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5779\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5780\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5781\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5782\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5783\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\.conda\\envs\\course02807\\Lib\\site-packages\\pandas\\core\\generic.py:1105\u001b[39m, in \u001b[36mNDFrame._rename\u001b[39m\u001b[34m(self, mapper, index, columns, axis, copy, inplace, level, errors)\u001b[39m\n\u001b[32m   1102\u001b[39m         index = mapper\n\u001b[32m   1104\u001b[39m \u001b[38;5;28mself\u001b[39m._check_inplace_and_allows_duplicate_labels(inplace)\n\u001b[32m-> \u001b[39m\u001b[32m1105\u001b[39m result = \u001b[38;5;28mself\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1107\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis_no, replacements \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m((index, columns)):\n\u001b[32m   1108\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m replacements \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\.conda\\envs\\course02807\\Lib\\site-packages\\pandas\\core\\generic.py:6830\u001b[39m, in \u001b[36mNDFrame.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m   6681\u001b[39m \u001b[38;5;129m@final\u001b[39m\n\u001b[32m   6682\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcopy\u001b[39m(\u001b[38;5;28mself\u001b[39m, deep: bool_t | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mTrue\u001b[39;00m) -> Self:\n\u001b[32m   6683\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   6684\u001b[39m \u001b[33;03m    Make a copy of this object's indices and data.\u001b[39;00m\n\u001b[32m   6685\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   6828\u001b[39m \u001b[33;03m    dtype: int64\u001b[39;00m\n\u001b[32m   6829\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m6830\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_mgr\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   6831\u001b[39m     \u001b[38;5;28mself\u001b[39m._clear_item_cache()\n\u001b[32m   6832\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._constructor_from_mgr(data, axes=data.axes).__finalize__(\n\u001b[32m   6833\u001b[39m         \u001b[38;5;28mself\u001b[39m, method=\u001b[33m\"\u001b[39m\u001b[33mcopy\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   6834\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\.conda\\envs\\course02807\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:593\u001b[39m, in \u001b[36mBaseBlockManager.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    590\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    591\u001b[39m         new_axes = \u001b[38;5;28mlist\u001b[39m(\u001b[38;5;28mself\u001b[39m.axes)\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcopy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m res.axes = new_axes\n\u001b[32m    596\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ndim > \u001b[32m1\u001b[39m:\n\u001b[32m    597\u001b[39m     \u001b[38;5;66;03m# Avoid needing to re-compute these\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\.conda\\envs\\course02807\\Lib\\site-packages\\pandas\\core\\internals\\managers.py:363\u001b[39m, in \u001b[36mBaseBlockManager.apply\u001b[39m\u001b[34m(self, f, align_keys, **kwargs)\u001b[39m\n\u001b[32m    361\u001b[39m         applied = b.apply(f, **kwargs)\n\u001b[32m    362\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m         applied = \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    364\u001b[39m     result_blocks = extend_blocks(applied, result_blocks)\n\u001b[32m    366\u001b[39m out = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m).from_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m.axes)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\elefa\\.conda\\envs\\course02807\\Lib\\site-packages\\pandas\\core\\internals\\blocks.py:822\u001b[39m, in \u001b[36mBlock.copy\u001b[39m\u001b[34m(self, deep)\u001b[39m\n\u001b[32m    820\u001b[39m refs: BlockValuesRefs | \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    821\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m822\u001b[39m     values = values.copy()\n\u001b[32m    823\u001b[39m     refs = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 119. MiB for an array with shape (1, 15646647) and data type object"
     ]
    }
   ],
   "source": [
    "df_filtered = ais_filtering.df_filter(df_28, True, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7d81267",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ais_filtering' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df_static, df_dynamic \u001b[38;5;241m=\u001b[39m ais_filtering\u001b[38;5;241m.\u001b[39msplit_static_dynamic(df_filtered, join_conflicts\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m | \u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'ais_filtering' is not defined"
     ]
    }
   ],
   "source": [
    "df_static, df_dynamic = ais_filtering.split_static_dynamic(df_filtered, join_conflicts=True, sep=\" | \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258a5c89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
