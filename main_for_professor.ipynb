{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e568f0d6",
   "metadata": {},
   "source": [
    "# vessel-cable-anomaly-hunter\n",
    "DTU Deep Learning project 29, group 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814a0",
   "metadata": {},
   "source": [
    "## Required Libraries Installation\n",
    "Run this in your terminal before executing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6f609e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2be5b84",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**NOTE ON PYTHON VERSION**\n",
    "\n",
    "This project requires **Python 3.10 or a later version.**\n",
    "\n",
    "**WARNING: Training Speed (CPU Usage)**\n",
    "\n",
    "If your execution environment does not utilize a dedicated Graphics Processing Unit (GPU, such as NVIDIA CUDA or Apple MPS), please be advised that training the optimal $\\text{LSTM}$ Autoencoder configuration will be extremely slow (potentially taking several hours per run). This is due to the computational intensity of processing long sequential data on a Central Processing Unit ($\\text{CPU}$).To quickly verify the functionality of the training loop and observe early convergence without excessive runtime, it is strongly recommended that you lower the EPOCHS value (set $\\text{EPOCHS} = 5$) in the config.py file before proceeding.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd44da",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "AIS Data scraper and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347b624",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c203ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.data.ais_downloader as ais_downloader\n",
    "import src.data.ais_filtering as ais_filtering\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.data.ais_to_parquet as ais_to_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a859",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc178db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675b5f",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6c341eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from config.py\n",
    "VERBOSE_MODE = config.VERBOSE_MODE                          # Whether to print verbose output\n",
    "\n",
    "START_DATE = config.START_DATE                              # Start date for data downloading\n",
    "END_DATE   = config.END_DATE                                # End date for data downloading\n",
    "\n",
    "DELETE_DOWNLOADED_CSV = config.DELETE_DOWNLOADED_CSV        # Whether to delete raw downloaded CSV files after parquet conversion\n",
    "\n",
    "BBOX = config.BBOX                                          # Bounding Box to prefilter AIS data\n",
    "POLYGON_COORDINATES = config.POLYGON_COORDINATES  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8db5",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2a99188",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(config.AIS_DATA_FOLDER)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "csv_folder_path = folder_path / config.AIS_DATA_FOLDER_CSV_SUBFOLDER\n",
    "csv_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "parquet_folder_path = folder_path / config.AIS_DATA_FOLDER_PARQUET_SUBFOLDER\n",
    "parquet_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_port_locations = folder_path / config.FILE_PORT_LOCATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98948aa7",
   "metadata": {},
   "source": [
    "#### Download, Filter and Save into Parquet\n",
    "1. **Download:** Download one single .csv AIS data file from http://aisdata.ais.dk (link to data column description http://aisdata.ais.dk/!_README_information_CSV_files.txt);\n",
    "2. **Filter:** For a given AOI in Denmark with known cable positions, filter AIS messages by cleansing unrealistic/unphysical messages or duplicates and removes error-prone messages within port areas;\n",
    "3. **Segmentation:** Segment the cleaned data into tracks based on time gaps and track duration;\n",
    "4. **Parquet Conversion:** Save the cleaned and filtered data into Parquet files for faster loading in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24af940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/8 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-08-01\n",
      "Skipping 2025-08-01 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,128,873 rows within bbox, 511 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,128,873 rows, 511 vessels\n",
      " [filter_ais_df] Type filtering: 1,093,101 rows (removed 35,772) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,093,079 rows, 508 vessels\n",
      " [filter_ais_df] Duplicate removal: 638,467 rows, 508 vessels\n",
      " [filter_ais_df] Polygon filtering: 337,847 rows (removed 300,620), 378 vessels\n",
      " [filter_ais_df] Port-area removal: removed 160,778 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 175,598 rows (removed 1,471) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 173,215 rows (removed 2,381) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 173,215 rows, 327 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 327 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  12%|█▎        | 1/8 [00:14<01:40, 14.36s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-02\n",
      "Skipping 2025-08-02 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,161,214 rows within bbox, 452 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,161,214 rows, 452 vessels\n",
      " [filter_ais_df] Type filtering: 1,120,127 rows (removed 41,087) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,114,486 rows, 447 vessels\n",
      " [filter_ais_df] Duplicate removal: 616,865 rows, 447 vessels\n",
      " [filter_ais_df] Polygon filtering: 314,494 rows (removed 302,371), 342 vessels\n",
      " [filter_ais_df] Port-area removal: removed 168,854 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 145,303 rows (removed 337) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 144,010 rows (removed 1,291) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 144,010 rows, 290 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 290 (MMSI, Date) combinations.\n",
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  25%|██▌       | 2/8 [00:27<01:22, 13.78s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-08-03\n",
      "Skipping 2025-08-03 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,079,891 rows within bbox, 399 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,079,891 rows, 399 vessels\n",
      " [filter_ais_df] Type filtering: 1,043,284 rows (removed 36,607) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,043,284 rows, 398 vessels\n",
      " [filter_ais_df] Duplicate removal: 599,515 rows, 398 vessels\n",
      " [filter_ais_df] Polygon filtering: 319,478 rows (removed 280,037), 288 vessels\n",
      " [filter_ais_df] Port-area removal: removed 165,183 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 153,909 rows (removed 386) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 152,641 rows (removed 1,266) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 152,641 rows, 248 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 248 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  38%|███▊      | 3/8 [00:40<01:05, 13.09s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-04\n",
      "Skipping 2025-08-04 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,161,332 rows within bbox, 380 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,161,332 rows, 380 vessels\n",
      " [filter_ais_df] Type filtering: 1,125,031 rows (removed 36,301) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,125,031 rows, 379 vessels\n",
      " [filter_ais_df] Duplicate removal: 637,785 rows, 379 vessels\n",
      " [filter_ais_df] Polygon filtering: 321,848 rows (removed 315,937), 272 vessels\n",
      " [filter_ais_df] Port-area removal: removed 156,758 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 164,980 rows (removed 110) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 162,923 rows (removed 2,057) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 162,923 rows, 232 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 232 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  50%|█████     | 4/8 [00:53<00:53, 13.40s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-05\n",
      "Skipping 2025-08-05 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,157,746 rows within bbox, 268 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,157,746 rows, 268 vessels\n",
      " [filter_ais_df] Type filtering: 1,119,953 rows (removed 37,793) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,116,828 rows, 264 vessels\n",
      " [filter_ais_df] Duplicate removal: 613,845 rows, 264 vessels\n",
      " [filter_ais_df] Polygon filtering: 301,015 rows (removed 312,830), 175 vessels\n",
      " [filter_ais_df] Port-area removal: removed 171,761 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 129,228 rows (removed 26) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 127,182 rows (removed 2,046) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 127,182 rows, 115 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 115 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  62%|██████▎   | 5/8 [01:05<00:38, 12.70s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-06\n",
      "Skipping 2025-08-06 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,145,520 rows within bbox, 261 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,145,520 rows, 261 vessels\n",
      " [filter_ais_df] Type filtering: 1,108,897 rows (removed 36,623) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,108,897 rows, 260 vessels\n",
      " [filter_ais_df] Duplicate removal: 622,309 rows, 260 vessels\n",
      " [filter_ais_df] Polygon filtering: 300,756 rows (removed 321,553), 169 vessels\n",
      " [filter_ais_df] Port-area removal: removed 163,859 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 136,873 rows (removed 24) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 134,698 rows (removed 2,175) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 134,698 rows, 123 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 123 (MMSI, Date) combinations.\n",
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  75%|███████▌  | 6/8 [01:16<00:24, 12.23s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-08-07\n",
      "Skipping 2025-08-07 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,202,886 rows within bbox, 385 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,202,886 rows, 385 vessels\n",
      " [filter_ais_df] Type filtering: 1,166,600 rows (removed 36,286) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,162,713 rows, 382 vessels\n",
      " [filter_ais_df] Duplicate removal: 642,517 rows, 382 vessels\n",
      " [filter_ais_df] Polygon filtering: 301,062 rows (removed 341,455), 277 vessels\n",
      " [filter_ais_df] Port-area removal: removed 133,073 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 167,842 rows (removed 147) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 166,354 rows (removed 1,488) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 166,354 rows, 247 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 247 (MMSI, Date) combinations.\n",
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  88%|████████▊ | 7/8 [01:29<00:12, 12.32s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-08-08\n",
      "Skipping 2025-08-08 download: already present in ais-data/csv folder\n",
      "Read AIS data: 1,160,724 rows within bbox, 399 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,160,724 rows, 399 vessels\n",
      " [filter_ais_df] Type filtering: 1,124,268 rows (removed 36,456) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,124,255 rows, 397 vessels\n",
      " [filter_ais_df] Duplicate removal: 618,010 rows, 397 vessels\n",
      " [filter_ais_df] Polygon filtering: 270,458 rows (removed 347,552), 285 vessels\n",
      " [filter_ais_df] Port-area removal: removed 115,708 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 154,292 rows (removed 458) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 151,933 rows (removed 2,359) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 151,933 rows, 262 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 262 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 8/8 [01:41<00:00, 12.65s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/16/213558/dark-vessel-hunter/ais-data/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Build the schedule of download string dates ---\n",
    "dates = ais_downloader.get_work_dates(START_DATE, END_DATE, csv_folder_path, filter=False)\n",
    "\n",
    "# --- Iterate with tqdm and download, unzip and delete ---\n",
    "for day in tqdm(dates, desc=f\"Processing data\", unit=\"file\" ):\n",
    "    tag = f\"{day:%Y-%m}\" if day < date.fromisoformat(\"2024-03-01\") else f\"{day:%Y-%m-%d}\"\n",
    "    print(f\"\\nProcessing date: {tag}\")\n",
    "\n",
    "    # --- Download one day ---\n",
    "    csv_path = ais_downloader.download_one_ais_data(day, csv_folder_path)\n",
    "    \n",
    "    # --- Load CSV into DataFrame ---\n",
    "    df_raw = ais_reader.read_single_ais_df(csv_path, BBOX, columns_to_drop=config.COLUMNS_TO_DROP, verbose=VERBOSE_MODE)\n",
    "    # --- Optionally delete the downloaded CSV file ---\n",
    "    if DELETE_DOWNLOADED_CSV: csv_path.unlink(missing_ok=True)\n",
    "    \n",
    "    # --- Filter and split ---\n",
    "    # Filter AIS data, keeping Class A and Class B by default,\n",
    "    df_filtered = ais_filtering.filter_ais_df(\n",
    "        df_raw,                                               # raw AIS DataFrame\n",
    "        polygon_coords=POLYGON_COORDINATES,                   # polygon coordinates for precise AOI filtering\n",
    "        allowed_mobile_types=config.VESSEL_AIS_CLASS,                # vessel AIS class filter\n",
    "        apply_polygon_filter=True,                            # keep polygon filtering enabled boolean\n",
    "        remove_zero_sog_vessels=config.REMOVE_ZERO_SOG_VESSELS,      # use True/False to enable/disable 90% zero-SOG removal\n",
    "        output_sog_in_ms=config.SOG_IN_MS,                           # convert SOG from knots in m/s (default) boolean\n",
    "        sog_min_knots=config.SOG_MIN_KNOTS,                          # min SOG in knots to keep (None to disable)\n",
    "        sog_max_knots=config.SOG_MAX_KNOTS,                          # max SOG in knots to keep (None to disable) \n",
    "        port_locodes_path=file_port_locations,                # path to port locodes CSV\n",
    "        exclude_ports=True,                                   # exclude port areas boolean \n",
    "        verbose=VERBOSE_MODE,                                 # verbose mode boolean\n",
    "    )\n",
    "    \n",
    "    # Free df_raw memory\n",
    "    del df_raw\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Parquet conversion ---\n",
    "    # Save to Parquet by MMSI\n",
    "    ais_to_parquet.save_by_mmsi(\n",
    "        df_filtered,                                             # filtered AIS DataFrame \n",
    "        verbose=VERBOSE_MODE,                                    # verbose mode boolean\n",
    "        output_folder=parquet_folder_path                        # output folder path\n",
    "    )\n",
    "\n",
    "    # Free df_filtered memory\n",
    "    del df_filtered\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb7e37",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50077b83",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3d240d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.pre_proc.pre_processing_utils as pre_processing_utils\n",
    "import src.pre_proc.ais_query as ais_query\n",
    "import src.pre_proc.ais_segment as ais_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417146",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "450bff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465104",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "149c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from config.py\n",
    "VERBOSE_MODE = config.VERBOSE_MODE\n",
    "\n",
    "FOLDER_NAME = config.AIS_DATA_FOLDER\n",
    "folder_path = Path(FOLDER_NAME)\n",
    "parquet_folder_path = folder_path / config.AIS_DATA_FOLDER_PARQUET_SUBFOLDER\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE\n",
    "TEST_END_DATE = config.TEST_END_DATE\n",
    "\n",
    "MAX_TIME_GAP_SEC = config.MAX_TIME_GAP_SEC\n",
    "MAX_TRACK_DURATION_SEC = config.MAX_TRACK_DURATION_SEC\n",
    "MIN_TRACK_DURATION_SEC = config.MIN_TRACK_DURATION_SEC\n",
    "MIN_SEGMENT_LENGTH = config.MIN_SEGMENT_LENGTH\n",
    "\n",
    "MIN_FREQ_POINTS_PER_MIN = config.MIN_FREQ_POINTS_PER_MIN\n",
    "\n",
    "RESAMPLING_RULE = config.RESAMPLING_RULE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e201661",
   "metadata": {},
   "source": [
    "#### Preprocess function\n",
    "1. **Data Loading:** Queries DuckDB for AIS data within specified date ranges for 'train' or 'test'.\n",
    "2. **Feature Engineering:** Converts COG (Course Over Ground) to sine/cosine components.\n",
    "3. **Cleaning:** Drops unnecessary columns and rows with missing values.\n",
    "4. **Ship Type Grouping:** Aggregates specific ship types into broader categories (Commercial,\n",
    "Passenger, Service, Other).\n",
    "5. **Segmentation:** Splits AIS tracks into segments based on time gaps and duration constraints using\n",
    "ais_segment\n",
    "6. **Filtering:** Removes segments with low point density.\n",
    "7. **Resampling:** Resamples tracks to a fixed time interval.\n",
    "8. **Labeling:** Encodes ship types into numerical IDs.\n",
    "9. **Saving:** Exports the processed DataFrame to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "929aae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_preprocess(dataframe_type: str = \"all\"):\n",
    "\n",
    "    if dataframe_type == \"all\":\n",
    "        main_preprocess(\"train\")\n",
    "        main_preprocess(\"test\")\n",
    "        return\n",
    "        \n",
    "    elif dataframe_type == \"train\":\n",
    "        print(f\"[preprocess] Querying AIS data for training period: {TRAIN_START_DATE} to {TRAIN_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, date_start=TRAIN_START_DATE, date_end=TRAIN_END_DATE, verbose=VERBOSE_MODE)\n",
    "        \n",
    "    elif dataframe_type == \"test\":\n",
    "        print(f\"[preprocess] Querying AIS data for testing period: {TEST_START_DATE} to {TEST_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, date_start=TEST_START_DATE, date_end=TEST_END_DATE, verbose=VERBOSE_MODE)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataframe_type: {dataframe_type}. Must be 'train' or 'test'.\")\n",
    "     \n",
    "    # Converting COG to sine and cosine components\n",
    "    df = pre_processing_utils.cog_to_sin_cos(df)\n",
    "    \n",
    "    # Dropping unnecessary columns and rows with missing values\n",
    "    df.drop(columns=[ \n",
    "        'Type of mobile', \n",
    "        'COG', \n",
    "        'Date'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Removing rows with NaN values in essential columns\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Grouping Ship types\n",
    "    commercial_types = [\"Cargo\", \"Tanker\"]\n",
    "    passenger_types = [\"Passenger\", \"Pleasure\", \"Sailing\"]\n",
    "    service_types = [\"Dredging\", \"Law enforcement\", \"Military\", \"Port tender\", \"SAR\", \"Towing\", \"Towing long/wide\",\"Tug\"]\n",
    "    valid_types =  [\"Fishing\", \"Service\", \"Commercial\", \"Passenger\"]\n",
    "\n",
    "    df.loc[df[\"Ship type\"].isin(commercial_types), \"Ship type\"] = \"Commercial\"\n",
    "    df.loc[df[\"Ship type\"].isin(passenger_types), \"Ship type\"] = \"Passenger\"\n",
    "    df.loc[df[\"Ship type\"].isin(service_types), \"Ship type\"] = \"Service\"\n",
    "    df.loc[~df[\"Ship type\"].isin(valid_types), \"Ship type\"] = \"Other\"\n",
    "    \n",
    "    print(\"[preprocess] Ship type counts:\")\n",
    "    print(df[\"Ship type\"].value_counts())\n",
    "\n",
    "    if VERBOSE_MODE:\n",
    "        print(f\"[preprocess] DataFrame after dropping unnecessary columns and NaNs: {len(df):,} rows\")\n",
    "\n",
    "    # Segmenting AIS tracks based on time gaps and max duration, filtering short segments\n",
    "    df = ais_segment.segment_ais_tracks(\n",
    "        df,\n",
    "        max_time_gap_sec=MAX_TIME_GAP_SEC,\n",
    "        max_track_duration_sec=MAX_TRACK_DURATION_SEC,\n",
    "        min_track_duration_sec=MIN_TRACK_DURATION_SEC,\n",
    "        min_track_len=MIN_SEGMENT_LENGTH,\n",
    "        verbose=VERBOSE_MODE\n",
    "    )\n",
    "\n",
    "    # Adding segment nr feature\n",
    "    # df = pre_processing_utils.add_segment_nr(df)\n",
    "\n",
    "    # Removing segments with low point density\n",
    "    df = pre_processing_utils.remove_notdense_segments(df, min_freq_points_per_min=MIN_FREQ_POINTS_PER_MIN)\n",
    "    \n",
    "    # Resampling all tracks to fixed time intervals\n",
    "    df = pre_processing_utils.resample_all_tracks(df, rule=RESAMPLING_RULE)\n",
    "\n",
    "    print(f\"[preprocess] Number of segments and rows after removing low-density segments and resampling: {df['Segment_nr'].nunique():,} segments, {len(df):,} rows\")\n",
    "\n",
    "    # Normalizing numeric columns\n",
    "    #df, mean, std = pre_processing_utils.normalize_df(df, NUMERIC_COLS)\n",
    "\n",
    "    # Ship type labeling (mapping to be used later)\n",
    "    df, ship_type_to_id = pre_processing_utils.label_ship_types(df)\n",
    "    \n",
    "    # Saving pre-processed DataFrame\n",
    "    if dataframe_type == \"train\":\n",
    "        print(f\"[preprocess] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TRAIN_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "        #metadata_path = config.PRE_PROCESSING_METADATA_TRAIN_PATH\n",
    "    else:\n",
    "        print(f\"[preprocess] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TEST_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "        #metadata_path = config.PRE_PROCESSING_METADATA_TEST_PATH\n",
    "\n",
    "    if VERBOSE_MODE: print(f\"[preprocess] Columns of pre-processed DataFrame:\\n{df.columns.tolist()}\")\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf852",
   "metadata": {},
   "source": [
    "#### Preprocess for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "597faf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] Querying AIS data for training period: 2025-08-01 to 2025-08-07\n",
      "[ais_query] Querying parquet files from: ais-data/parquet  from date 2025-08-01  to date  2025-08-07\n",
      "[ais_query] 1,061,023 rows, 1,133 vessels, from date 2025-08-01 to date 2025-08-07\n",
      "[preprocess] Ship type counts:\n",
      "Ship type\n",
      "Commercial    489776\n",
      "Passenger     265809\n",
      "Fishing       236166\n",
      "Service        43170\n",
      "Other          26102\n",
      "Name: count, dtype: int64\n",
      "[preprocess] DataFrame after dropping unnecessary columns and NaNs: 1,061,023 rows\n",
      "[segment_ais_tracks] Starting with 1,061,023 rows, 1,133 unique vessels\n",
      "[segment_ais_tracks] Final processed data: 1,056,993 rows, 1,691 segments.\n",
      "[preprocess] Number of segments and rows after removing low-density segments and resampling: 1,572 segments, 199,476 rows\n",
      "[preprocess] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_train.parquet...\n",
      "[preprocess] Columns of pre-processed DataFrame:\n",
      "['Segment_nr', 'Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG_sin', 'COG_cos', 'TrackID', 'MMSI', 'ShipTypeID']\n"
     ]
    }
   ],
   "source": [
    "main_preprocess(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "61d81e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess] Querying AIS data for testing period: 2025-08-08 to 2025-08-08\n",
      "[ais_query] Querying parquet files from: ais-data/parquet  from date 2025-08-08  to date  2025-08-08\n",
      "[ais_query] 151,933 rows, 262 vessels, from date 2025-08-08 to date 2025-08-08\n",
      "[preprocess] Ship type counts:\n",
      "Ship type\n",
      "Commercial    72556\n",
      "Passenger     37184\n",
      "Fishing       35480\n",
      "Other          3914\n",
      "Service        2799\n",
      "Name: count, dtype: int64\n",
      "[preprocess] DataFrame after dropping unnecessary columns and NaNs: 151,933 rows\n",
      "[segment_ais_tracks] Starting with 151,933 rows, 262 unique vessels\n",
      "[segment_ais_tracks] Final processed data: 151,180 rows, 278 segments.\n",
      "[preprocess] Number of segments and rows after removing low-density segments and resampling: 255 segments, 29,510 rows\n",
      "[preprocess] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_test.parquet...\n",
      "[preprocess] Columns of pre-processed DataFrame:\n",
      "['Segment_nr', 'Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG_sin', 'COG_cos', 'TrackID', 'MMSI', 'ShipTypeID']\n"
     ]
    }
   ],
   "source": [
    "main_preprocess(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724caf9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f656cc",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c2de52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as config_file\n",
    "from src.train.ais_dataset import AISDataset, ais_collate_fn\n",
    "from src.train.model_anchoring import AIS_LSTM_Autoencoder\n",
    "from src.train.training_loop import run_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73047b",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "39bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b9e0b",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74e19d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "PARQUET_FILE = config_file.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "TRAIN_OUTPUT_DIR = config_file.TRAIN_OUTPUT_DIR\n",
    "LOSS_TYPE = config_file.LOSS_TYPE\n",
    "\n",
    "# ensure output directory exists\n",
    "os.makedirs(TRAIN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SPLIT_TRAIN_VAL_RATIO = config_file.SPLIT_TRAIN_VAL_RATIO\n",
    "EPOCHS = config_file.EPOCHS\n",
    "PATIENCE = config_file.PATIENCE\n",
    "FEATURES = config_file.FEATURE_COLS\n",
    "NUM_SHIP_TYPES = config_file.NUM_SHIP_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953ac75",
   "metadata": {},
   "source": [
    "#### Hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7388c647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running a single configuration: H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "# HYPERPARAMETERS\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "top_params = {\n",
    "        'hidden_dim': config_file.HIDDEN_DIM,       # Capacity of the LSTM\n",
    "        'latent_dim': config_file.LATENT_DIM,         # Bottleneck\n",
    "        'num_layers': config_file.NUM_LAYERS,           # Depth\n",
    "        'lr': config_file.LEARNING_RATE,          # Learning Rate\n",
    "        'batch_size': config_file.BATCH_SIZE,        # Batch Size\n",
    "        'dropout': config_file.DROP_OUT           # Regularization\n",
    "    }\n",
    "\n",
    "run_name_suffix = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "run_name = (f\"H{top_params['hidden_dim']}_L{top_params['latent_dim']}_\"\n",
    "            f\"Lay{top_params['num_layers']}_lr{top_params['lr']}_\"\n",
    "            f\"BS{top_params['batch_size']}_Drop{top_params['dropout']}_{run_name_suffix}_{LOSS_TYPE}\")\n",
    "\n",
    "config = {\n",
    "    \"run_name\": run_name,\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"patience\": PATIENCE,\n",
    "    \"features\": FEATURES,\n",
    "    \"num_ship_types\": NUM_SHIP_TYPES,\n",
    "    \"shiptype_emb_dim\": 8,\n",
    "    \"loss_type\": LOSS_TYPE,\n",
    "    \n",
    "    # Dynamic Params (ora fissi)\n",
    "    \"hidden_dim\": top_params['hidden_dim'],\n",
    "    \"latent_dim\": top_params['latent_dim'],\n",
    "    \"num_layers\": top_params['num_layers'],\n",
    "    \"lr\": top_params['lr'],\n",
    "    \"batch_size\": top_params['batch_size'],\n",
    "    \"dropout\": top_params['dropout']\n",
    "}\n",
    "\n",
    "configs = [config]\n",
    "\n",
    "print(f\"Running a single configuration: {run_name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923b10b",
   "metadata": {},
   "source": [
    "#### Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d3a4e1e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu (CPU)\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # for PC with NVIDIA\n",
    "    print(f\"Using device: {device} (NVIDIA GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # for Mac Apple Silicon\n",
    "    print(f\"Using device: {device} (Apple GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Fallback on CPU\n",
    "    print(f\"Using device: {device} (CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce4a00",
   "metadata": {},
   "source": [
    "#### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fc17fc85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ais-data/df_preprocessed/pre_processed_df_train.parquet...\n",
      "Normalizing features...\n",
      "Grouping segments...\n",
      "Processed 1572 unique segments.\n",
      "Train samples: 1257, Val samples: 315\n"
     ]
    }
   ],
   "source": [
    "if not os.path.exists(PARQUET_FILE):\n",
    "    print(f\"Error: {PARQUET_FILE} not found.\")\n",
    "\n",
    "# Initialize Dataset\n",
    "full_dataset = AISDataset(PARQUET_FILE)\n",
    "input_dim = full_dataset.input_dim\n",
    "\n",
    "# Split Train/Val (80/20)\n",
    "train_size = int(SPLIT_TRAIN_VAL_RATIO * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d1ea6",
   "metadata": {},
   "source": [
    "#### Experiment Loop\n",
    "1. **Create DataLoaders:** Creates PyTorch DataLoaders for training and validation datasets.\n",
    "2. **Training Setup:** Initialize Model with FIXED num_ship_types, optimizer, and loss function.\n",
    "3. **Training Loop:** Trains the model over a set number of epochs, implementing early stopping based on validation loss.\n",
    "4. **Evaluation:** Assesses model performance on the validation set after each epoch.\n",
    "5. **Model Saving:** Saves the best-performing model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0b256aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Run: H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse (Loss: mse)---\n",
      "Epoch [1/1] Train Loss: 0.137647 | Val Loss: 0.099916\n",
      "Validation loss decreased. Model saved to models/weights_H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse.pth\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(  # Training DataLoader\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True, \n",
    "        collate_fn=ais_collate_fn\n",
    "    )\n",
    "    \n",
    "    val_loader = DataLoader(    # Validation DataLoader\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False, \n",
    "        collate_fn=ais_collate_fn\n",
    "    )\n",
    "    \n",
    "    # Initialize Model with FIXED num_ship_types\n",
    "    model = AIS_LSTM_Autoencoder(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        latent_dim=config['latent_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        num_ship_types=NUM_SHIP_TYPES, # Always use the fixed constant\n",
    "        shiptype_emb_dim=config['shiptype_emb_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    # Run Pipeline\n",
    "    save_path = f\"{TRAIN_OUTPUT_DIR}/weights_{config['run_name']}.pth\"\n",
    "    history, best_loss = run_experiment(config, model, train_loader, val_loader, device, save_path=f\"{TRAIN_OUTPUT_DIR}/weights_{config['run_name']}.pth\")\n",
    "    \n",
    "    # Save results\n",
    "    results.append({\n",
    "        \"config\": config['run_name'],\n",
    "        \"best_val_loss\": best_loss,\n",
    "        \"history\": history\n",
    "    })\n",
    "\n",
    "    # Save model and config\n",
    "    os.makedirs(TRAIN_OUTPUT_DIR, exist_ok=True)\n",
    "    with open(f\"{TRAIN_OUTPUT_DIR}/config_{config['run_name']}.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c083c",
   "metadata": {},
   "source": [
    "#### Summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8e63c157",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Single Configuration Result ===\n",
      "Run: H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse | Best Val Loss: 0.099916\n"
     ]
    }
   ],
   "source": [
    "results_path = os.path.join(TRAIN_OUTPUT_DIR, \"results_summary_single\"+ datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\".json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print result\n",
    "print(\"\\n=== Single Configuration Result ===\") \n",
    "print(f\"Run: {config['run_name']} | Best Val Loss: {float(best_loss):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7304f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb787",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f4e75767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as config_file\n",
    "from src.test.ais_tester import AISTester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd90563",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "45c86bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15213e",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a4aa85",
   "metadata": {},
   "source": [
    "COPY the name of the model from above where it outputs \"Starting Run: **H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_210024_mse (Loss: mse)**\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0bb9e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the model configuration to use\n",
    "MODEL_NAME = \"H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse\"  # Change as needed\n",
    "\n",
    "N_BEST_WORST = config_file.N_BEST_WORST\n",
    "N_MAP_RANDOM = config_file.N_MAP_RANDOM\n",
    "\n",
    "# Data to test on\n",
    "PARQUET_FILE = config_file.PRE_PROCESSING_DF_TEST_PATH\n",
    "\n",
    "# Output Directory\n",
    "OUTPUT_DIR = config_file.TEST_OUTPUT_DIR + \"/\" + MODEL_NAME\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "WEIGHTS_FILE = config_file.TRAIN_OUTPUT_DIR + \"/weights_\" + MODEL_NAME + \".pth\"\n",
    "MODEL_CONFIG_FILE = config_file.TRAIN_OUTPUT_DIR + \"/config_\" + MODEL_NAME + \".json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d3719",
   "metadata": {},
   "source": [
    "#### Load Model and Init Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2cb45a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Loading weights from models/weights_H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse.pth...\n"
     ]
    }
   ],
   "source": [
    "# Load Model Config\n",
    "with open(MODEL_CONFIG_FILE, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Initialize Tester\n",
    "tester = AISTester(model_config, WEIGHTS_FILE, output_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1b9a4",
   "metadata": {},
   "source": [
    "#### Run Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c1cc68f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from ais-data/df_preprocessed/pre_processed_df_test.parquet...\n",
      "Normalizing features...\n",
      "Grouping segments...\n",
      "Processed 255 unique segments.\n",
      "Test data loaded: 255 segments.\n",
      "Running predictions...\n",
      "Evaluation complete. Processed 255 segments.\n",
      "Error distribution plot saved to: test_results/H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse/error_distribution.png\n",
      "\n",
      "--- Saving Top 3 Best Reconstructions (Line Plots) ---\n",
      "\n",
      "--- Saving Top 3 Worst Reconstructions (Line Plots) ---\n",
      "Map saved: test_results/H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse/map_BEST_3_segments.html\n",
      "Map saved: test_results/H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse/map_WORST_3_segments.html\n",
      "Map saved: test_results/H256_L64_Lay1_lr0.001_BS64_Drop0.0_20251205_220544_mse/map_RANDOM_5_segments.html\n"
     ]
    }
   ],
   "source": [
    "# Run tester pipeline (assumes PARQUET_FILE and tester are defined elsewhere in the notebook)\n",
    "if os.path.exists(PARQUET_FILE):\n",
    "    # 1. Evaluate ALL data first\n",
    "    tester.load_data(PARQUET_FILE)\n",
    "    tester.evaluate()\n",
    "        \n",
    "    # 2. Plot General Stats\n",
    "    tester.plot_error_distributions()\n",
    "        \n",
    "    # 3. Plot Filtered Stats (Example)\n",
    "    # You can pass a list of IDs to filter just the plot without re-running evaluate\n",
    "    # my_interesting_ids = [\"segment_A\", \"segment_B\"]\n",
    "    # tester.plot_error_distributions(filter_ids=my_interesting_ids, filename_suffix=\"_special_group\")\n",
    "        \n",
    "    # 4. Standard Best/Worst\n",
    "    tester.plot_best_worst_segments(n=N_BEST_WORST)\n",
    "        \n",
    "    # 5. Maps\n",
    "    tester.generate_maps(n_best_worst=N_BEST_WORST, n_random=N_MAP_RANDOM)\n",
    "\n",
    "    # 6. Filtered Map Example\n",
    "    # tester.generate_filtered_map(segment_ids=[\"segment_1\", \"segment_2\"], map_name=\"map_special_segments\")\n",
    "        \n",
    "else:\n",
    "    print(f\"File {PARQUET_FILE} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "idlcvid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
