{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e568f0d6",
   "metadata": {},
   "source": [
    "# dark-vessel-hunter\n",
    "DTU Deep Learning project 29, group 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814a0",
   "metadata": {},
   "source": [
    "\n",
    "### Run this in your terminal before executing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c6f609e",
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datetime in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (5.5)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pathlib in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: zipfile36 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.1.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.3.2)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (19.0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.7.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: shapely in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.1.2)\n",
      "Requirement already satisfied: folium in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: fastparquet in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (2024.11.0)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (6.0.3)\n",
      "Requirement already satisfied: typing in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (3.7.4.3)\n",
      "Requirement already satisfied: duckdb in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (1.4.2)\n",
      "Requirement already satisfied: zope.interface in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from datetime->-r requirements.txt (line 1)) (8.1)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from datetime->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from folium->-r requirements.txt (line 12)) (0.8.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from folium->-r requirements.txt (line 12)) (3.1.6)\n",
      "Requirement already satisfied: xyzservices in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from folium->-r requirements.txt (line 12)) (2025.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (3.2.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from fastparquet->-r requirements.txt (line 14)) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from fastparquet->-r requirements.txt (line 14)) (2024.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from jinja2>=2.9->folium->-r requirements.txt (line 12)) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd8b671",
   "metadata": {},
   "source": [
    "## Import of the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a993f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.data.ais_downloader as ais_downloader\n",
    "import src.data.ais_filtering as ais_filtering\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.data.ais_to_parquet as ais_to_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd44da",
   "metadata": {},
   "source": [
    "## Data setup\n",
    "### Set data preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0c5cdbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_DATE = \"2025-11-01\"\n",
    "END_DATE   = \"2025-11-03\"\n",
    "\n",
    "FOLDER_NAME = \"ais-data\"\n",
    "DELETE_DOWNLOADED_CSV = False\n",
    "VERBOSE_MODE = True\n",
    "\n",
    "VESSEL_AIS_CLASS = (\"Class A\", \"Class B\")\n",
    "\n",
    "MIN_SEGMENT_LENGTH = 30     # datapoints\n",
    "MAX_TIME_GAP_SEC = 30       # seconds\n",
    "\n",
    "# Bounding Box to prefilter AIS data [lat_max, lon_min, lat_min, lon_max]\n",
    "bbox = [57.58, 10.5, 57.12, 11.92]\n",
    "\n",
    "# Polygon coordinates for precise Area of Interest (AOI) filtering (lon, lat)\n",
    "polygon_coords = [\n",
    "    (10.5162, 57.3500),  # coast top left (lon, lat)\n",
    "    (10.9314, 57.5120),  # sea top left\n",
    "    (11.5128, 57.5785),  # sea top right\n",
    "    (11.9132, 57.5230),  # top right (Swedish coast)\n",
    "    (11.9189, 57.4078),  # bottom right (Swedish coast)\n",
    "    (11.2133, 57.1389),  # sea bottom right\n",
    "    (11.0067, 57.1352),  # sea bottom left\n",
    "    (10.5400, 57.1880),  # coast bottom left\n",
    "    (10.5162, 57.3500),  # close polygon\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d18fd6d",
   "metadata": {},
   "source": [
    "### Imports for the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc178db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8db5",
   "metadata": {},
   "source": [
    "### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f2a99188",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/3 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-11-01\n",
      "Skipping 2025-11-01 download: already present in ais-data/csv folder\n",
      " Read AIS data: 988,647 rows within bbox,  241 unique vessels\n",
      " [filter_ais_df] Before filtering: 988,647 rows, 241 vessels\n",
      " [filter_ais_df] Type filtering: 950,987 rows (removed 37,660) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 950,965 rows, 238 vessels\n",
      " [filter_ais_df] Duplicate removal: 535,909 rows, 238 vessels\n",
      " [filter_ais_df] BBOX filtering: 535,909 rows (removed 0), 238 vessels\n",
      " [filter_ais_df] Polygon filtering: 276,112 rows (removed 259,797), 176 vessels\n",
      " [filter_ais_df] Port-area removal: removed 33,676 rows in 2 overlapping ports\n",
      " [filter_ais_df] Final: 242,436 rows, 172 unique vessels\n",
      " [segment_ais_tracks] Starting with 242,436 rows,  172 unique vessels\n",
      " [segment_ais_tracks] After MMSI-level filter: 95,465 rows,  128 vessels\n",
      " [segment_ais_tracks] After segment-level filter: 78,982 rows,  115 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  33%|███▎      | 1/3 [00:14<00:28, 14.05s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213582/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-11-02\n",
      "Skipping 2025-11-02 download: already present in ais-data/csv folder\n",
      " Read AIS data: 933,867 rows within bbox,  226 unique vessels\n",
      " [filter_ais_df] Before filtering: 933,867 rows, 226 vessels\n",
      " [filter_ais_df] Type filtering: 895,249 rows (removed 38,618) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 895,249 rows, 225 vessels\n",
      " [filter_ais_df] Duplicate removal: 500,728 rows, 225 vessels\n",
      " [filter_ais_df] BBOX filtering: 500,728 rows (removed 0), 225 vessels\n",
      " [filter_ais_df] Polygon filtering: 259,909 rows (removed 240,819), 152 vessels\n",
      " [filter_ais_df] Port-area removal: removed 37,230 rows in 2 overlapping ports\n",
      " [filter_ais_df] Final: 222,679 rows, 146 unique vessels\n",
      " [segment_ais_tracks] Starting with 222,679 rows,  146 unique vessels\n",
      " [segment_ais_tracks] After MMSI-level filter: 126,579 rows,  114 vessels\n",
      " [segment_ais_tracks] After segment-level filter: 103,116 rows,  115 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  67%|██████▋   | 2/3 [00:29<00:14, 14.62s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213582/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-11-03\n",
      "Skipping 2025-11-03 download: already present in ais-data/csv folder\n",
      " Read AIS data: 965,559 rows within bbox,  230 unique vessels\n",
      " [filter_ais_df] Before filtering: 965,559 rows, 230 vessels\n",
      " [filter_ais_df] Type filtering: 927,905 rows (removed 37,654) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 927,789 rows, 227 vessels\n",
      " [filter_ais_df] Duplicate removal: 526,789 rows, 227 vessels\n",
      " [filter_ais_df] BBOX filtering: 526,789 rows (removed 0), 227 vessels\n",
      " [filter_ais_df] Polygon filtering: 278,910 rows (removed 247,879), 153 vessels\n",
      " [filter_ais_df] Port-area removal: removed 37,542 rows in 2 overlapping ports\n",
      " [filter_ais_df] Final: 241,368 rows, 147 unique vessels\n",
      " [segment_ais_tracks] Starting with 241,368 rows,  147 unique vessels\n",
      " [segment_ais_tracks] After MMSI-level filter: 181,135 rows,  127 vessels\n",
      " [segment_ais_tracks] After segment-level filter: 143,895 rows,  132 segments\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 3/3 [00:45<00:00, 15.07s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213582/dark-vessel-hunter/ais-data/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- Create paths ---\n",
    "folder_path = Path(FOLDER_NAME)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "csv_folder_path = folder_path / \"csv\"\n",
    "csv_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "parquet_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_port_locations = folder_path / \"port_locodes.csv\"\n",
    "\n",
    "\n",
    "# --- If you want to download all csv files before, uncomment the line below ---\n",
    "# ais_downloader.download_multiple_ais_data(START_DATE, END_DATE, folder_path)\n",
    "\n",
    "# --- Build the schedule of download string dates ---\n",
    "dates = ais_downloader.get_work_dates(START_DATE, END_DATE, csv_folder_path, filter=False)\n",
    "\n",
    "# --- Iterate with tqdm and download, unzip and delete ---\n",
    "for day in tqdm(dates, desc=f\"Processing data\", unit=\"file\" ):\n",
    "    tag = f\"{day:%Y-%m}\" if day < date.fromisoformat(\"2024-03-01\") else f\"{day:%Y-%m-%d}\"\n",
    "    print(f\"\\nProcessing date: {tag}\")\n",
    "\n",
    "    # --- Download one day ---\n",
    "    csv_path = ais_downloader.download_one_ais_data(day, csv_folder_path)\n",
    "    \n",
    "    # --- Load CSV into DataFrame ---\n",
    "    df_raw = ais_reader.read_single_ais_df(csv_path, bbox, verbose=VERBOSE_MODE)\n",
    "    # --- Optionally delete the downloaded CSV file ---\n",
    "    if DELETE_DOWNLOADED_CSV: csv_path.unlink(missing_ok=True)\n",
    "    \n",
    "    # --- Filter and split ---\n",
    "    # Filter AIS data, keeping Class A and Class B by default,\n",
    "    df_filtered = ais_filtering.filter_ais_df(\n",
    "        df_raw,\n",
    "        polygon_coords=polygon_coords,\n",
    "        allowed_mobile_types=VESSEL_AIS_CLASS,\n",
    "        bbox=bbox,                          # select bbox \n",
    "        apply_polygon_filter=True,          # keep polygon filtering enabled boolean\n",
    "        remove_zero_sog_vessels=False,      # use True/False to enable/disable 90% zero-SOG removal\n",
    "        sog_in_knots=False,                 # convert SOG from knots in m/s (default) boolean\n",
    "        port_locodes_path=file_port_locations,\n",
    "        exclude_ports=True,                 # exclude port areas boolean \n",
    "        verbose=VERBOSE_MODE,               # verbose mode boolean\n",
    "    )\n",
    "\n",
    "    # --- Parquet conversion ---\n",
    "    # Segment and save to Parquet by MMSI\n",
    "    df_seg = ais_to_parquet.segment_ais_tracks(df_filtered, min_track_len=MIN_SEGMENT_LENGTH, max_time_gap_sec=MAX_TIME_GAP_SEC, verbose=VERBOSE_MODE)\n",
    "    # Save segmented data to Parquet files\n",
    "    ais_to_parquet.save_by_mmsi(df_seg, verbose=VERBOSE_MODE, output_folder=parquet_folder_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
