{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e568f0d6",
   "metadata": {},
   "source": [
    "# dark-vessel-hunter\n",
    "DTU Deep Learning project 29, group 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814a0",
   "metadata": {},
   "source": [
    "## Required Libraries Installation\n",
    "Run this in your terminal before executing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f609e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd44da",
   "metadata": {},
   "source": [
    "# Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347b624",
   "metadata": {},
   "source": [
    "## File imports for the data download and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4c203ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.data.ais_downloader as ais_downloader\n",
    "import src.data.ais_filtering as ais_filtering\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.data.ais_to_parquet as ais_to_parquet\n",
    "\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.pre_proc.ais_query as ais_query \n",
    "import src.utils.ais_maps as ais_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a859",
   "metadata": {},
   "source": [
    "## Library imports for the data download and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bc178db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675b5f",
   "metadata": {},
   "source": [
    "## Set data preferences and configuration inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c341eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_MODE = True\n",
    "\n",
    "START_DATE = \"2025-05-21\"  # Start date for data downloading\n",
    "END_DATE   = \"2025-05-21\"  # End date for data downloading\n",
    "\n",
    "AIS_DATA_NAME = config.AIS_DATA_FOLDER  # Name of the folder to store AIS data\n",
    "DELETE_DOWNLOADED_CSV = False           # Whether to delete raw downloaded CSV files after processing\n",
    "\n",
    "VESSEL_AIS_CLASS = (\"Class A\", \"Class B\") # AIS classes of vessels to include\n",
    "\n",
    "MIN_SEGMENT_LENGTH = 30     # datapoints\n",
    "MAX_TIME_GAP_SEC = 30       # seconds\n",
    "MIN_TRACK_DURATION_SEC = 60 * 60 # seconds\n",
    "\n",
    "REMOVE_ZERO_SOG_VESSELS = False # Whether to remove vessels with zero Speed Over Ground\n",
    "SOG_IN_MS = True                # If True, SOG is in meters/second; if False, SOG is in knots\n",
    "SOG_MIN_KNOTS = 0.5             # Minimum SOG in knots\n",
    "SOG_MAX_KNOTS = 35.0            # Maximum SOG in knots\n",
    "\n",
    "# Bounding Box to prefilter AIS data [lat_max, lon_min, lat_min, lon_max]\n",
    "BBOX = [57.58, 10.5, 57.12, 11.92]\n",
    "\n",
    "# Polygon coordinates for precise Area of Interest (AOI) filtering (lon, lat)\n",
    "POLYGON_COORDINATES = [\n",
    "    (10.5162, 57.3500),  # coast top left (lon, lat)\n",
    "    (10.9314, 57.5120),  # sea top left\n",
    "    (11.5128, 57.5785),  # sea top right\n",
    "    (11.9132, 57.5230),  # top right (Swedish coast)\n",
    "    (11.9189, 57.4078),  # bottom right (Swedish coast)\n",
    "    (11.2133, 57.1389),  # sea bottom right\n",
    "    (11.0067, 57.1352),  # sea bottom left\n",
    "    (10.5400, 57.1880),  # coast bottom left\n",
    "    (10.5162, 57.3500),  # close polygon\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8db5",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f2a99188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create paths ---\n",
    "folder_path = Path(AIS_DATA_NAME)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "csv_folder_path = folder_path / \"csv\"\n",
    "csv_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "parquet_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_port_locations = folder_path / \"port_locodes.csv\" # Path to port locations file\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- If you want to download all csv files before, uncomment the line below ---\n",
    "# ais_downloader.download_multiple_ais_data(START_DATE, END_DATE, folder_path)\n",
    "\n",
    "# --- Build the schedule of download string dates ---\n",
    "dates = ais_downloader.get_work_dates(START_DATE, END_DATE, csv_folder_path, filter=False)\n",
    "\n",
    "# --- Iterate with tqdm and download, unzip and delete ---\n",
    "for day in tqdm(dates, desc=f\"Processing data\", unit=\"file\" ):\n",
    "    tag = f\"{day:%Y-%m}\" if day < date.fromisoformat(\"2024-03-01\") else f\"{day:%Y-%m-%d}\"\n",
    "    print(f\"\\nProcessing date: {tag}\")\n",
    "\n",
    "    # --- Download one day ---\n",
    "    csv_path = ais_downloader.download_one_ais_data(day, csv_folder_path)\n",
    "    \n",
    "    # --- Load CSV into DataFrame ---\n",
    "    df_raw = ais_reader.read_single_ais_df(csv_path, BBOX, verbose=VERBOSE_MODE)\n",
    "    # --- Optionally delete the downloaded CSV file ---\n",
    "    if DELETE_DOWNLOADED_CSV: csv_path.unlink(missing_ok=True)\n",
    "    \n",
    "    # --- Filter and split ---\n",
    "    # Filter AIS data, keeping Class A and Class B by default,\n",
    "    df_filtered = ais_filtering.filter_ais_df(\n",
    "            df_raw,                                               # raw AIS DataFrame\n",
    "            polygon_coords=POLYGON_COORDINATES,                   # polygon coordinates for precise AOI filtering\n",
    "            allowed_mobile_types=VESSEL_AIS_CLASS,                # vessel AIS class filter\n",
    "            apply_polygon_filter=True,                            # keep polygon filtering enabled boolean\n",
    "            remove_zero_sog_vessels=REMOVE_ZERO_SOG_VESSELS,      # use True/False to enable/disable 90% zero-SOG removal\n",
    "            output_sog_in_ms=SOG_IN_MS,                           # convert SOG from knots in m/s (default) boolean\n",
    "            sog_min_knots=SOG_MIN_KNOTS,                          # min SOG in knots to keep (None to disable)\n",
    "            sog_max_knots=SOG_MAX_KNOTS,                          # max SOG in knots to keep (None to disable) \n",
    "            port_locodes_path=file_port_locations,                # path to port locodes CSV\n",
    "            exclude_ports=True,                                   # exclude port areas boolean \n",
    "            verbose=VERBOSE_MODE,                                 # verbose mode boolean\n",
    "        )\n",
    "        \n",
    "    # --- Parquet conversion ---\n",
    "    # Segment and save to Parquet by MMSI\n",
    "    df_seg = ais_to_parquet.segment_ais_tracks(df_filtered, min_track_len=MIN_SEGMENT_LENGTH, max_time_gap_sec=MAX_TIME_GAP_SEC, min_track_duration_sec=MIN_TRACK_DURATION_SEC, verbose=VERBOSE_MODE)\n",
    "    # Save segmented data to Parquet files\n",
    "    ais_to_parquet.save_by_mmsi(df_seg, verbose=VERBOSE_MODE, output_folder=parquet_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa885c2d",
   "metadata": {},
   "source": [
    "## Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51e29060",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[read_raw_csv_with_filters] 513,285 rows, 207 vessels; no time filter applied\n",
      " [filter_ais_df] Before filtering: 513,285 rows, 207 vessels\n",
      " [filter_ais_df] Type filtering: 476,649 rows (removed 36,636) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 476,646 rows, 205 vessels\n",
      " [filter_ais_df] Duplicate removal: 280,803 rows, 205 vessels\n",
      " [filter_ais_df] Polygon filtering: 280,803 rows (removed 0), 205 vessels\n",
      " [filter_ais_df] Port-area removal: removed 141,579 rows in 3 overlapping ports\n",
      " [filter_ais_df] SOG sanity: 135,422 rows (removed 3,773) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 135,422 rows, 180 unique vessels (SOG in m/s)\n",
      "[query_ais_duckdb] SQL:\n",
      " SELECT * FROM read_parquet('ais-data/parquet/**/*.parquet') WHERE 1=1\n",
      "[query_ais_duckdb] 95,498 rows, 121 vessels; no time filter applied\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "df_csv = ais_reader.read_raw_csv_with_filters(\n",
    "    csv_name=\"aisdk-2025-05-21.csv\",\n",
    "    bbox=BBOX,\n",
    "    time_start=None,\n",
    "    time_end=None,\n",
    "    csv_root=\"ais-data/csv\",\n",
    "    timestamp_format=\"%d/%m/%Y %H:%M:%S\",\n",
    "    polygon_coords=POLYGON_COORDINATES,\n",
    "    verbose=True,\n",
    ")\n",
    "df_csv_filtered = ais_filtering.filter_ais_df(\n",
    "    df_csv,\n",
    "    polygon_coords=POLYGON_COORDINATES,\n",
    "    allowed_mobile_types=VESSEL_AIS_CLASS,\n",
    "    apply_polygon_filter=True,\n",
    "    remove_zero_sog_vessels=REMOVE_ZERO_SOG_VESSELS,\n",
    "    output_sog_in_ms=SOG_IN_MS,\n",
    "    sog_min_knots=SOG_MIN_KNOTS,\n",
    "    sog_max_knots=SOG_MAX_KNOTS,\n",
    "    port_locodes_path=file_port_locations\n",
    ",\n",
    "    exclude_ports=True,     \n",
    "    verbose=True,\n",
    ")\n",
    "df_parquet = ais_query.query_ais_duckdb(root_path=\"ais-data/parquet\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1cfbcf9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "m_parquet = ais_maps.make_ais_tracks_map(df_list=[df_parquet], \n",
    "    bbox=BBOX,\n",
    "    polygon_coords=POLYGON_COORDINATES,\n",
    "    max_vessels=None, \n",
    ")\n",
    "\n",
    "m_parquet.save(\"ais_tracks_map_parquet.html\")\n",
    "\n",
    "\n",
    "m_csv = ais_maps.make_ais_tracks_map(df_list=[df_csv], \n",
    "    bbox=BBOX,\n",
    "    polygon_coords=POLYGON_COORDINATES,\n",
    "    max_vessels=None,\n",
    ")\n",
    "\n",
    "m_csv.save(\"ais_tracks_map_csv.html\")\n",
    "\n",
    "\n",
    "m_csv_filtered = ais_maps.make_ais_tracks_map(df_list=[df_csv_filtered], \n",
    "    bbox=BBOX,\n",
    "    polygon_coords=POLYGON_COORDINATES,\n",
    "    max_vessels=None,\n",
    ")\n",
    "\n",
    "m_csv_filtered.save(\"ais_tracks_map_csv_filtered.html\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb7e37",
   "metadata": {},
   "source": [
    "## Pre processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50077b83",
   "metadata": {},
   "source": [
    "#### File imports for the pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.pre_proc.pre_processing_utils as pre_processing_utils\n",
    "import src.pre_proc.ais_query as ais_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417146",
   "metadata": {},
   "source": [
    "#### Library imports for the pre processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465104",
   "metadata": {},
   "source": [
    "#### Set pre processing preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_DATA_NAME = config.AIS_DATA_FOLDER\n",
    "folder_path = Path(AIS_DATA_NAME)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "\n",
    "SEGMENT_MAX_LENGTH = 300  # datapoints\n",
    "\n",
    "NUMERIC_COLS = config.NUMERIC_COLS\n",
    "# if u want to do it withouth a end date comment next line\n",
    "TRAIN_START_DATE = \"2025-10-20\"\n",
    "TRAIN_END_DATE = \"2025-11-08\"\n",
    "\n",
    "TEST_START_DATE = \"2025-11-09\"\n",
    "TEST_END_DATE = \"2025-11-10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e201661",
   "metadata": {},
   "source": [
    "#### Pre processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pre_processing(dataframe_type: str = \"all\"):\n",
    "\n",
    "    if dataframe_type == \"all\":\n",
    "        main_pre_processing(\"train\")\n",
    "        main_pre_processing(\"test\")\n",
    "        return\n",
    "        \n",
    "    elif dataframe_type == \"train\":\n",
    "        print(f\"[pre_processing] Querying AIS data for training period: {TRAIN_START_DATE} to {TRAIN_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        dates = (\n",
    "            pd.date_range(TRAIN_START_DATE, TRAIN_END_DATE, freq=\"D\")\n",
    "            .strftime(\"%Y-%m-%d\")\n",
    "            .tolist()\n",
    "        )\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, dates=dates, verbose=VERBOSE_MODE)\n",
    "        \n",
    "    elif dataframe_type == \"test\":\n",
    "        print(f\"[pre_processing] Querying AIS data for testing period: {TEST_START_DATE} to {TEST_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        dates = (\n",
    "            pd.date_range(TEST_START_DATE, TEST_END_DATE, freq=\"D\")\n",
    "            .strftime(\"%Y-%m-%d\")\n",
    "            .tolist()\n",
    "        )\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, dates=dates, verbose=VERBOSE_MODE)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataframe_type: {dataframe_type}. Must be 'train' or 'test'.\")\n",
    "    \n",
    "\n",
    "    # Dropping unnecessary columns and rows with missing values\n",
    "    print(f\"[pre_processing] Initial data size: {len(df)} records.\")\n",
    "    print(f\"[pre_processing] Dropping unnecessary columns and rows with missing values...\")\n",
    "    df.drop(columns=[ \n",
    "        'Type of mobile', \n",
    "        'ROT', \n",
    "        'Heading', \n",
    "        'IMO', \n",
    "        'Callsign', \n",
    "        'Name', \n",
    "        'Navigational status',\n",
    "        'Cargo type', \n",
    "        'Width', \n",
    "        'Length',\n",
    "        'Type of position fixing device', \n",
    "        'Draught', \n",
    "        'Destination', \n",
    "        'ETA',\n",
    "        'Data source type', \n",
    "        'A', 'B', 'C', 'D', \n",
    "        'Date'], inplace=True, errors='ignore')\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"[pre_processing] Data size after dropping: {len(df)} records.\")\n",
    "\n",
    "    # Grouping Ship types\n",
    "    commercial_types = [\"Cargo\", \"Tanker\"]\n",
    "    passenger_types = [\"Passenger\", \"Pleasure\", \"Sailing\"]\n",
    "    service_types = [\"Dredging\", \"Law enforcement\", \"Military\", \"Port tender\", \"SAR\", \"Towing\", \"Towing long/wide\",\"Tug\"]\n",
    "    valid_types =  [\"Fishing\", \"Service\", \"Commercial\", \"Passenger\"]\n",
    "\n",
    "    df.loc[df[\"Ship type\"].isin(commercial_types), \"Ship type\"] = \"Commercial\"\n",
    "    df.loc[df[\"Ship type\"].isin(passenger_types), \"Ship type\"] = \"Passenger\"\n",
    "    df.loc[df[\"Ship type\"].isin(service_types), \"Ship type\"] = \"Service\"\n",
    "    df.loc[~df[\"Ship type\"].isin(valid_types), \"Ship type\"] = \"Other\"\n",
    "\n",
    "    print(\"[pre_processing] Ship type counts:\")\n",
    "    print(df[\"Ship type\"].value_counts())\n",
    "\n",
    "    # Adding â–³T feature\n",
    "    df = pre_processing_utils.add_delta_t(df)\n",
    "    df.drop(columns=[\"DeltaT\"], inplace=True)\n",
    "\n",
    "    # Splitting segments\n",
    "    print(f\"[pre_processing] Splitting segments to max length {SEGMENT_MAX_LENGTH}...\")\n",
    "    df = pre_processing_utils.split_segments_fixed_length(df, max_len=SEGMENT_MAX_LENGTH)\n",
    "\n",
    "    # Normalizing numeric columns\n",
    "    df, mean, std = pre_processing_utils.normalize_df(df, NUMERIC_COLS)\n",
    "\n",
    "    # Encoding Navicational Status as one-hot\n",
    "    #df, nav_status_to_id = pre_processing_utils.one_hot_encode_nav_status(df)\n",
    "\n",
    "    # Ship type labeling (mapping to be used later)\n",
    "    df, ship_type_to_id = pre_processing_utils.label_ship_types(df)\n",
    "    \n",
    "    # Saving pre-processed DataFrame\n",
    "    if dataframe_type == \"train\":\n",
    "        print(f\"[pre_processing] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TRAIN_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "        metadata_path = config.PRE_PROCESSING_METADATA_TRAIN_PATH\n",
    "    else:\n",
    "        print(f\"[pre_processing] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TEST_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "        metadata_path = config.PRE_PROCESSING_METADATA_TEST_PATH\n",
    "\n",
    "    print(f\"[pre_processing] Columns of pre-processed DataFrame:\\n{df.columns.tolist()}\")\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "\n",
    "    # Saving preprocessing metadata\n",
    "    print(f\"[pre_processing] Saving preprocessing metadata to {metadata_path}...\")\n",
    "    meta = {\n",
    "        \"mean\": mean.tolist(),\n",
    "        \"std\": std.tolist(),\n",
    "        #\"nav_status_to_id\": nav_status_to_id,\n",
    "        \"ship_type_to_id\": ship_type_to_id\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf852",
   "metadata": {},
   "source": [
    "#### Pre processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597faf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pre_processing(\"train\")\n",
    "main_pre_processing(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724caf9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f656cc",
   "metadata": {},
   "source": [
    "#### File imports for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de52b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d73047b",
   "metadata": {},
   "source": [
    "#### Library imports for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "927b9e0b",
   "metadata": {},
   "source": [
    "#### Set training preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9953ac75",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca7304f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb787",
   "metadata": {},
   "source": [
    "#### File imports for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd90563",
   "metadata": {},
   "source": [
    "#### Library imports for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c86bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df15213e",
   "metadata": {},
   "source": [
    "#### Set evaluation preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9e93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0d3719",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb45a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb69a421",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6993541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
