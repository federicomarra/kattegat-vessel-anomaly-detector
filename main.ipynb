{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e568f0d6",
   "metadata": {},
   "source": [
    "# vessel-cable-anomaly-hunter\n",
    "DTU Deep Learning project 29, group 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814a0",
   "metadata": {},
   "source": [
    "## Required Libraries Installation\n",
    "Run this in your terminal before executing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9c6f609e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datetime in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 1)) (5.5)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: pathlib in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 3)) (1.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 4)) (2.32.5)\n",
      "Requirement already satisfied: zipfile36 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 5)) (0.1.3)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 6)) (2.3.2)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 7)) (19.0.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 8)) (1.7.2)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 9)) (1.26.4)\n",
      "Requirement already satisfied: seaborn in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 10)) (0.13.2)\n",
      "Requirement already satisfied: shapely in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 11)) (2.1.2)\n",
      "Requirement already satisfied: folium in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 12)) (0.20.0)\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 13)) (3.10.6)\n",
      "Requirement already satisfied: fastparquet in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 14)) (2024.11.0)\n",
      "Requirement already satisfied: PyYAML in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 15)) (6.0.3)\n",
      "Requirement already satisfied: typing in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 16)) (3.7.4.3)\n",
      "Requirement already satisfied: duckdb in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 17)) (1.4.2)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from -r requirements.txt (line 18)) (0.24.1)\n",
      "Requirement already satisfied: zope.interface in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from datetime->-r requirements.txt (line 1)) (8.1)\n",
      "Requirement already satisfied: pytz in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from datetime->-r requirements.txt (line 1)) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from requests->-r requirements.txt (line 4)) (2025.10.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 6)) (2.9.0.post0)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from pandas->-r requirements.txt (line 6)) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from scikit-learn->-r requirements.txt (line 8)) (3.6.0)\n",
      "Requirement already satisfied: branca>=0.6.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from folium->-r requirements.txt (line 12)) (0.8.2)\n",
      "Requirement already satisfied: jinja2>=2.9 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from folium->-r requirements.txt (line 12)) (3.1.6)\n",
      "Requirement already satisfied: xyzservices in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from folium->-r requirements.txt (line 12)) (2025.10.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (4.59.2)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from matplotlib->-r requirements.txt (line 13)) (3.2.0)\n",
      "Requirement already satisfied: cramjam>=2.3 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from fastparquet->-r requirements.txt (line 14)) (2.11.0)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from fastparquet->-r requirements.txt (line 14)) (2024.12.0)\n",
      "Requirement already satisfied: torch==2.9.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from torchvision->-r requirements.txt (line 18)) (2.9.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from torch==2.9.1->torchvision->-r requirements.txt (line 18)) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from torch==2.9.1->torchvision->-r requirements.txt (line 18)) (4.15.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from torch==2.9.1->torchvision->-r requirements.txt (line 18)) (1.14.0)\n",
      "Requirement already satisfied: networkx>=2.5.1 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from torch==2.9.1->torchvision->-r requirements.txt (line 18)) (3.5)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from jinja2>=2.9->folium->-r requirements.txt (line 12)) (3.0.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas->-r requirements.txt (line 6)) (1.17.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/dl/lib/python3.11/site-packages (from sympy>=1.13.3->torch==2.9.1->torchvision->-r requirements.txt (line 18)) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd44da",
   "metadata": {},
   "source": [
    "## 1) Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347b624",
   "metadata": {},
   "source": [
    "#### File imports for the data download and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c203ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.data.ais_downloader as ais_downloader\n",
    "import src.data.ais_filtering as ais_filtering\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.data.ais_to_parquet as ais_to_parquet\n",
    "\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.pre_proc.ais_query as ais_query \n",
    "import src.utils.ais_maps as ais_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a859",
   "metadata": {},
   "source": [
    "#### Library imports for the data download and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc178db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675b5f",
   "metadata": {},
   "source": [
    "#### Set data preferences and configuration inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c341eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_MODE = config.VERBOSE_MODE                          # Whether to print verbose output\n",
    "\n",
    "START_DATE = config.START_DATE                              # Start date for data downloading\n",
    "END_DATE   = config.END_DATE                                # End date for data downloading\n",
    "\n",
    "AIS_DATA_NAME = config.AIS_DATA_FOLDER                      # Name of the folder to store AIS data\n",
    "DELETE_DOWNLOADED_CSV = config.DELETE_DOWNLOADED_CSV        # Whether to delete raw downloaded CSV files after processing\n",
    "\n",
    "VESSEL_AIS_CLASS = config.VESSEL_AIS_CLASS                  # AIS classes of vessels to include\n",
    "\n",
    "REMOVE_ZERO_SOG_VESSELS = config.REMOVE_ZERO_SOG_VESSELS    # Whether to remove vessels with zero Speed Over Ground\n",
    "SOG_IN_MS = config.SOG_IN_MS                                # If True, SOG is in meters/second; if False, SOG is in knots\n",
    "SOG_MIN_KNOTS = config.SOG_MIN_KNOTS                        # Minimum SOG in knots\n",
    "SOG_MAX_KNOTS = config.SOG_MAX_KNOTS                        # Maximum SOG in knots\n",
    "\n",
    "BBOX = config.BBOX                                          # Bounding Box to prefilter AIS data\n",
    "POLYGON_COORDINATES = config.POLYGON_COORDINATES            # Polygon coordinates for filter Area of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8db5",
   "metadata": {},
   "source": [
    "#### Create Directories and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a99188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create paths ---\n",
    "folder_path = Path(AIS_DATA_NAME)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "csv_folder_path = folder_path / \"csv\"\n",
    "csv_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "parquet_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_port_locations = folder_path / \"port_locodes.csv\" # Path to port locations file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98948aa7",
   "metadata": {},
   "source": [
    "#### Main Script\n",
    "1) Download one single .csv AIS data file from http://aisdata.ais.dk (link to data column description http://aisdata.ais.dk/!_README_information_CSV_files.txt);\n",
    "2) For a given AOI in Denmark with known cable positions, filter AIS messages by cleansing unrealistic/unphysical messages or duplicates and removes error-prone messages within port areas;\n",
    "3) Segmentation of vessel tracks per MMSI and date;\n",
    "4) Parquet Conversion;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24af940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/30 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-08-01\n",
      "Starting download and extraction for 2025-08-01\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-01 zip file: 100%|██████████| 661M/661M [00:23<00:00, 29.8MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:17<00:00, 17.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-01\n",
      "Completed download and extraction for 2025-08-01\n",
      " Read AIS data: 1,128,873 rows within bbox,  511 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,128,873 rows, 511 vessels\n",
      " [filter_ais_df] Type filtering: 1,093,101 rows (removed 35,772) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,093,079 rows, 508 vessels\n",
      " [filter_ais_df] Duplicate removal: 638,467 rows, 508 vessels\n",
      " [filter_ais_df] Polygon filtering: 337,847 rows (removed 300,620), 378 vessels\n",
      " [filter_ais_df] Port-area removal: removed 160,778 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 175,598 rows (removed 1,471) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 173,215 rows (removed 2,381) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 173,215 rows, 327 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 327 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   3%|▎         | 1/30 [01:07<32:33, 67.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-02\n",
      "Starting download and extraction for 2025-08-02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-02 zip file: 100%|██████████| 662M/662M [00:19<00:00, 35.2MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:17<00:00, 17.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-02\n",
      "Completed download and extraction for 2025-08-02\n",
      " Read AIS data: 1,161,214 rows within bbox,  452 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,161,214 rows, 452 vessels\n",
      " [filter_ais_df] Type filtering: 1,120,127 rows (removed 41,087) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,114,486 rows, 447 vessels\n",
      " [filter_ais_df] Duplicate removal: 616,865 rows, 447 vessels\n",
      " [filter_ais_df] Polygon filtering: 314,494 rows (removed 302,371), 342 vessels\n",
      " [filter_ais_df] Port-area removal: removed 168,854 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 145,303 rows (removed 337) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 144,010 rows (removed 1,291) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 144,010 rows, 290 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 290 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   7%|▋         | 2/30 [02:06<29:06, 62.38s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-03\n",
      "Starting download and extraction for 2025-08-03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-03 zip file: 100%|██████████| 624M/624M [00:18<00:00, 34.7MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:15<00:00, 15.90s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-03\n",
      "Completed download and extraction for 2025-08-03\n",
      " Read AIS data: 1,079,891 rows within bbox,  399 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,079,891 rows, 399 vessels\n",
      " [filter_ais_df] Type filtering: 1,043,284 rows (removed 36,607) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,043,284 rows, 398 vessels\n",
      " [filter_ais_df] Duplicate removal: 599,515 rows, 398 vessels\n",
      " [filter_ais_df] Polygon filtering: 319,478 rows (removed 280,037), 288 vessels\n",
      " [filter_ais_df] Port-area removal: removed 165,183 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 153,909 rows (removed 386) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 152,641 rows (removed 1,266) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 152,641 rows, 248 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 248 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  10%|█         | 3/30 [02:58<26:01, 57.82s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-04\n",
      "Starting download and extraction for 2025-08-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-04 zip file: 100%|██████████| 643M/643M [00:24<00:00, 27.7MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:16<00:00, 16.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-04\n",
      "Completed download and extraction for 2025-08-04\n",
      " Read AIS data: 1,161,332 rows within bbox,  380 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,161,332 rows, 380 vessels\n",
      " [filter_ais_df] Type filtering: 1,125,031 rows (removed 36,301) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,125,031 rows, 379 vessels\n",
      " [filter_ais_df] Duplicate removal: 637,785 rows, 379 vessels\n",
      " [filter_ais_df] Polygon filtering: 321,848 rows (removed 315,937), 272 vessels\n",
      " [filter_ais_df] Port-area removal: removed 156,758 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 164,980 rows (removed 110) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 162,923 rows (removed 2,057) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 162,923 rows, 232 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 232 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  13%|█▎        | 4/30 [03:56<25:05, 57.89s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-05\n",
      "Starting download and extraction for 2025-08-05\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-05 zip file: 100%|██████████| 569M/569M [00:15<00:00, 39.3MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:14<00:00, 14.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-05\n",
      "Completed download and extraction for 2025-08-05\n",
      " Read AIS data: 1,157,746 rows within bbox,  268 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,157,746 rows, 268 vessels\n",
      " [filter_ais_df] Type filtering: 1,119,953 rows (removed 37,793) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,116,828 rows, 264 vessels\n",
      " [filter_ais_df] Duplicate removal: 613,845 rows, 264 vessels\n",
      " [filter_ais_df] Polygon filtering: 301,015 rows (removed 312,830), 175 vessels\n",
      " [filter_ais_df] Port-area removal: removed 171,761 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 129,228 rows (removed 26) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 127,182 rows (removed 2,046) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 127,182 rows, 115 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 115 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  17%|█▋        | 5/30 [04:44<22:34, 54.17s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-06\n",
      "Starting download and extraction for 2025-08-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-06 zip file: 100%|██████████| 580M/580M [00:17<00:00, 34.3MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:14<00:00, 14.87s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-06\n",
      "Completed download and extraction for 2025-08-06\n",
      " Read AIS data: 1,145,520 rows within bbox,  261 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,145,520 rows, 261 vessels\n",
      " [filter_ais_df] Type filtering: 1,108,897 rows (removed 36,623) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,108,897 rows, 260 vessels\n",
      " [filter_ais_df] Duplicate removal: 622,309 rows, 260 vessels\n",
      " [filter_ais_df] Polygon filtering: 300,756 rows (removed 321,553), 169 vessels\n",
      " [filter_ais_df] Port-area removal: removed 163,859 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 136,873 rows (removed 24) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 134,698 rows (removed 2,175) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 134,698 rows, 123 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 123 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  20%|██        | 6/30 [05:32<20:52, 52.17s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-07\n",
      "Starting download and extraction for 2025-08-07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-07 zip file: 100%|██████████| 664M/664M [00:25<00:00, 27.3MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:17<00:00, 17.07s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-07\n",
      "Completed download and extraction for 2025-08-07\n",
      " Read AIS data: 1,202,886 rows within bbox,  385 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,202,886 rows, 385 vessels\n",
      " [filter_ais_df] Type filtering: 1,166,600 rows (removed 36,286) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,162,713 rows, 382 vessels\n",
      " [filter_ais_df] Duplicate removal: 642,517 rows, 382 vessels\n",
      " [filter_ais_df] Polygon filtering: 301,062 rows (removed 341,455), 277 vessels\n",
      " [filter_ais_df] Port-area removal: removed 133,073 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 167,842 rows (removed 147) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 166,354 rows (removed 1,488) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 166,354 rows, 247 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 247 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  23%|██▎       | 7/30 [06:35<21:20, 55.66s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-08\n",
      "Starting download and extraction for 2025-08-08\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-08 zip file: 100%|██████████| 713M/713M [00:20<00:00, 36.1MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:18<00:00, 18.44s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-08\n",
      "Completed download and extraction for 2025-08-08\n",
      " Read AIS data: 1,160,724 rows within bbox,  399 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,160,724 rows, 399 vessels\n",
      " [filter_ais_df] Type filtering: 1,124,268 rows (removed 36,456) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,124,255 rows, 397 vessels\n",
      " [filter_ais_df] Duplicate removal: 618,010 rows, 397 vessels\n",
      " [filter_ais_df] Polygon filtering: 270,458 rows (removed 347,552), 285 vessels\n",
      " [filter_ais_df] Port-area removal: removed 115,708 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 154,292 rows (removed 458) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 151,933 rows (removed 2,359) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 151,933 rows, 262 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 262 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  27%|██▋       | 8/30 [07:32<20:38, 56.28s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-09\n",
      "Starting download and extraction for 2025-08-09\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-09 zip file: 100%|██████████| 723M/723M [00:25<00:00, 29.4MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:18<00:00, 18.84s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-09\n",
      "Completed download and extraction for 2025-08-09\n",
      " Read AIS data: 1,206,051 rows within bbox,  424 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,206,051 rows, 424 vessels\n",
      " [filter_ais_df] Type filtering: 1,168,258 rows (removed 37,793) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,168,258 rows, 423 vessels\n",
      " [filter_ais_df] Duplicate removal: 649,751 rows, 423 vessels\n",
      " [filter_ais_df] Polygon filtering: 292,802 rows (removed 356,949), 308 vessels\n",
      " [filter_ais_df] Port-area removal: removed 132,829 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 159,617 rows (removed 356) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 158,925 rows (removed 692) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 158,925 rows, 271 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 271 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  30%|███       | 9/30 [08:49<21:54, 62.57s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-10\n",
      "Starting download and extraction for 2025-08-10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-10 zip file: 100%|██████████| 626M/626M [00:19<00:00, 34.2MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:16<00:00, 16.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-10\n",
      "Completed download and extraction for 2025-08-10\n",
      " Read AIS data: 1,032,887 rows within bbox,  304 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,032,887 rows, 304 vessels\n",
      " [filter_ais_df] Type filtering: 996,909 rows (removed 35,978) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 996,909 rows, 303 vessels\n",
      " [filter_ais_df] Duplicate removal: 577,793 rows, 303 vessels\n",
      " [filter_ais_df] Polygon filtering: 253,367 rows (removed 324,426), 202 vessels\n",
      " [filter_ais_df] Port-area removal: removed 135,402 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 117,942 rows (removed 23) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 116,931 rows (removed 1,011) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 116,931 rows, 160 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 160 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  33%|███▎      | 10/30 [09:39<19:36, 58.81s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-11\n",
      "Starting download and extraction for 2025-08-11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-11 zip file: 100%|██████████| 670M/670M [00:19<00:00, 35.7MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:17<00:00, 17.01s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-11\n",
      "Completed download and extraction for 2025-08-11\n",
      " Read AIS data: 1,156,494 rows within bbox,  368 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,156,494 rows, 368 vessels\n",
      " [filter_ais_df] Type filtering: 1,120,818 rows (removed 35,676) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,120,789 rows, 365 vessels\n",
      " [filter_ais_df] Duplicate removal: 658,127 rows, 365 vessels\n",
      " [filter_ais_df] Polygon filtering: 332,555 rows (removed 325,572), 275 vessels\n",
      " [filter_ais_df] Port-area removal: removed 128,044 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 204,484 rows (removed 27) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 201,774 rows (removed 2,710) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 201,774 rows, 247 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 247 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  37%|███▋      | 11/30 [10:39<18:41, 59.01s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-12\n",
      "Starting download and extraction for 2025-08-12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-12 zip file: 100%|██████████| 792M/792M [00:23<00:00, 36.0MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:21<00:00, 21.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-12\n",
      "Completed download and extraction for 2025-08-12\n",
      " Read AIS data: 1,266,622 rows within bbox,  410 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,266,622 rows, 410 vessels\n",
      " [filter_ais_df] Type filtering: 1,229,954 rows (removed 36,668) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,229,743 rows, 407 vessels\n",
      " [filter_ais_df] Duplicate removal: 684,142 rows, 407 vessels\n",
      " [filter_ais_df] Polygon filtering: 369,055 rows (removed 315,087), 295 vessels\n",
      " [filter_ais_df] Port-area removal: removed 115,656 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 252,095 rows (removed 1,304) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 250,511 rows (removed 1,581) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 250,511 rows, 276 unique vessels (SOG in m/s)\n",
      " [save_by_mmsi] Removed existing partitions for 276 (MMSI, Date) combinations.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  40%|████      | 12/30 [11:47<18:35, 61.96s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /dtu/blackhole/0e/213550/dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-13\n",
      "Starting download and extraction for 2025-08-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading 2025-08-13 zip file: 100%|██████████| 929M/929M [00:25<00:00, 38.2MB/s]\n",
      "Unzipping into ais-data/csv folder : 100%|██████████| 1/1 [00:25<00:00, 25.73s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted downloaded zip file for 2025-08-13\n",
      "Completed download and extraction for 2025-08-13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  40%|████      | 12/30 [12:55<19:22, 64.59s/file]\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 137. MiB for an array with shape (10, 1795771) and data type object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m csv_path \u001b[38;5;241m=\u001b[39m ais_downloader\u001b[38;5;241m.\u001b[39mdownload_one_ais_data(day, csv_folder_path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# --- Load CSV into DataFrame ---\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m df_raw \u001b[38;5;241m=\u001b[39m \u001b[43mais_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_single_ais_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcsv_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBBOX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mVERBOSE_MODE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# --- Optionally delete the downloaded CSV file ---\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m DELETE_DOWNLOADED_CSV: csv_path\u001b[38;5;241m.\u001b[39munlink(missing_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/src/data/ais_reader.py:59\u001b[0m, in \u001b[0;36mread_single_ais_df\u001b[0;34m(csv_path, bbox, verbose)\u001b[0m\n\u001b[1;32m     47\u001b[0m lat_max, lon_min, lat_min, lon_max \u001b[38;5;241m=\u001b[39m bbox\n\u001b[1;32m     49\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;124mSELECT *\u001b[39m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;124mFROM read_csv_auto(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcsv_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, AUTO_DETECT=TRUE, ignore_errors=TRUE)\u001b[39m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;124m;\u001b[39m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;124m\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 59\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mduckdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mquery\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_df\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;66;03m# Rename Timestamp column and parse to datetime\u001b[39;00m\n\u001b[1;32m     62\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m# Timestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/frame.py:1926\u001b[0m, in \u001b[0;36mDataFrame.from_dict\u001b[0;34m(cls, data, orient, dtype, columns)\u001b[0m\n\u001b[1;32m   1920\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1921\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m or \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m for orient parameter. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1922\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00morient\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m instead\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1923\u001b[0m     )\n\u001b[1;32m   1925\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtight\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 1926\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1928\u001b[0m     realdata \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/frame.py:782\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    776\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_mgr(\n\u001b[1;32m    777\u001b[0m         data, axes\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m: index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: columns}, dtype\u001b[38;5;241m=\u001b[39mdtype, copy\u001b[38;5;241m=\u001b[39mcopy\n\u001b[1;32m    778\u001b[0m     )\n\u001b[1;32m    780\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    781\u001b[0m     \u001b[38;5;66;03m# GH#38939 de facto copy defaults to False only in non-dict cases\u001b[39;00m\n\u001b[0;32m--> 782\u001b[0m     mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdict_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmanager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    783\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ma\u001b[38;5;241m.\u001b[39mMaskedArray):\n\u001b[1;32m    784\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mma\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m mrecords\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:503\u001b[0m, in \u001b[0;36mdict_to_mgr\u001b[0;34m(data, index, columns, dtype, typ, copy)\u001b[0m\n\u001b[1;32m    499\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;66;03m# dtype check to exclude e.g. range objects, scalars\u001b[39;00m\n\u001b[1;32m    501\u001b[0m         arrays \u001b[38;5;241m=\u001b[39m [x\u001b[38;5;241m.\u001b[39mcopy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(x, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m arrays]\n\u001b[0;32m--> 503\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marrays_to_mgr\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtyp\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtyp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/internals/construction.py:152\u001b[0m, in \u001b[0;36marrays_to_mgr\u001b[0;34m(arrays, columns, index, dtype, verify_integrity, typ, consolidate)\u001b[0m\n\u001b[1;32m    149\u001b[0m axes \u001b[38;5;241m=\u001b[39m [columns, index]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mblock\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcreate_block_manager_from_column_arrays\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m        \u001b[49m\u001b[43marrays\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconsolidate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrefs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrefs\u001b[49m\n\u001b[1;32m    154\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ArrayManager(arrays, [index, columns])\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2163\u001b[0m, in \u001b[0;36mcreate_block_manager_from_column_arrays\u001b[0;34m(arrays, axes, consolidate, refs)\u001b[0m\n\u001b[1;32m   2161\u001b[0m     raise_construction_error(\u001b[38;5;28mlen\u001b[39m(arrays), arrays[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape, axes, e)\n\u001b[1;32m   2162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m consolidate:\n\u001b[0;32m-> 2163\u001b[0m     \u001b[43mmgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_consolidate_inplace\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2164\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mgr\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1807\u001b[0m, in \u001b[0;36mBlockManager._consolidate_inplace\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1801\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_consolidate_inplace\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1802\u001b[0m     \u001b[38;5;66;03m# In general, _consolidate_inplace should only be called via\u001b[39;00m\n\u001b[1;32m   1803\u001b[0m     \u001b[38;5;66;03m#  DataFrame._consolidate_inplace, otherwise we will fail to invalidate\u001b[39;00m\n\u001b[1;32m   1804\u001b[0m     \u001b[38;5;66;03m#  the DataFrame's _item_cache. The exception is for newly-created\u001b[39;00m\n\u001b[1;32m   1805\u001b[0m     \u001b[38;5;66;03m#  BlockManager objects not yet attached to a DataFrame.\u001b[39;00m\n\u001b[1;32m   1806\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_consolidated():\n\u001b[0;32m-> 1807\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mblocks \u001b[38;5;241m=\u001b[39m \u001b[43m_consolidate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mblocks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1808\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1809\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_known_consolidated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2288\u001b[0m, in \u001b[0;36m_consolidate\u001b[0;34m(blocks)\u001b[0m\n\u001b[1;32m   2286\u001b[0m new_blocks: \u001b[38;5;28mlist\u001b[39m[Block] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m   2287\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (_can_consolidate, dtype), group_blocks \u001b[38;5;129;01min\u001b[39;00m grouper:\n\u001b[0;32m-> 2288\u001b[0m     merged_blocks, _ \u001b[38;5;241m=\u001b[39m \u001b[43m_merge_blocks\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2289\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mgroup_blocks\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcan_consolidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_can_consolidate\u001b[49m\n\u001b[1;32m   2290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2291\u001b[0m     new_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(merged_blocks, new_blocks)\n\u001b[1;32m   2292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(new_blocks)\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:2313\u001b[0m, in \u001b[0;36m_merge_blocks\u001b[0;34m(blocks, dtype, can_consolidate)\u001b[0m\n\u001b[1;32m   2306\u001b[0m new_values: ArrayLike\n\u001b[1;32m   2308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(blocks[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdtype, np\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[1;32m   2309\u001b[0m     \u001b[38;5;66;03m# error: List comprehension has incompatible type List[Union[ndarray,\u001b[39;00m\n\u001b[1;32m   2310\u001b[0m     \u001b[38;5;66;03m# ExtensionArray]]; expected List[Union[complex, generic,\u001b[39;00m\n\u001b[1;32m   2311\u001b[0m     \u001b[38;5;66;03m# Sequence[Union[int, float, complex, str, bytes, generic]],\u001b[39;00m\n\u001b[1;32m   2312\u001b[0m     \u001b[38;5;66;03m# Sequence[Sequence[Any]], SupportsArray]]\u001b[39;00m\n\u001b[0;32m-> 2313\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mblocks\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   2314\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2315\u001b[0m     bvals \u001b[38;5;241m=\u001b[39m [blk\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m blk \u001b[38;5;129;01min\u001b[39;00m blocks]\n",
      "File \u001b[0;32m/dtu/blackhole/0e/213550/dark-vessel-hunter/.venv/lib/python3.10/site-packages/numpy/_core/shape_base.py:292\u001b[0m, in \u001b[0;36mvstack\u001b[0;34m(tup, dtype, casting)\u001b[0m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m    291\u001b[0m     arrs \u001b[38;5;241m=\u001b[39m (arrs,)\n\u001b[0;32m--> 292\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate 137. MiB for an array with shape (10, 1795771) and data type object"
     ]
    }
   ],
   "source": [
    "# --- If you want to download all csv files before, uncomment the line below ---\n",
    "# ais_downloader.download_multiple_ais_data(START_DATE, END_DATE, folder_path)\n",
    "\n",
    "# --- Build the schedule of download string dates ---\n",
    "dates = ais_downloader.get_work_dates(START_DATE, END_DATE, csv_folder_path, filter=False)\n",
    "\n",
    "# --- Iterate with tqdm and download, unzip and delete ---\n",
    "for day in tqdm(dates, desc=f\"Processing data\", unit=\"file\" ):\n",
    "    tag = f\"{day:%Y-%m}\" if day < date.fromisoformat(\"2024-03-01\") else f\"{day:%Y-%m-%d}\"\n",
    "    print(f\"\\nProcessing date: {tag}\")\n",
    "\n",
    "    # --- Download one day ---\n",
    "    csv_path = ais_downloader.download_one_ais_data(day, csv_folder_path)\n",
    "    \n",
    "    # --- Load CSV into DataFrame ---\n",
    "    df_raw = ais_reader.read_single_ais_df(csv_path, BBOX, verbose=VERBOSE_MODE)\n",
    "    # --- Optionally delete the downloaded CSV file ---\n",
    "    if DELETE_DOWNLOADED_CSV: csv_path.unlink(missing_ok=True)\n",
    "    \n",
    "    # --- Filter and split ---\n",
    "    # Filter AIS data, keeping Class A and Class B by default,\n",
    "    df_filtered = ais_filtering.filter_ais_df(\n",
    "        df_raw,                                               # raw AIS DataFrame\n",
    "        polygon_coords=POLYGON_COORDINATES,                   # polygon coordinates for precise AOI filtering\n",
    "        allowed_mobile_types=VESSEL_AIS_CLASS,                # vessel AIS class filter\n",
    "        apply_polygon_filter=True,                            # keep polygon filtering enabled boolean\n",
    "        remove_zero_sog_vessels=REMOVE_ZERO_SOG_VESSELS,      # use True/False to enable/disable 90% zero-SOG removal\n",
    "        output_sog_in_ms=SOG_IN_MS,                           # convert SOG from knots in m/s (default) boolean\n",
    "        sog_min_knots=SOG_MIN_KNOTS,                          # min SOG in knots to keep (None to disable)\n",
    "        sog_max_knots=SOG_MAX_KNOTS,                          # max SOG in knots to keep (None to disable) \n",
    "        port_locodes_path=file_port_locations,                # path to port locodes CSV\n",
    "        exclude_ports=True,                                   # exclude port areas boolean \n",
    "        verbose=VERBOSE_MODE,                                 # verbose mode boolean\n",
    "    )\n",
    "        \n",
    "    # --- Parquet conversion ---\n",
    "    # Save to Parquet by MMSI\n",
    "    ais_to_parquet.save_by_mmsi(\n",
    "        df_filtered,                                             # filtered AIS DataFrame \n",
    "        verbose=VERBOSE_MODE,                                    # verbose mode boolean\n",
    "        output_folder=parquet_folder_path                        # output folder path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb7e37",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50077b83",
   "metadata": {},
   "source": [
    "#### File imports for the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.pre_proc.pre_processing_utils as pre_processing_utils\n",
    "import src.pre_proc.ais_query as ais_query\n",
    "import src.pre_proc.ais_segment as ais_segment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417146",
   "metadata": {},
   "source": [
    "#### Library imports for the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "450bff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465104",
   "metadata": {},
   "source": [
    "#### Set pre-processing preferences and input configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "149c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_MODE = config.VERBOSE_MODE\n",
    "\n",
    "FOLDER_NAME = config.AIS_DATA_FOLDER\n",
    "folder_path = Path(FOLDER_NAME)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "\n",
    "MAX_TIME_GAP_SEC = config.MAX_TIME_GAP_SEC\n",
    "MAX_TRACK_DURATION_SEC = config.MAX_TRACK_DURATION_SEC\n",
    "MIN_TRACK_DURATION_SEC = config.MIN_TRACK_DURATION_SEC\n",
    "MIN_SEGMENT_LENGTH = config.MIN_SEGMENT_LENGTH\n",
    "\n",
    "\n",
    "NUMERIC_COLS = config.NUMERIC_COLS\n",
    "# if u want to do it withouth a end date comment next line\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE\n",
    "TEST_END_DATE = config.TEST_END_DATE\n",
    "\n",
    "RESAMPLING_RULE = config.RESAMPLING_RULE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e201661",
   "metadata": {},
   "source": [
    "#### Pre processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pre_processing(dataframe_type: str = \"all\"):\n",
    "\n",
    "    if dataframe_type == \"all\":\n",
    "        main_pre_processing(\"train\")\n",
    "        main_pre_processing(\"test\")\n",
    "        return\n",
    "        \n",
    "    elif dataframe_type == \"train\":\n",
    "        print(f\"[pre_processing] Querying AIS data for training period: {TRAIN_START_DATE} to {TRAIN_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        dates = (\n",
    "            pd.date_range(TRAIN_START_DATE, TRAIN_END_DATE, freq=\"D\")\n",
    "            .strftime(\"%Y-%m-%d\")\n",
    "            .tolist()\n",
    "        )\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, dates=dates, verbose=VERBOSE_MODE)\n",
    "        \n",
    "    elif dataframe_type == \"test\":\n",
    "        print(f\"[pre_processing] Querying AIS data for testing period: {TEST_START_DATE} to {TEST_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        dates = (\n",
    "            pd.date_range(TEST_START_DATE, TEST_END_DATE, freq=\"D\")\n",
    "            .strftime(\"%Y-%m-%d\")\n",
    "            .tolist()\n",
    "        )\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, dates=dates, verbose=VERBOSE_MODE)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataframe_type: {dataframe_type}. Must be 'train' or 'test'.\")\n",
    "    \n",
    "    # Converting COG to sine and cosine components\n",
    "    df = pre_processing_utils.cog_to_sin_cos(df)\n",
    "    \n",
    "    # Dropping unnecessary columns and rows with missing values\n",
    "    df.drop(columns=[ \n",
    "        'Type of mobile', \n",
    "        'ROT',\n",
    "        'COG',\n",
    "        'Heading', \n",
    "        'IMO', \n",
    "        'Callsign', \n",
    "        'Name', \n",
    "        'Navigational status',\n",
    "        'Cargo type', \n",
    "        'Width', \n",
    "        'Length',\n",
    "        'Type of position fixing device', \n",
    "        'Draught', \n",
    "        'Destination', \n",
    "        'ETA',\n",
    "        'Data source type', \n",
    "        'A', 'B', 'C', 'D', \n",
    "        'Date'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Removing rows with NaN values in essential columns\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Grouping Ship types\n",
    "    commercial_types = [\"Cargo\", \"Tanker\"]\n",
    "    passenger_types = [\"Passenger\", \"Pleasure\", \"Sailing\"]\n",
    "    service_types = [\"Dredging\", \"Law enforcement\", \"Military\", \"Port tender\", \"SAR\", \"Towing\", \"Towing long/wide\",\"Tug\"]\n",
    "    valid_types =  [\"Fishing\", \"Service\", \"Commercial\", \"Passenger\"]\n",
    "\n",
    "    df.loc[df[\"Ship type\"].isin(commercial_types), \"Ship type\"] = \"Commercial\"\n",
    "    df.loc[df[\"Ship type\"].isin(passenger_types), \"Ship type\"] = \"Passenger\"\n",
    "    df.loc[df[\"Ship type\"].isin(service_types), \"Ship type\"] = \"Service\"\n",
    "    df.loc[~df[\"Ship type\"].isin(valid_types), \"Ship type\"] = \"Other\"\n",
    "\n",
    "    print(\"[pre_processing] Ship type counts:\")\n",
    "    print(df[\"Ship type\"].value_counts())\n",
    "    \n",
    "    if VERBOSE_MODE:\n",
    "        print(f\"[pre_processing] DataFrame after dropping unnecessary columns and NaNs: {len(df):,} rows\")\n",
    "\n",
    "    # Segmenting AIS tracks based on time gaps and max duration, filtering short segments\n",
    "    df = ais_segment.segment_ais_tracks(\n",
    "        df,\n",
    "        max_time_gap_sec=MAX_TIME_GAP_SEC,\n",
    "        max_track_duration_sec=MAX_TRACK_DURATION_SEC,\n",
    "        min_track_duration_sec=MIN_TRACK_DURATION_SEC,\n",
    "        min_track_len=MIN_SEGMENT_LENGTH,\n",
    "        verbose=VERBOSE_MODE\n",
    "    )\n",
    "\n",
    "    # Adding segment nr feature\n",
    "    df = pre_processing_utils.add_segment_nr(df)\n",
    "\n",
    "    # Resampling all tracks to fixed time intervals\n",
    "    df = pre_processing_utils.resample_all_tracks(df, rule=RESAMPLING_RULE)\n",
    "\n",
    "    # Normalizing numeric columns\n",
    "    df, mean, std = pre_processing_utils.normalize_df(df, NUMERIC_COLS)\n",
    "\n",
    "    # Ship type labeling (mapping to be used later)\n",
    "    df, ship_type_to_id = pre_processing_utils.label_ship_types(df)\n",
    "    \n",
    "    # Saving pre-processed DataFrame\n",
    "    if dataframe_type == \"train\":\n",
    "        print(f\"[pre_processing] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TRAIN_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "        metadata_path = config.PRE_PROCESSING_METADATA_TRAIN_PATH\n",
    "    else:\n",
    "        print(f\"[pre_processing] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TEST_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "        metadata_path = config.PRE_PROCESSING_METADATA_TEST_PATH\n",
    "\n",
    "    if VERBOSE_MODE: print(f\"[pre_processing] Columns of pre-processed DataFrame:\\n{df.columns.tolist()}\")\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "\n",
    "    # Saving preprocessing metadata\n",
    "    if VERBOSE_MODE: print(f\"[pre_processing] Saving preprocessing metadata to {metadata_path}...\")\n",
    "    meta = {\n",
    "        \"mean\": mean.tolist(),\n",
    "        \"std\": std.tolist(),\n",
    "        \"ship_type_to_id\": ship_type_to_id\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf852",
   "metadata": {},
   "source": [
    "#### Pre processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "597faf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for training period: 2025-08-01 to 2025-08-28\n",
      "[ais_query] Querying parquet files from: ais-data/parquet  from date None  to date  None\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbb57bf119c04c3c951d77769d1347a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ais_query] 4,845,439 rows, 3,060 vessels; Date IN [2025-08-01, 2025-08-02, 2025-08-03, 2025-08-04, 2025-08-05, 2025-08-06, 2025-08-07, 2025-08-08, 2025-08-09, 2025-08-10, 2025-08-11, 2025-08-12, 2025-08-13, 2025-08-14, 2025-08-15, 2025-08-16, 2025-08-17, 2025-08-18, 2025-08-19, 2025-08-20, 2025-08-21, 2025-08-22, 2025-08-23, 2025-08-24, 2025-08-25, 2025-08-26, 2025-08-27, 2025-08-28]\n",
      "[pre_processing] DataFrame after dropping unnecessary columns and NaNs: 4,845,439 rows\n",
      "[segment_ais_tracks] Starting with 4,845,439 rows, 3,060 unique vessels\n",
      "[segment_ais_tracks] After segment-level filter: 4,838,589 rows, 7,331 segments\n",
      "[pre_processing] Ship type counts:\n",
      "Ship type\n",
      "Commercial    1909334\n",
      "Fishing       1679937\n",
      "Passenger      960974\n",
      "Service        157074\n",
      "Other          131270\n",
      "Name: count, dtype: int64\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_train.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Segment_nr', 'Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG_sin', 'COG_cos', 'TrackID', 'MMSI', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/df_preprocessed/pre_processing_metadata_train.json...\n"
     ]
    }
   ],
   "source": [
    "main_pre_processing(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "61d81e09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[pre_processing] Querying AIS data for testing period: 2025-08-29 to 2025-08-30\n",
      "[ais_query] Querying parquet files from: ais-data/parquet  from date None  to date  None\n",
      "[ais_query] 249,888 rows, 358 vessels; Date IN [2025-08-29, 2025-08-30]\n",
      "[pre_processing] DataFrame after dropping unnecessary columns and NaNs: 249,888 rows\n",
      "[segment_ais_tracks] Starting with 249,888 rows, 358 unique vessels\n",
      "[segment_ais_tracks] After segment-level filter: 249,208 rows, 427 segments\n",
      "[pre_processing] Ship type counts:\n",
      "Ship type\n",
      "Commercial    140526\n",
      "Passenger      45466\n",
      "Fishing        44096\n",
      "Service        13495\n",
      "Other           5625\n",
      "Name: count, dtype: int64\n",
      "[pre_processing] Saving pre-processed DataFrame to ais-data/df_preprocessed/pre_processed_df_test.parquet...\n",
      "[pre_processing] Columns of pre-processed DataFrame:\n",
      "['Segment_nr', 'Timestamp', 'Latitude', 'Longitude', 'SOG', 'COG_sin', 'COG_cos', 'TrackID', 'MMSI', 'ShipTypeID']\n",
      "[pre_processing] Saving preprocessing metadata to ais-data/df_preprocessed/pre_processing_metadata_test.json...\n"
     ]
    }
   ],
   "source": [
    "main_pre_processing(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724caf9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f656cc",
   "metadata": {},
   "source": [
    "#### File imports for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de52b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d73047b",
   "metadata": {},
   "source": [
    "#### Library imports for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "927b9e0b",
   "metadata": {},
   "source": [
    "#### Set training preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9953ac75",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca7304f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb787",
   "metadata": {},
   "source": [
    "#### File imports for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd90563",
   "metadata": {},
   "source": [
    "#### Library imports for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c86bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df15213e",
   "metadata": {},
   "source": [
    "#### Set evaluation preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9e93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0d3719",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb45a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb69a421",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6993541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
