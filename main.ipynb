{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e568f0d6",
   "metadata": {},
   "source": [
    "# vessel-cable-anomaly-hunter\n",
    "DTU Deep Learning project 29, group 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814a0",
   "metadata": {},
   "source": [
    "## Required Libraries Installation\n",
    "Run this in your terminal before executing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f609e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd44da",
   "metadata": {},
   "source": [
    "## 1) Data Download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347b624",
   "metadata": {},
   "source": [
    "#### File imports for the data download and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c203ad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/dl/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import config\n",
    "import src.data.ais_downloader as ais_downloader\n",
    "import src.data.ais_filtering as ais_filtering\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.data.ais_to_parquet as ais_to_parquet\n",
    "\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.pre_proc.ais_query as ais_query \n",
    "import src.utils.ais_maps as ais_maps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a859",
   "metadata": {},
   "source": [
    "#### Library imports for the data download and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc178db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from datetime import date, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675b5f",
   "metadata": {},
   "source": [
    "#### Set data preferences and configuration inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6c341eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE_MODE = config.VERBOSE_MODE                          # Whether to print verbose output\n",
    "\n",
    "START_DATE = config.START_DATE                              # Start date for data downloading\n",
    "END_DATE   = config.END_DATE                                # End date for data downloading\n",
    "\n",
    "AIS_DATA_NAME = config.AIS_DATA_FOLDER                      # Name of the folder to store AIS data\n",
    "DELETE_DOWNLOADED_CSV = config.DELETE_DOWNLOADED_CSV        # Whether to delete raw downloaded CSV files after processing\n",
    "\n",
    "VESSEL_AIS_CLASS = config.VESSEL_AIS_CLASS                  # AIS classes of vessels to include\n",
    "\n",
    "REMOVE_ZERO_SOG_VESSELS = config.REMOVE_ZERO_SOG_VESSELS    # Whether to remove vessels with zero Speed Over Ground\n",
    "SOG_IN_MS = config.SOG_IN_MS                                # If True, SOG is in meters/second; if False, SOG is in knots\n",
    "SOG_MIN_KNOTS = config.SOG_MIN_KNOTS                        # Minimum SOG in knots\n",
    "SOG_MAX_KNOTS = config.SOG_MAX_KNOTS                        # Maximum SOG in knots\n",
    "\n",
    "BBOX = config.BBOX                                          # Bounding Box to prefilter AIS data\n",
    "POLYGON_COORDINATES = config.POLYGON_COORDINATES            # Polygon coordinates for filter Area of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8db5",
   "metadata": {},
   "source": [
    "#### Create Directories and Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f2a99188",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Create paths ---\n",
    "folder_path = Path(AIS_DATA_NAME)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "csv_folder_path = folder_path / \"csv\"\n",
    "csv_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "parquet_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_port_locations = folder_path / \"port_locodes.csv\" # Path to port locations file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98948aa7",
   "metadata": {},
   "source": [
    "#### Main Script\n",
    "1) Download one single .csv AIS data file from http://aisdata.ais.dk (link to data column description http://aisdata.ais.dk/!_README_information_CSV_files.txt);\n",
    "2) For a given AOI in Denmark with known cable positions, filter AIS messages by cleansing unrealistic/unphysical messages or duplicates and removes error-prone messages within port areas;\n",
    "3) Segmentation of vessel tracks per MMSI and date;\n",
    "4) Parquet Conversion;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24af940c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:   0%|          | 0/3 [00:00<?, ?file/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing date: 2025-08-01\n",
      "Skipping 2025-08-01 download: already present in ais-data/csv folder\n",
      " Read AIS data: 1,128,873 rows within bbox,  511 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,128,873 rows, 511 vessels\n",
      " [filter_ais_df] Type filtering: 1,093,101 rows (removed 35,772) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,093,079 rows, 508 vessels\n",
      " [filter_ais_df] Duplicate removal: 638,467 rows, 508 vessels\n",
      " [filter_ais_df] Polygon filtering: 337,847 rows (removed 300,620), 378 vessels\n",
      " [filter_ais_df] Port-area removal: removed 160,778 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 175,598 rows (removed 1,471) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 173,215 rows (removed 2,381) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 173,215 rows, 327 unique vessels (SOG in m/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  33%|███▎      | 1/3 [00:04<00:08,  4.21s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /Users/federicomarra/Documents/GitHub/dl-dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-02\n",
      "Skipping 2025-08-02 download: already present in ais-data/csv folder\n",
      " Read AIS data: 1,161,214 rows within bbox,  452 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,161,214 rows, 452 vessels\n",
      " [filter_ais_df] Type filtering: 1,120,127 rows (removed 41,087) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,114,486 rows, 447 vessels\n",
      " [filter_ais_df] Duplicate removal: 616,865 rows, 447 vessels\n",
      " [filter_ais_df] Polygon filtering: 314,494 rows (removed 302,371), 342 vessels\n",
      " [filter_ais_df] Port-area removal: removed 168,854 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 145,303 rows (removed 337) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 144,010 rows (removed 1,291) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 144,010 rows, 290 unique vessels (SOG in m/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data:  67%|██████▋   | 2/3 [00:08<00:04,  4.47s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /Users/federicomarra/Documents/GitHub/dl-dark-vessel-hunter/ais-data/parquet\n",
      "\n",
      "Processing date: 2025-08-03\n",
      "Skipping 2025-08-03 download: already present in ais-data/csv folder\n",
      " Read AIS data: 1,079,891 rows within bbox,  399 unique vessels\n",
      " [filter_ais_df] Before filtering: 1,079,891 rows, 399 vessels\n",
      " [filter_ais_df] Type filtering: 1,043,284 rows (removed 36,607) using ['Class A', 'Class B']\n",
      " [filter_ais_df] MMSI filtering: 1,043,284 rows, 398 vessels\n",
      " [filter_ais_df] Duplicate removal: 599,515 rows, 398 vessels\n",
      " [filter_ais_df] Polygon filtering: 319,478 rows (removed 280,037), 288 vessels\n",
      " [filter_ais_df] Port-area removal: removed 165,183 rows in 3 overlapping ports\n",
      " [filter_ais_df] COG sanity: 153,909 rows (removed 386) with range [0, 360] deg\n",
      " [filter_ais_df] SOG sanity: 152,641 rows (removed 1,266) with range [0.5, 35.0] knots\n",
      " [filter_ais_df] Final: 152,641 rows, 248 unique vessels (SOG in m/s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|██████████| 3/3 [00:13<00:00,  4.45s/file]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [save_by_mmsi] Parquet dataset written/appended at: /Users/federicomarra/Documents/GitHub/dl-dark-vessel-hunter/ais-data/parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# --- If you want to download all csv files before, uncomment the line below ---\n",
    "# ais_downloader.download_multiple_ais_data(START_DATE, END_DATE, folder_path)\n",
    "\n",
    "# --- Build the schedule of download string dates ---\n",
    "dates = ais_downloader.get_work_dates(START_DATE, END_DATE, csv_folder_path, filter=False)\n",
    "\n",
    "# --- Iterate with tqdm and download, unzip and delete ---\n",
    "for day in tqdm(dates, desc=f\"Processing data\", unit=\"file\" ):\n",
    "    tag = f\"{day:%Y-%m}\" if day < date.fromisoformat(\"2024-03-01\") else f\"{day:%Y-%m-%d}\"\n",
    "    print(f\"\\nProcessing date: {tag}\")\n",
    "\n",
    "    # --- Download one day ---\n",
    "    csv_path = ais_downloader.download_one_ais_data(day, csv_folder_path)\n",
    "    \n",
    "    # --- Load CSV into DataFrame ---\n",
    "    df_raw = ais_reader.read_single_ais_df(csv_path, BBOX, verbose=VERBOSE_MODE)\n",
    "    # --- Optionally delete the downloaded CSV file ---\n",
    "    if DELETE_DOWNLOADED_CSV: csv_path.unlink(missing_ok=True)\n",
    "    \n",
    "    # --- Filter and split ---\n",
    "    # Filter AIS data, keeping Class A and Class B by default,\n",
    "    df_filtered = ais_filtering.filter_ais_df(\n",
    "        df_raw,                                               # raw AIS DataFrame\n",
    "        polygon_coords=POLYGON_COORDINATES,                   # polygon coordinates for precise AOI filtering\n",
    "        allowed_mobile_types=VESSEL_AIS_CLASS,                # vessel AIS class filter\n",
    "        apply_polygon_filter=True,                            # keep polygon filtering enabled boolean\n",
    "        remove_zero_sog_vessels=REMOVE_ZERO_SOG_VESSELS,      # use True/False to enable/disable 90% zero-SOG removal\n",
    "        output_sog_in_ms=SOG_IN_MS,                           # convert SOG from knots in m/s (default) boolean\n",
    "        sog_min_knots=SOG_MIN_KNOTS,                          # min SOG in knots to keep (None to disable)\n",
    "        sog_max_knots=SOG_MAX_KNOTS,                          # max SOG in knots to keep (None to disable) \n",
    "        port_locodes_path=file_port_locations,                # path to port locodes CSV\n",
    "        exclude_ports=True,                                   # exclude port areas boolean \n",
    "        verbose=VERBOSE_MODE,                                 # verbose mode boolean\n",
    "    )\n",
    "        \n",
    "    # --- Parquet conversion ---\n",
    "    # Save to Parquet by MMSI\n",
    "    ais_to_parquet.save_by_mmsi(\n",
    "        df_filtered,                                             # filtered AIS DataFrame \n",
    "        verbose=VERBOSE_MODE,                                    # verbose mode boolean\n",
    "        output_folder=parquet_folder_path                        # output folder path\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb7e37",
   "metadata": {},
   "source": [
    "## 2) Pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50077b83",
   "metadata": {},
   "source": [
    "#### File imports for the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.pre_proc.pre_processing_utils as pre_processing_utils\n",
    "import src.pre_proc.ais_query as ais_query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417146",
   "metadata": {},
   "source": [
    "#### Library imports for the pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465104",
   "metadata": {},
   "source": [
    "#### Set pre-processing preferences and input configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "AIS_DATA_NAME = config.AIS_DATA_FOLDER\n",
    "folder_path = Path(AIS_DATA_NAME)\n",
    "parquet_folder_path = folder_path / \"parquet\"\n",
    "\n",
    "SEGMENT_MAX_LENGTH = 300  # datapoints\n",
    "\n",
    "NUMERIC_COLS = config.NUMERIC_COLS\n",
    "# if u want to do it withouth a end date comment next line\n",
    "TRAIN_START_DATE = \"2025-10-20\"\n",
    "TRAIN_END_DATE = \"2025-11-08\"\n",
    "\n",
    "TEST_START_DATE = \"2025-11-09\"\n",
    "TEST_END_DATE = \"2025-11-10\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e201661",
   "metadata": {},
   "source": [
    "#### Pre processing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pre_processing(dataframe_type: str = \"all\"):\n",
    "\n",
    "    if dataframe_type == \"all\":\n",
    "        main_pre_processing(\"train\")\n",
    "        main_pre_processing(\"test\")\n",
    "        return\n",
    "        \n",
    "    elif dataframe_type == \"train\":\n",
    "        print(f\"[pre_processing] Querying AIS data for training period: {TRAIN_START_DATE} to {TRAIN_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        dates = (\n",
    "            pd.date_range(TRAIN_START_DATE, TRAIN_END_DATE, freq=\"D\")\n",
    "            .strftime(\"%Y-%m-%d\")\n",
    "            .tolist()\n",
    "        )\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, dates=dates, verbose=VERBOSE_MODE)\n",
    "        \n",
    "    elif dataframe_type == \"test\":\n",
    "        print(f\"[pre_processing] Querying AIS data for testing period: {TEST_START_DATE} to {TEST_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        dates = (\n",
    "            pd.date_range(TEST_START_DATE, TEST_END_DATE, freq=\"D\")\n",
    "            .strftime(\"%Y-%m-%d\")\n",
    "            .tolist()\n",
    "        )\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, dates=dates, verbose=VERBOSE_MODE)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataframe_type: {dataframe_type}. Must be 'train' or 'test'.\")\n",
    "    \n",
    "\n",
    "    # Dropping unnecessary columns and rows with missing values\n",
    "    print(f\"[pre_processing] Initial data size: {len(df)} records.\")\n",
    "    print(f\"[pre_processing] Dropping unnecessary columns and rows with missing values...\")\n",
    "    df.drop(columns=[ \n",
    "        'Type of mobile', \n",
    "        'ROT', \n",
    "        'Heading', \n",
    "        'IMO', \n",
    "        'Callsign', \n",
    "        'Name', \n",
    "        'Navigational status',\n",
    "        'Cargo type', \n",
    "        'Width', \n",
    "        'Length',\n",
    "        'Type of position fixing device', \n",
    "        'Draught', \n",
    "        'Destination', \n",
    "        'ETA',\n",
    "        'Data source type', \n",
    "        'A', 'B', 'C', 'D', \n",
    "        'Date'], inplace=True, errors='ignore')\n",
    "\n",
    "    df.dropna(inplace=True)\n",
    "    print(f\"[pre_processing] Data size after dropping: {len(df)} records.\")\n",
    "\n",
    "    # Grouping Ship types\n",
    "    commercial_types = [\"Cargo\", \"Tanker\"]\n",
    "    passenger_types = [\"Passenger\", \"Pleasure\", \"Sailing\"]\n",
    "    service_types = [\"Dredging\", \"Law enforcement\", \"Military\", \"Port tender\", \"SAR\", \"Towing\", \"Towing long/wide\",\"Tug\"]\n",
    "    valid_types =  [\"Fishing\", \"Service\", \"Commercial\", \"Passenger\"]\n",
    "\n",
    "    df.loc[df[\"Ship type\"].isin(commercial_types), \"Ship type\"] = \"Commercial\"\n",
    "    df.loc[df[\"Ship type\"].isin(passenger_types), \"Ship type\"] = \"Passenger\"\n",
    "    df.loc[df[\"Ship type\"].isin(service_types), \"Ship type\"] = \"Service\"\n",
    "    df.loc[~df[\"Ship type\"].isin(valid_types), \"Ship type\"] = \"Other\"\n",
    "\n",
    "    print(\"[pre_processing] Ship type counts:\")\n",
    "    print(df[\"Ship type\"].value_counts())\n",
    "\n",
    "    # Adding △T feature\n",
    "    df = pre_processing_utils.add_delta_t(df)\n",
    "    df.drop(columns=[\"DeltaT\"], inplace=True)\n",
    "\n",
    "    # Splitting segments\n",
    "    print(f\"[pre_processing] Splitting segments to max length {SEGMENT_MAX_LENGTH}...\")\n",
    "    df = pre_processing_utils.split_segments_fixed_length(df, max_len=SEGMENT_MAX_LENGTH)\n",
    "\n",
    "    # Normalizing numeric columns\n",
    "    df, mean, std = pre_processing_utils.normalize_df(df, NUMERIC_COLS)\n",
    "\n",
    "    # Encoding Navicational Status as one-hot\n",
    "    #df, nav_status_to_id = pre_processing_utils.one_hot_encode_nav_status(df)\n",
    "\n",
    "    # Ship type labeling (mapping to be used later)\n",
    "    df, ship_type_to_id = pre_processing_utils.label_ship_types(df)\n",
    "    \n",
    "    # Saving pre-processed DataFrame\n",
    "    if dataframe_type == \"train\":\n",
    "        print(f\"[pre_processing] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TRAIN_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "        metadata_path = config.PRE_PROCESSING_METADATA_TRAIN_PATH\n",
    "    else:\n",
    "        print(f\"[pre_processing] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TEST_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "        metadata_path = config.PRE_PROCESSING_METADATA_TEST_PATH\n",
    "\n",
    "    print(f\"[pre_processing] Columns of pre-processed DataFrame:\\n{df.columns.tolist()}\")\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(output_path, index=False)\n",
    "\n",
    "    # Saving preprocessing metadata\n",
    "    print(f\"[pre_processing] Saving preprocessing metadata to {metadata_path}...\")\n",
    "    meta = {\n",
    "        \"mean\": mean.tolist(),\n",
    "        \"std\": std.tolist(),\n",
    "        #\"nav_status_to_id\": nav_status_to_id,\n",
    "        \"ship_type_to_id\": ship_type_to_id\n",
    "    }\n",
    "\n",
    "    with open(metadata_path, \"w\") as f:\n",
    "        json.dump(meta, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf852",
   "metadata": {},
   "source": [
    "#### Pre processing script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597faf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_pre_processing(\"train\")\n",
    "main_pre_processing(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724caf9",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f656cc",
   "metadata": {},
   "source": [
    "#### File imports for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de52b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3d73047b",
   "metadata": {},
   "source": [
    "#### Library imports for the training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "927b9e0b",
   "metadata": {},
   "source": [
    "#### Set training preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19d1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9953ac75",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388c647",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dca7304f",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb787",
   "metadata": {},
   "source": [
    "#### File imports for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75767",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "acd90563",
   "metadata": {},
   "source": [
    "#### Library imports for the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c86bc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "df15213e",
   "metadata": {},
   "source": [
    "#### Set evaluation preferences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9e93b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4b0d3719",
   "metadata": {},
   "source": [
    "#### Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb45a6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "eb69a421",
   "metadata": {},
   "source": [
    "## Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6993541",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
