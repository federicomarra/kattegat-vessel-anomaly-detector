{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e568f0d6",
   "metadata": {},
   "source": [
    "# vessel-cable-anomaly-hunter\n",
    "DTU Deep Learning project 29, group 80"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425814a0",
   "metadata": {},
   "source": [
    "## Required Libraries Installation\n",
    "Run this in your terminal before executing this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6f609e",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install -q -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccd44da",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "AIS Data scraper and filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c347b624",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c203ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.data.ais_downloader as ais_downloader\n",
    "import src.data.ais_filtering as ais_filtering\n",
    "import src.data.ais_reader as ais_reader\n",
    "import src.data.ais_to_parquet as ais_to_parquet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3813a859",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc178db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from tqdm import tqdm\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65675b5f",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c341eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from config.py\n",
    "VERBOSE_MODE = config.VERBOSE_MODE                          # Whether to print verbose output\n",
    "\n",
    "START_DATE = config.START_DATE                              # Start date for data downloading\n",
    "END_DATE   = config.END_DATE                                # End date for data downloading\n",
    "\n",
    "DELETE_DOWNLOADED_CSV = config.DELETE_DOWNLOADED_CSV        # Whether to delete raw downloaded CSV files after parquet conversion\n",
    "\n",
    "BBOX = config.BBOX                                          # Bounding Box to prefilter AIS data\n",
    "POLYGON_COORDINATES = config.POLYGON_COORDINATES  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9d8db5",
   "metadata": {},
   "source": [
    "#### Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a99188",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = Path(config.AIS_DATA_FOLDER)\n",
    "folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "csv_folder_path = folder_path / config.AIS_DATA_FOLDER_CSV_SUBFOLDER\n",
    "csv_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "parquet_folder_path = folder_path / config.AIS_DATA_FOLDER_PARQUET_SUBFOLDER\n",
    "parquet_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_port_locations = folder_path / config.FILE_PORT_LOCATIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98948aa7",
   "metadata": {},
   "source": [
    "#### Download, Filter and Save into Parquet\n",
    "1. **Download:** Download one single .csv AIS data file from http://aisdata.ais.dk (link to data column description http://aisdata.ais.dk/!_README_information_CSV_files.txt);\n",
    "2. **Filter:** For a given AOI in Denmark with known cable positions, filter AIS messages by cleansing unrealistic/unphysical messages or duplicates and removes error-prone messages within port areas;\n",
    "3. **Segmentation:** Segment the cleaned data into tracks based on time gaps and track duration;\n",
    "4. **Parquet Conversion:** Save the cleaned and filtered data into Parquet files for faster loading in the next steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24af940c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Build the schedule of download string dates ---\n",
    "dates = ais_downloader.get_work_dates(START_DATE, END_DATE, csv_folder_path, filter=False)\n",
    "\n",
    "# --- Iterate with tqdm and download, unzip and delete ---\n",
    "for day in tqdm(dates, desc=f\"Processing data\", unit=\"file\" ):\n",
    "    tag = f\"{day:%Y-%m}\" if day < date.fromisoformat(\"2024-03-01\") else f\"{day:%Y-%m-%d}\"\n",
    "    print(f\"\\nProcessing date: {tag}\")\n",
    "\n",
    "    # --- Download one day ---\n",
    "    csv_path = ais_downloader.download_one_ais_data(day, csv_folder_path)\n",
    "    \n",
    "    # --- Load CSV into DataFrame ---\n",
    "    df_raw = ais_reader.read_single_ais_df(csv_path, BBOX, columns_to_drop=config.COLUMNS_TO_DROP, verbose=VERBOSE_MODE)\n",
    "    # --- Optionally delete the downloaded CSV file ---\n",
    "    if DELETE_DOWNLOADED_CSV: csv_path.unlink(missing_ok=True)\n",
    "    \n",
    "    # --- Filter and split ---\n",
    "    # Filter AIS data, keeping Class A and Class B by default,\n",
    "    df_filtered = ais_filtering.filter_ais_df(\n",
    "        df_raw,                                               # raw AIS DataFrame\n",
    "        polygon_coords=POLYGON_COORDINATES,                   # polygon coordinates for precise AOI filtering\n",
    "        allowed_mobile_types=config.VESSEL_AIS_CLASS,                # vessel AIS class filter\n",
    "        apply_polygon_filter=True,                            # keep polygon filtering enabled boolean\n",
    "        remove_zero_sog_vessels=config.REMOVE_ZERO_SOG_VESSELS,      # use True/False to enable/disable 90% zero-SOG removal\n",
    "        output_sog_in_ms=config.SOG_IN_MS,                           # convert SOG from knots in m/s (default) boolean\n",
    "        sog_min_knots=config.SOG_MIN_KNOTS,                          # min SOG in knots to keep (None to disable)\n",
    "        sog_max_knots=config.SOG_MAX_KNOTS,                          # max SOG in knots to keep (None to disable) \n",
    "        port_locodes_path=file_port_locations,                # path to port locodes CSV\n",
    "        exclude_ports=True,                                   # exclude port areas boolean \n",
    "        verbose=VERBOSE_MODE,                                 # verbose mode boolean\n",
    "    )\n",
    "    \n",
    "    # Free df_raw memory\n",
    "    del df_raw\n",
    "    gc.collect()\n",
    "\n",
    "    # --- Parquet conversion ---\n",
    "    # Save to Parquet by MMSI\n",
    "    ais_to_parquet.save_by_mmsi(\n",
    "        df_filtered,                                             # filtered AIS DataFrame \n",
    "        verbose=VERBOSE_MODE,                                    # verbose mode boolean\n",
    "        output_folder=parquet_folder_path                        # output folder path\n",
    "    )\n",
    "\n",
    "    # Free df_filtered memory\n",
    "    del df_filtered\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb7e37",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50077b83",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d240d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config\n",
    "import src.pre_proc.pre_processing_utils as pre_processing_utils\n",
    "import src.pre_proc.ais_query as ais_query\n",
    "import src.pre_proc.ais_segment as ais_segment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c417146",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450bff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23465104",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149c3adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read configuration from config.py\n",
    "VERBOSE_MODE = config.VERBOSE_MODE\n",
    "\n",
    "FOLDER_NAME = config.AIS_DATA_FOLDER\n",
    "folder_path = Path(FOLDER_NAME)\n",
    "parquet_folder_path = folder_path / config.AIS_DATA_FOLDER_PARQUET_SUBFOLDER\n",
    "\n",
    "TRAIN_START_DATE = config.TRAIN_START_DATE\n",
    "TRAIN_END_DATE = config.TRAIN_END_DATE\n",
    "\n",
    "TEST_START_DATE = config.TEST_START_DATE\n",
    "TEST_END_DATE = config.TEST_END_DATE\n",
    "\n",
    "MAX_TIME_GAP_SEC = config.MAX_TIME_GAP_SEC\n",
    "MAX_TRACK_DURATION_SEC = config.MAX_TRACK_DURATION_SEC\n",
    "MIN_TRACK_DURATION_SEC = config.MIN_TRACK_DURATION_SEC\n",
    "MIN_SEGMENT_LENGTH = config.MIN_SEGMENT_LENGTH\n",
    "\n",
    "MIN_FREQ_POINTS_PER_MIN = config.MIN_FREQ_POINTS_PER_MIN\n",
    "\n",
    "RESAMPLING_RULE = config.RESAMPLING_RULE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e201661",
   "metadata": {},
   "source": [
    "#### Preprocess function\n",
    "1. **Data Loading:** Queries DuckDB for AIS data within specified date ranges for 'train' or 'test'.\n",
    "2. **Feature Engineering:** Converts COG (Course Over Ground) to sine/cosine components.\n",
    "3. **Cleaning:** Drops unnecessary columns and rows with missing values.\n",
    "4. **Ship Type Grouping:** Aggregates specific ship types into broader categories (Commercial,\n",
    "Passenger, Service, Other).\n",
    "5. **Segmentation:** Splits AIS tracks into segments based on time gaps and duration constraints using\n",
    "ais_segment\n",
    "6. **Filtering:** Removes segments with low point density.\n",
    "7. **Resampling:** Resamples tracks to a fixed time interval.\n",
    "8. **Labeling:** Encodes ship types into numerical IDs.\n",
    "9. **Saving:** Exports the processed DataFrame to a Parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929aae55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_preprocess(dataframe_type: str = \"all\"):\n",
    "\n",
    "    if dataframe_type == \"all\":\n",
    "        main_preprocess(\"train\")\n",
    "        main_preprocess(\"test\")\n",
    "        return\n",
    "        \n",
    "    elif dataframe_type == \"train\":\n",
    "        print(f\"[preprocess] Querying AIS data for training period: {TRAIN_START_DATE} to {TRAIN_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, date_start=TRAIN_START_DATE, date_end=TRAIN_END_DATE, verbose=VERBOSE_MODE)\n",
    "        \n",
    "    elif dataframe_type == \"test\":\n",
    "        print(f\"[preprocess] Querying AIS data for testing period: {TEST_START_DATE} to {TEST_END_DATE}\")\n",
    "        # Loading filtered data from parquet files\n",
    "        df = ais_query.query_ais_duckdb(parquet_folder_path, date_start=TEST_START_DATE, date_end=TEST_END_DATE, verbose=VERBOSE_MODE)\n",
    "    else:\n",
    "        raise ValueError(f\"Invalid dataframe_type: {dataframe_type}. Must be 'train' or 'test'.\")\n",
    "     \n",
    "    # Converting COG to sine and cosine components\n",
    "    df = pre_processing_utils.cog_to_sin_cos(df)\n",
    "    \n",
    "    # Dropping unnecessary columns and rows with missing values\n",
    "    df.drop(columns=[ \n",
    "        'Type of mobile', \n",
    "        'COG', \n",
    "        'Date'], inplace=True, errors='ignore')\n",
    "    \n",
    "    # Removing rows with NaN values in essential columns\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Grouping Ship types\n",
    "    commercial_types = [\"Cargo\", \"Tanker\"]\n",
    "    passenger_types = [\"Passenger\", \"Pleasure\", \"Sailing\"]\n",
    "    service_types = [\"Dredging\", \"Law enforcement\", \"Military\", \"Port tender\", \"SAR\", \"Towing\", \"Towing long/wide\",\"Tug\"]\n",
    "    valid_types =  [\"Fishing\", \"Service\", \"Commercial\", \"Passenger\"]\n",
    "\n",
    "    df.loc[df[\"Ship type\"].isin(commercial_types), \"Ship type\"] = \"Commercial\"\n",
    "    df.loc[df[\"Ship type\"].isin(passenger_types), \"Ship type\"] = \"Passenger\"\n",
    "    df.loc[df[\"Ship type\"].isin(service_types), \"Ship type\"] = \"Service\"\n",
    "    df.loc[~df[\"Ship type\"].isin(valid_types), \"Ship type\"] = \"Other\"\n",
    "    \n",
    "    print(\"[preprocess] Ship type counts:\")\n",
    "    print(df[\"Ship type\"].value_counts())\n",
    "\n",
    "    if VERBOSE_MODE:\n",
    "        print(f\"[preprocess] DataFrame after dropping unnecessary columns and NaNs: {len(df):,} rows\")\n",
    "\n",
    "    # Segmenting AIS tracks based on time gaps and max duration, filtering short segments\n",
    "    df = ais_segment.segment_ais_tracks(\n",
    "        df,\n",
    "        max_time_gap_sec=MAX_TIME_GAP_SEC,\n",
    "        max_track_duration_sec=MAX_TRACK_DURATION_SEC,\n",
    "        min_track_duration_sec=MIN_TRACK_DURATION_SEC,\n",
    "        min_track_len=MIN_SEGMENT_LENGTH,\n",
    "        verbose=VERBOSE_MODE\n",
    "    )\n",
    "\n",
    "    # Adding segment nr feature\n",
    "    df = pre_processing_utils.add_segment_nr(df)\n",
    "\n",
    "    # Removing segments with low point density\n",
    "    df = pre_processing_utils.remove_notdense_segments(df, min_freq_points_per_min=MIN_FREQ_POINTS_PER_MIN)\n",
    "    \n",
    "    # Resampling all tracks to fixed time intervals\n",
    "    df = pre_processing_utils.resample_all_tracks(df, rule=RESAMPLING_RULE)\n",
    "\n",
    "    print(f\"[preprocess] Number of segments and rows after removing low-density segments and resampling: {df['Segment_nr'].nunique():,} segments, {len(df):,} rows\")\n",
    "\n",
    "    # Normalizing numeric columns\n",
    "    #df, mean, std = pre_processing_utils.normalize_df(df, NUMERIC_COLS)\n",
    "\n",
    "    # Ship type labeling (mapping to be used later)\n",
    "    df, ship_type_to_id = pre_processing_utils.label_ship_types(df)\n",
    "    \n",
    "    # Saving pre-processed DataFrame\n",
    "    if dataframe_type == \"train\":\n",
    "        print(f\"[preprocess] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TRAIN_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "        #metadata_path = config.PRE_PROCESSING_METADATA_TRAIN_PATH\n",
    "    else:\n",
    "        print(f\"[preprocess] Saving pre-processed DataFrame to {config.PRE_PROCESSING_DF_TEST_PATH}...\")\n",
    "        output_path = config.PRE_PROCESSING_DF_TEST_PATH\n",
    "        #metadata_path = config.PRE_PROCESSING_METADATA_TEST_PATH\n",
    "\n",
    "    if VERBOSE_MODE: print(f\"[preprocess] Columns of pre-processed DataFrame:\\n{df.columns.tolist()}\")\n",
    "    Path(output_path).parent.mkdir(parents=True, exist_ok=True)\n",
    "    df.to_parquet(output_path, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756cf852",
   "metadata": {},
   "source": [
    "#### Preprocess for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597faf76",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocess(\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d81e09",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_preprocess(\"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7724caf9",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f656cc",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de52b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as config_file\n",
    "from src.train.ais_dataset import AISDataset, ais_collate_fn\n",
    "from src.train.model import AIS_LSTM_Autoencoder\n",
    "from src.train.training_loop import run_experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d73047b",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc4c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import os\n",
    "import json\n",
    "import itertools # Added for grid search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927b9e0b",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74e19d1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to pre-processed training data\n",
    "PARQUET_FILE = config_file.PRE_PROCESSING_DF_TRAIN_PATH\n",
    "TRAIN_OUTPUT_DIR = config_file.TRAIN_OUTPUT_DIR\n",
    "\n",
    "# ensure output directory exists\n",
    "os.makedirs(TRAIN_OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "SPLIT_TRAIN_VAL_RATIO = config_file.SPLIT_TRAIN_VAL_RATIO\n",
    "EPOCHS = config_file.EPOCHS\n",
    "PATIENCE = config_file.PATIENCE\n",
    "FEATURES = config_file.FEATURE_COLS\n",
    "NUM_SHIP_TYPES = config_file.NUM_SHIP_TYPES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9953ac75",
   "metadata": {},
   "source": [
    "#### Hyperparameters grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7388c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# HYPERPARAMETER GRID SEARCH\n",
    "# ---------------------------------------------------------\n",
    "# Define ranges for grid search\n",
    "# Since you have high compute power, we explore Width (hidden) vs Depth (layers)\n",
    "# and Bottleneck tightness (latent).\n",
    "\n",
    "param_grid = {\n",
    "    'hidden_dim': [128, 256],       # Capacity of the LSTM\n",
    "    'latent_dim': [16, 64],         # Bottleneck: 16 (Anomaly Detection) vs 64 (Reconstruction)\n",
    "    'num_layers': [1, 2],           # Depth\n",
    "    'lr': [0.001, 0.0001],          # Learning Rate\n",
    "    'batch_size': [64, 128],        # Batch Size\n",
    "    'dropout': [0.0, 0.2]           # Regularization\n",
    "}\n",
    "\n",
    "configs = []\n",
    "\n",
    "# Use itertools.product to create all combinations\n",
    "keys, values = zip(*param_grid.items())\n",
    "for bundle in itertools.product(*values):\n",
    "    params = dict(zip(keys, bundle))\n",
    "    \n",
    "    # Optimization: Dropout is only useful if num_layers > 1\n",
    "    # Skip dropout=0.2 if num_layers=1 to avoid duplicate equivalent runs\n",
    "    if params['num_layers'] == 1 and params['dropout'] > 0:\n",
    "        continue\n",
    "        \n",
    "    # Create a descriptive run name\n",
    "    run_name = (f\"H{params['hidden_dim']}_L{params['latent_dim']}_\"\n",
    "                f\"Lay{params['num_layers']}_lr{params['lr']}_\"\n",
    "                f\"BS{params['batch_size']}_Drop{params['dropout']}\")\n",
    "    \n",
    "    config = {\n",
    "        \"run_name\": run_name,\n",
    "        \"epochs\": EPOCHS,              # Fixed epochs\n",
    "        \"patience\": PATIENCE,             # Fixed patience\n",
    "        \"features\": FEATURES,\n",
    "        \"num_ship_types\": NUM_SHIP_TYPES,\n",
    "        \"shiptype_emb_dim\": 8,     # Keep embedding dim constant for now\n",
    "        \n",
    "        # Dynamic Params\n",
    "        \"hidden_dim\": params['hidden_dim'],\n",
    "        \"latent_dim\": params['latent_dim'],\n",
    "        \"num_layers\": params['num_layers'],\n",
    "        \"lr\": params['lr'],\n",
    "        \"batch_size\": params['batch_size'],\n",
    "        \"dropout\": params['dropout']\n",
    "    }\n",
    "    configs.append(config)\n",
    "\n",
    "print(f\"Generated {len(configs)} unique configurations for training.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4923b10b",
   "metadata": {},
   "source": [
    "#### Device setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a4e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")  # for PC with NVIDIA\n",
    "    print(f\"Using device: {device} (NVIDIA GPU)\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")   # for Mac Apple Silicon\n",
    "    print(f\"Using device: {device} (Apple GPU)\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")   # Fallback on CPU\n",
    "    print(f\"Using device: {device} (CPU)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ce4a00",
   "metadata": {},
   "source": [
    "#### Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc17fc85",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(PARQUET_FILE):\n",
    "    print(f\"Error: {PARQUET_FILE} not found.\")\n",
    "    raise FileNotFoundError(f\"{PARQUET_FILE} not found. Stopping execution.\")\n",
    "\n",
    "# Initialize Dataset\n",
    "full_dataset = AISDataset(PARQUET_FILE)\n",
    "input_dim = full_dataset.input_dim\n",
    "\n",
    "# Split Train/Val (80/20)\n",
    "train_size = int(SPLIT_TRAIN_VAL_RATIO * len(full_dataset))\n",
    "val_size = len(full_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(full_dataset, [train_size, val_size])\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset)}, Val samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838d1ea6",
   "metadata": {},
   "source": [
    "#### Experiment Loop\n",
    "1. **Create DataLoaders:** Creates PyTorch DataLoaders for training and validation datasets.\n",
    "2. **Training Setup:** Initialize Model with FIXED num_ship_types, optimizer, and loss function.\n",
    "3. **Training Loop:** Trains the model over a set number of epochs, implementing early stopping based on validation loss.\n",
    "4. **Evaluation:** Assesses model performance on the validation set after each epoch.\n",
    "5. **Model Saving:** Saves the best-performing model based on validation loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b256aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "\n",
    "for config in configs:\n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(  # Training DataLoader\n",
    "        train_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        collate_fn=ais_collate_fn\n",
    "    )\n",
    "\n",
    "    val_loader = DataLoader(    # Validation DataLoader\n",
    "        val_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        collate_fn=ais_collate_fn\n",
    "    )\n",
    "\n",
    "    # Initialize Model with FIXED num_ship_types\n",
    "    model = AIS_LSTM_Autoencoder(\n",
    "        input_dim=input_dim,\n",
    "        hidden_dim=config['hidden_dim'],\n",
    "        latent_dim=config['latent_dim'],\n",
    "        num_layers=config['num_layers'],\n",
    "        num_ship_types=NUM_SHIP_TYPES, # Always use the fixed constant\n",
    "        shiptype_emb_dim=config['shiptype_emb_dim'],\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    # Run Pipeline\n",
    "    history, best_loss = run_experiment(\n",
    "        config,\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        device,\n",
    "        save_path=f\"{TRAIN_OUTPUT_DIR}/weights_{config['run_name']}.pth\"\n",
    "    )\n",
    "\n",
    "    # Save results\n",
    "    results.append({\n",
    "        \"config\": config['run_name'],\n",
    "        \"best_val_loss\": best_loss,\n",
    "        \"history\": history\n",
    "    })\n",
    "\n",
    "    # Save model and config\n",
    "    os.makedirs(TRAIN_OUTPUT_DIR, exist_ok=True)\n",
    "    with open(f\"{TRAIN_OUTPUT_DIR}/config_{config['run_name']}.json\", 'w') as f:\n",
    "        json.dump(config, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7c083c",
   "metadata": {},
   "source": [
    "#### Summary of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e63c157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save full results to JSON (make sure everything is serializable)\n",
    "results_path = os.path.join(TRAIN_OUTPUT_DIR, \"results_summary_\"+ datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")+\".json\")\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=4)\n",
    "\n",
    "# Print only the top 3 configurations (lowest validation loss)\n",
    "sorted_results = sorted(results, key=lambda r: float(r[\"best_val_loss\"]))\n",
    "top_k = sorted_results[:3]\n",
    "print(\"\\n=== Top 3 Configurations ===\") \n",
    "for i, res in enumerate(top_k, 1):\n",
    "    print(f\"{i}. Run: {res['config']} | Best Val Loss: {float(res['best_val_loss']):.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca7304f",
   "metadata": {},
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6fb787",
   "metadata": {},
   "source": [
    "#### File imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4e75767",
   "metadata": {},
   "outputs": [],
   "source": [
    "import config as config_file\n",
    "from src.test.ais_tester import AISTester"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd90563",
   "metadata": {},
   "source": [
    "#### Library imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c86bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df15213e",
   "metadata": {},
   "source": [
    "#### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb9e93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name of the model configuration to use\n",
    "MODEL_NAME = \"H128_L16_Lay1_lr0.001_BS64_Drop0.0\"  # Change as needed\n",
    "\n",
    "N_BEST_WORST = config_file.N_BEST_WORST\n",
    "N_MAP_RANDOM = config_file.N_MAP_RANDOM\n",
    "\n",
    "# Data to test on\n",
    "PARQUET_FILE = config_file.PRE_PROCESSING_DF_TEST_PATH\n",
    "\n",
    "# Output Directory\n",
    "OUTPUT_DIR = config_file.TEST_OUTPUT_DIR + \"/\" + MODEL_NAME\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "WEIGHTS_FILE = config_file.TRAIN_OUTPUT_DIR + \"/weights_\" + MODEL_NAME + \".pth\"\n",
    "MODEL_CONFIG_FILE = config_file.TRAIN_OUTPUT_DIR + \"/config_\" + MODEL_NAME + \".json\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0d3719",
   "metadata": {},
   "source": [
    "#### Load Model and Init Tester"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb45a6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model Config\n",
    "with open(MODEL_CONFIG_FILE, 'r') as f:\n",
    "    model_config = json.load(f)\n",
    "\n",
    "# Initialize Tester\n",
    "tester = AISTester(model_config, WEIGHTS_FILE, output_dir=OUTPUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea1b9a4",
   "metadata": {},
   "source": [
    "#### Run Testing and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cc68f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run tester pipeline (assumes PARQUET_FILE and tester are defined elsewhere in the notebook)\n",
    "if os.path.exists(PARQUET_FILE):\n",
    "    # 1. Evaluate ALL data first\n",
    "    tester.load_data(PARQUET_FILE)\n",
    "    tester.evaluate()\n",
    "        \n",
    "    # 2. Plot General Stats\n",
    "    tester.plot_error_distributions()\n",
    "        \n",
    "    # 3. Plot Filtered Stats (Example)\n",
    "    # You can pass a list of IDs to filter just the plot without re-running evaluate\n",
    "    # my_interesting_ids = [\"segment_A\", \"segment_B\"]\n",
    "    # tester.plot_error_distributions(filter_ids=my_interesting_ids, filename_suffix=\"_special_group\")\n",
    "        \n",
    "    # 4. Standard Best/Worst\n",
    "    tester.plot_best_worst_segments(n=N_BEST_WORST)\n",
    "        \n",
    "    # 5. Maps\n",
    "    tester.generate_maps(n_best_worst=N_BEST_WORST, n_random=N_MAP_RANDOM)\n",
    "\n",
    "    # 6. Filtered Map Example\n",
    "    # tester.generate_filtered_map(segment_ids=[\"segment_1\", \"segment_2\"], map_name=\"map_special_segments\")\n",
    "        \n",
    "else:\n",
    "    print(f\"File {PARQUET_FILE} not found.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
